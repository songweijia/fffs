// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: fsimage.proto

package org.apache.hadoop.hdfs.server.namenode;

public final class FsImageProto {
  private FsImageProto() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public interface FileSummaryOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required uint32 ondiskVersion = 1;
    /**
     * <code>required uint32 ondiskVersion = 1;</code>
     *
     * <pre>
     * The version of the above EBNF grammars.
     * </pre>
     */
    boolean hasOndiskVersion();
    /**
     * <code>required uint32 ondiskVersion = 1;</code>
     *
     * <pre>
     * The version of the above EBNF grammars.
     * </pre>
     */
    int getOndiskVersion();

    // required uint32 layoutVersion = 2;
    /**
     * <code>required uint32 layoutVersion = 2;</code>
     *
     * <pre>
     * layoutVersion describes which features are available in the
     * FSImage.
     * </pre>
     */
    boolean hasLayoutVersion();
    /**
     * <code>required uint32 layoutVersion = 2;</code>
     *
     * <pre>
     * layoutVersion describes which features are available in the
     * FSImage.
     * </pre>
     */
    int getLayoutVersion();

    // optional string codec = 3;
    /**
     * <code>optional string codec = 3;</code>
     */
    boolean hasCodec();
    /**
     * <code>optional string codec = 3;</code>
     */
    java.lang.String getCodec();
    /**
     * <code>optional string codec = 3;</code>
     */
    com.google.protobuf.ByteString
        getCodecBytes();

    // repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;
    /**
     * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section> 
        getSectionsList();
    /**
     * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
     */
    org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section getSections(int index);
    /**
     * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
     */
    int getSectionsCount();
    /**
     * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.SectionOrBuilder> 
        getSectionsOrBuilderList();
    /**
     * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
     */
    org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.SectionOrBuilder getSectionsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.hdfs.fsimage.FileSummary}
   */
  public static final class FileSummary extends
      com.google.protobuf.GeneratedMessage
      implements FileSummaryOrBuilder {
    // Use FileSummary.newBuilder() to construct.
    private FileSummary(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private FileSummary(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final FileSummary defaultInstance;
    public static FileSummary getDefaultInstance() {
      return defaultInstance;
    }

    public FileSummary getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private FileSummary(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              ondiskVersion_ = input.readUInt32();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              layoutVersion_ = input.readUInt32();
              break;
            }
            case 26: {
              bitField0_ |= 0x00000004;
              codec_ = input.readBytes();
              break;
            }
            case 34: {
              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
                sections_ = new java.util.ArrayList<org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section>();
                mutable_bitField0_ |= 0x00000008;
              }
              sections_.add(input.readMessage(org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
          sections_ = java.util.Collections.unmodifiableList(sections_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_FileSummary_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_FileSummary_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Builder.class);
    }

    public static com.google.protobuf.Parser<FileSummary> PARSER =
        new com.google.protobuf.AbstractParser<FileSummary>() {
      public FileSummary parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new FileSummary(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<FileSummary> getParserForType() {
      return PARSER;
    }

    public interface SectionOrBuilder
        extends com.google.protobuf.MessageOrBuilder {

      // optional string name = 1;
      /**
       * <code>optional string name = 1;</code>
       */
      boolean hasName();
      /**
       * <code>optional string name = 1;</code>
       */
      java.lang.String getName();
      /**
       * <code>optional string name = 1;</code>
       */
      com.google.protobuf.ByteString
          getNameBytes();

      // optional uint64 length = 2;
      /**
       * <code>optional uint64 length = 2;</code>
       */
      boolean hasLength();
      /**
       * <code>optional uint64 length = 2;</code>
       */
      long getLength();

      // optional uint64 offset = 3;
      /**
       * <code>optional uint64 offset = 3;</code>
       */
      boolean hasOffset();
      /**
       * <code>optional uint64 offset = 3;</code>
       */
      long getOffset();
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.FileSummary.Section}
     *
     * <pre>
     * index for each section
     * </pre>
     */
    public static final class Section extends
        com.google.protobuf.GeneratedMessage
        implements SectionOrBuilder {
      // Use Section.newBuilder() to construct.
      private Section(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
        super(builder);
        this.unknownFields = builder.getUnknownFields();
      }
      private Section(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

      private static final Section defaultInstance;
      public static Section getDefaultInstance() {
        return defaultInstance;
      }

      public Section getDefaultInstanceForType() {
        return defaultInstance;
      }

      private final com.google.protobuf.UnknownFieldSet unknownFields;
      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
          getUnknownFields() {
        return this.unknownFields;
      }
      private Section(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        initFields();
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
              case 10: {
                bitField0_ |= 0x00000001;
                name_ = input.readBytes();
                break;
              }
              case 16: {
                bitField0_ |= 0x00000002;
                length_ = input.readUInt64();
                break;
              }
              case 24: {
                bitField0_ |= 0x00000004;
                offset_ = input.readUInt64();
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e.getMessage()).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_FileSummary_Section_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_FileSummary_Section_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section.Builder.class);
      }

      public static com.google.protobuf.Parser<Section> PARSER =
          new com.google.protobuf.AbstractParser<Section>() {
        public Section parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Section(input, extensionRegistry);
        }
      };

      @java.lang.Override
      public com.google.protobuf.Parser<Section> getParserForType() {
        return PARSER;
      }

      private int bitField0_;
      // optional string name = 1;
      public static final int NAME_FIELD_NUMBER = 1;
      private java.lang.Object name_;
      /**
       * <code>optional string name = 1;</code>
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string name = 1;</code>
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (ref instanceof java.lang.String) {
          return (java.lang.String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            name_ = s;
          }
          return s;
        }
      }
      /**
       * <code>optional string name = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof java.lang.String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      // optional uint64 length = 2;
      public static final int LENGTH_FIELD_NUMBER = 2;
      private long length_;
      /**
       * <code>optional uint64 length = 2;</code>
       */
      public boolean hasLength() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional uint64 length = 2;</code>
       */
      public long getLength() {
        return length_;
      }

      // optional uint64 offset = 3;
      public static final int OFFSET_FIELD_NUMBER = 3;
      private long offset_;
      /**
       * <code>optional uint64 offset = 3;</code>
       */
      public boolean hasOffset() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional uint64 offset = 3;</code>
       */
      public long getOffset() {
        return offset_;
      }

      private void initFields() {
        name_ = "";
        length_ = 0L;
        offset_ = 0L;
      }
      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized != -1) return isInitialized == 1;

        memoizedIsInitialized = 1;
        return true;
      }

      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          output.writeBytes(1, getNameBytes());
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          output.writeUInt64(2, length_);
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          output.writeUInt64(3, offset_);
        }
        getUnknownFields().writeTo(output);
      }

      private int memoizedSerializedSize = -1;
      public int getSerializedSize() {
        int size = memoizedSerializedSize;
        if (size != -1) return size;

        size = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          size += com.google.protobuf.CodedOutputStream
            .computeBytesSize(1, getNameBytes());
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(2, length_);
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(3, offset_);
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSerializedSize = size;
        return size;
      }

      private static final long serialVersionUID = 0L;
      @java.lang.Override
      protected java.lang.Object writeReplace()
          throws java.io.ObjectStreamException {
        return super.writeReplace();
      }

      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }

      public static Builder newBuilder() { return Builder.create(); }
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section prototype) {
        return newBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() { return newBuilder(this); }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code hadoop.hdfs.fsimage.FileSummary.Section}
       *
       * <pre>
       * index for each section
       * </pre>
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder>
         implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.SectionOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_FileSummary_Section_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_FileSummary_Section_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section.Builder.class);
        }

        // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          }
        }
        private static Builder create() {
          return new Builder();
        }

        public Builder clear() {
          super.clear();
          name_ = "";
          bitField0_ = (bitField0_ & ~0x00000001);
          length_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000002);
          offset_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000004);
          return this;
        }

        public Builder clone() {
          return create().mergeFrom(buildPartial());
        }

        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_FileSummary_Section_descriptor;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section getDefaultInstanceForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section.getDefaultInstance();
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section build() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section buildPartial() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
            to_bitField0_ |= 0x00000001;
          }
          result.name_ = name_;
          if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
            to_bitField0_ |= 0x00000002;
          }
          result.length_ = length_;
          if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
            to_bitField0_ |= 0x00000004;
          }
          result.offset_ = offset_;
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section) {
            return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section other) {
          if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section.getDefaultInstance()) return this;
          if (other.hasName()) {
            bitField0_ |= 0x00000001;
            name_ = other.name_;
            onChanged();
          }
          if (other.hasLength()) {
            setLength(other.getLength());
          }
          if (other.hasOffset()) {
            setOffset(other.getOffset());
          }
          this.mergeUnknownFields(other.getUnknownFields());
          return this;
        }

        public final boolean isInitialized() {
          return true;
        }

        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section) e.getUnfinishedMessage();
            throw e;
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        // optional string name = 1;
        private java.lang.Object name_ = "";
        /**
         * <code>optional string name = 1;</code>
         */
        public boolean hasName() {
          return ((bitField0_ & 0x00000001) == 0x00000001);
        }
        /**
         * <code>optional string name = 1;</code>
         */
        public java.lang.String getName() {
          java.lang.Object ref = name_;
          if (!(ref instanceof java.lang.String)) {
            java.lang.String s = ((com.google.protobuf.ByteString) ref)
                .toStringUtf8();
            name_ = s;
            return s;
          } else {
            return (java.lang.String) ref;
          }
        }
        /**
         * <code>optional string name = 1;</code>
         */
        public com.google.protobuf.ByteString
            getNameBytes() {
          java.lang.Object ref = name_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            name_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <code>optional string name = 1;</code>
         */
        public Builder setName(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
          name_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional string name = 1;</code>
         */
        public Builder clearName() {
          bitField0_ = (bitField0_ & ~0x00000001);
          name_ = getDefaultInstance().getName();
          onChanged();
          return this;
        }
        /**
         * <code>optional string name = 1;</code>
         */
        public Builder setNameBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
          name_ = value;
          onChanged();
          return this;
        }

        // optional uint64 length = 2;
        private long length_ ;
        /**
         * <code>optional uint64 length = 2;</code>
         */
        public boolean hasLength() {
          return ((bitField0_ & 0x00000002) == 0x00000002);
        }
        /**
         * <code>optional uint64 length = 2;</code>
         */
        public long getLength() {
          return length_;
        }
        /**
         * <code>optional uint64 length = 2;</code>
         */
        public Builder setLength(long value) {
          bitField0_ |= 0x00000002;
          length_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 length = 2;</code>
         */
        public Builder clearLength() {
          bitField0_ = (bitField0_ & ~0x00000002);
          length_ = 0L;
          onChanged();
          return this;
        }

        // optional uint64 offset = 3;
        private long offset_ ;
        /**
         * <code>optional uint64 offset = 3;</code>
         */
        public boolean hasOffset() {
          return ((bitField0_ & 0x00000004) == 0x00000004);
        }
        /**
         * <code>optional uint64 offset = 3;</code>
         */
        public long getOffset() {
          return offset_;
        }
        /**
         * <code>optional uint64 offset = 3;</code>
         */
        public Builder setOffset(long value) {
          bitField0_ |= 0x00000004;
          offset_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 offset = 3;</code>
         */
        public Builder clearOffset() {
          bitField0_ = (bitField0_ & ~0x00000004);
          offset_ = 0L;
          onChanged();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.FileSummary.Section)
      }

      static {
        defaultInstance = new Section(true);
        defaultInstance.initFields();
      }

      // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.FileSummary.Section)
    }

    private int bitField0_;
    // required uint32 ondiskVersion = 1;
    public static final int ONDISKVERSION_FIELD_NUMBER = 1;
    private int ondiskVersion_;
    /**
     * <code>required uint32 ondiskVersion = 1;</code>
     *
     * <pre>
     * The version of the above EBNF grammars.
     * </pre>
     */
    public boolean hasOndiskVersion() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required uint32 ondiskVersion = 1;</code>
     *
     * <pre>
     * The version of the above EBNF grammars.
     * </pre>
     */
    public int getOndiskVersion() {
      return ondiskVersion_;
    }

    // required uint32 layoutVersion = 2;
    public static final int LAYOUTVERSION_FIELD_NUMBER = 2;
    private int layoutVersion_;
    /**
     * <code>required uint32 layoutVersion = 2;</code>
     *
     * <pre>
     * layoutVersion describes which features are available in the
     * FSImage.
     * </pre>
     */
    public boolean hasLayoutVersion() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required uint32 layoutVersion = 2;</code>
     *
     * <pre>
     * layoutVersion describes which features are available in the
     * FSImage.
     * </pre>
     */
    public int getLayoutVersion() {
      return layoutVersion_;
    }

    // optional string codec = 3;
    public static final int CODEC_FIELD_NUMBER = 3;
    private java.lang.Object codec_;
    /**
     * <code>optional string codec = 3;</code>
     */
    public boolean hasCodec() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional string codec = 3;</code>
     */
    public java.lang.String getCodec() {
      java.lang.Object ref = codec_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          codec_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string codec = 3;</code>
     */
    public com.google.protobuf.ByteString
        getCodecBytes() {
      java.lang.Object ref = codec_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        codec_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;
    public static final int SECTIONS_FIELD_NUMBER = 4;
    private java.util.List<org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section> sections_;
    /**
     * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
     */
    public java.util.List<org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section> getSectionsList() {
      return sections_;
    }
    /**
     * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.SectionOrBuilder> 
        getSectionsOrBuilderList() {
      return sections_;
    }
    /**
     * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
     */
    public int getSectionsCount() {
      return sections_.size();
    }
    /**
     * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
     */
    public org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section getSections(int index) {
      return sections_.get(index);
    }
    /**
     * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
     */
    public org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.SectionOrBuilder getSectionsOrBuilder(
        int index) {
      return sections_.get(index);
    }

    private void initFields() {
      ondiskVersion_ = 0;
      layoutVersion_ = 0;
      codec_ = "";
      sections_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasOndiskVersion()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasLayoutVersion()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeUInt32(1, ondiskVersion_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt32(2, layoutVersion_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBytes(3, getCodecBytes());
      }
      for (int i = 0; i < sections_.size(); i++) {
        output.writeMessage(4, sections_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(1, ondiskVersion_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(2, layoutVersion_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(3, getCodecBytes());
      }
      for (int i = 0; i < sections_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, sections_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.FileSummary}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummaryOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_FileSummary_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_FileSummary_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getSectionsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        ondiskVersion_ = 0;
        bitField0_ = (bitField0_ & ~0x00000001);
        layoutVersion_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        codec_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        if (sectionsBuilder_ == null) {
          sections_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
        } else {
          sectionsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_FileSummary_descriptor;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary build() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary buildPartial() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.ondiskVersion_ = ondiskVersion_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.layoutVersion_ = layoutVersion_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.codec_ = codec_;
        if (sectionsBuilder_ == null) {
          if (((bitField0_ & 0x00000008) == 0x00000008)) {
            sections_ = java.util.Collections.unmodifiableList(sections_);
            bitField0_ = (bitField0_ & ~0x00000008);
          }
          result.sections_ = sections_;
        } else {
          result.sections_ = sectionsBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary) {
          return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary other) {
        if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.getDefaultInstance()) return this;
        if (other.hasOndiskVersion()) {
          setOndiskVersion(other.getOndiskVersion());
        }
        if (other.hasLayoutVersion()) {
          setLayoutVersion(other.getLayoutVersion());
        }
        if (other.hasCodec()) {
          bitField0_ |= 0x00000004;
          codec_ = other.codec_;
          onChanged();
        }
        if (sectionsBuilder_ == null) {
          if (!other.sections_.isEmpty()) {
            if (sections_.isEmpty()) {
              sections_ = other.sections_;
              bitField0_ = (bitField0_ & ~0x00000008);
            } else {
              ensureSectionsIsMutable();
              sections_.addAll(other.sections_);
            }
            onChanged();
          }
        } else {
          if (!other.sections_.isEmpty()) {
            if (sectionsBuilder_.isEmpty()) {
              sectionsBuilder_.dispose();
              sectionsBuilder_ = null;
              sections_ = other.sections_;
              bitField0_ = (bitField0_ & ~0x00000008);
              sectionsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getSectionsFieldBuilder() : null;
            } else {
              sectionsBuilder_.addAllMessages(other.sections_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasOndiskVersion()) {
          
          return false;
        }
        if (!hasLayoutVersion()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required uint32 ondiskVersion = 1;
      private int ondiskVersion_ ;
      /**
       * <code>required uint32 ondiskVersion = 1;</code>
       *
       * <pre>
       * The version of the above EBNF grammars.
       * </pre>
       */
      public boolean hasOndiskVersion() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required uint32 ondiskVersion = 1;</code>
       *
       * <pre>
       * The version of the above EBNF grammars.
       * </pre>
       */
      public int getOndiskVersion() {
        return ondiskVersion_;
      }
      /**
       * <code>required uint32 ondiskVersion = 1;</code>
       *
       * <pre>
       * The version of the above EBNF grammars.
       * </pre>
       */
      public Builder setOndiskVersion(int value) {
        bitField0_ |= 0x00000001;
        ondiskVersion_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint32 ondiskVersion = 1;</code>
       *
       * <pre>
       * The version of the above EBNF grammars.
       * </pre>
       */
      public Builder clearOndiskVersion() {
        bitField0_ = (bitField0_ & ~0x00000001);
        ondiskVersion_ = 0;
        onChanged();
        return this;
      }

      // required uint32 layoutVersion = 2;
      private int layoutVersion_ ;
      /**
       * <code>required uint32 layoutVersion = 2;</code>
       *
       * <pre>
       * layoutVersion describes which features are available in the
       * FSImage.
       * </pre>
       */
      public boolean hasLayoutVersion() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required uint32 layoutVersion = 2;</code>
       *
       * <pre>
       * layoutVersion describes which features are available in the
       * FSImage.
       * </pre>
       */
      public int getLayoutVersion() {
        return layoutVersion_;
      }
      /**
       * <code>required uint32 layoutVersion = 2;</code>
       *
       * <pre>
       * layoutVersion describes which features are available in the
       * FSImage.
       * </pre>
       */
      public Builder setLayoutVersion(int value) {
        bitField0_ |= 0x00000002;
        layoutVersion_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint32 layoutVersion = 2;</code>
       *
       * <pre>
       * layoutVersion describes which features are available in the
       * FSImage.
       * </pre>
       */
      public Builder clearLayoutVersion() {
        bitField0_ = (bitField0_ & ~0x00000002);
        layoutVersion_ = 0;
        onChanged();
        return this;
      }

      // optional string codec = 3;
      private java.lang.Object codec_ = "";
      /**
       * <code>optional string codec = 3;</code>
       */
      public boolean hasCodec() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional string codec = 3;</code>
       */
      public java.lang.String getCodec() {
        java.lang.Object ref = codec_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          codec_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string codec = 3;</code>
       */
      public com.google.protobuf.ByteString
          getCodecBytes() {
        java.lang.Object ref = codec_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          codec_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string codec = 3;</code>
       */
      public Builder setCodec(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        codec_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string codec = 3;</code>
       */
      public Builder clearCodec() {
        bitField0_ = (bitField0_ & ~0x00000004);
        codec_ = getDefaultInstance().getCodec();
        onChanged();
        return this;
      }
      /**
       * <code>optional string codec = 3;</code>
       */
      public Builder setCodecBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        codec_ = value;
        onChanged();
        return this;
      }

      // repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;
      private java.util.List<org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section> sections_ =
        java.util.Collections.emptyList();
      private void ensureSectionsIsMutable() {
        if (!((bitField0_ & 0x00000008) == 0x00000008)) {
          sections_ = new java.util.ArrayList<org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section>(sections_);
          bitField0_ |= 0x00000008;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section, org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.SectionOrBuilder> sectionsBuilder_;

      /**
       * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section> getSectionsList() {
        if (sectionsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(sections_);
        } else {
          return sectionsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
       */
      public int getSectionsCount() {
        if (sectionsBuilder_ == null) {
          return sections_.size();
        } else {
          return sectionsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section getSections(int index) {
        if (sectionsBuilder_ == null) {
          return sections_.get(index);
        } else {
          return sectionsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
       */
      public Builder setSections(
          int index, org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section value) {
        if (sectionsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSectionsIsMutable();
          sections_.set(index, value);
          onChanged();
        } else {
          sectionsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
       */
      public Builder setSections(
          int index, org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section.Builder builderForValue) {
        if (sectionsBuilder_ == null) {
          ensureSectionsIsMutable();
          sections_.set(index, builderForValue.build());
          onChanged();
        } else {
          sectionsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
       */
      public Builder addSections(org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section value) {
        if (sectionsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSectionsIsMutable();
          sections_.add(value);
          onChanged();
        } else {
          sectionsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
       */
      public Builder addSections(
          int index, org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section value) {
        if (sectionsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSectionsIsMutable();
          sections_.add(index, value);
          onChanged();
        } else {
          sectionsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
       */
      public Builder addSections(
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section.Builder builderForValue) {
        if (sectionsBuilder_ == null) {
          ensureSectionsIsMutable();
          sections_.add(builderForValue.build());
          onChanged();
        } else {
          sectionsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
       */
      public Builder addSections(
          int index, org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section.Builder builderForValue) {
        if (sectionsBuilder_ == null) {
          ensureSectionsIsMutable();
          sections_.add(index, builderForValue.build());
          onChanged();
        } else {
          sectionsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
       */
      public Builder addAllSections(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section> values) {
        if (sectionsBuilder_ == null) {
          ensureSectionsIsMutable();
          super.addAll(values, sections_);
          onChanged();
        } else {
          sectionsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
       */
      public Builder clearSections() {
        if (sectionsBuilder_ == null) {
          sections_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
          onChanged();
        } else {
          sectionsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
       */
      public Builder removeSections(int index) {
        if (sectionsBuilder_ == null) {
          ensureSectionsIsMutable();
          sections_.remove(index);
          onChanged();
        } else {
          sectionsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section.Builder getSectionsBuilder(
          int index) {
        return getSectionsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.SectionOrBuilder getSectionsOrBuilder(
          int index) {
        if (sectionsBuilder_ == null) {
          return sections_.get(index);  } else {
          return sectionsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.SectionOrBuilder> 
           getSectionsOrBuilderList() {
        if (sectionsBuilder_ != null) {
          return sectionsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(sections_);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section.Builder addSectionsBuilder() {
        return getSectionsFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section.Builder addSectionsBuilder(
          int index) {
        return getSectionsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.fsimage.FileSummary.Section sections = 4;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section.Builder> 
           getSectionsBuilderList() {
        return getSectionsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section, org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.SectionOrBuilder> 
          getSectionsFieldBuilder() {
        if (sectionsBuilder_ == null) {
          sectionsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section, org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.Section.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.FileSummary.SectionOrBuilder>(
                  sections_,
                  ((bitField0_ & 0x00000008) == 0x00000008),
                  getParentForChildren(),
                  isClean());
          sections_ = null;
        }
        return sectionsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.FileSummary)
    }

    static {
      defaultInstance = new FileSummary(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.FileSummary)
  }

  public interface NameSystemSectionOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional uint32 namespaceId = 1;
    /**
     * <code>optional uint32 namespaceId = 1;</code>
     */
    boolean hasNamespaceId();
    /**
     * <code>optional uint32 namespaceId = 1;</code>
     */
    int getNamespaceId();

    // optional uint64 genstampV1 = 2;
    /**
     * <code>optional uint64 genstampV1 = 2;</code>
     */
    boolean hasGenstampV1();
    /**
     * <code>optional uint64 genstampV1 = 2;</code>
     */
    long getGenstampV1();

    // optional uint64 genstampV2 = 3;
    /**
     * <code>optional uint64 genstampV2 = 3;</code>
     */
    boolean hasGenstampV2();
    /**
     * <code>optional uint64 genstampV2 = 3;</code>
     */
    long getGenstampV2();

    // optional uint64 genstampV1Limit = 4;
    /**
     * <code>optional uint64 genstampV1Limit = 4;</code>
     */
    boolean hasGenstampV1Limit();
    /**
     * <code>optional uint64 genstampV1Limit = 4;</code>
     */
    long getGenstampV1Limit();

    // optional uint64 lastAllocatedBlockId = 5;
    /**
     * <code>optional uint64 lastAllocatedBlockId = 5;</code>
     */
    boolean hasLastAllocatedBlockId();
    /**
     * <code>optional uint64 lastAllocatedBlockId = 5;</code>
     */
    long getLastAllocatedBlockId();

    // optional uint64 transactionId = 6;
    /**
     * <code>optional uint64 transactionId = 6;</code>
     */
    boolean hasTransactionId();
    /**
     * <code>optional uint64 transactionId = 6;</code>
     */
    long getTransactionId();

    // optional uint64 rollingUpgradeStartTime = 7;
    /**
     * <code>optional uint64 rollingUpgradeStartTime = 7;</code>
     */
    boolean hasRollingUpgradeStartTime();
    /**
     * <code>optional uint64 rollingUpgradeStartTime = 7;</code>
     */
    long getRollingUpgradeStartTime();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.fsimage.NameSystemSection}
   *
   * <pre>
   **
   * Name: NS_INFO
   * </pre>
   */
  public static final class NameSystemSection extends
      com.google.protobuf.GeneratedMessage
      implements NameSystemSectionOrBuilder {
    // Use NameSystemSection.newBuilder() to construct.
    private NameSystemSection(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private NameSystemSection(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final NameSystemSection defaultInstance;
    public static NameSystemSection getDefaultInstance() {
      return defaultInstance;
    }

    public NameSystemSection getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private NameSystemSection(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              namespaceId_ = input.readUInt32();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              genstampV1_ = input.readUInt64();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              genstampV2_ = input.readUInt64();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              genstampV1Limit_ = input.readUInt64();
              break;
            }
            case 40: {
              bitField0_ |= 0x00000010;
              lastAllocatedBlockId_ = input.readUInt64();
              break;
            }
            case 48: {
              bitField0_ |= 0x00000020;
              transactionId_ = input.readUInt64();
              break;
            }
            case 56: {
              bitField0_ |= 0x00000040;
              rollingUpgradeStartTime_ = input.readUInt64();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_NameSystemSection_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_NameSystemSection_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection.Builder.class);
    }

    public static com.google.protobuf.Parser<NameSystemSection> PARSER =
        new com.google.protobuf.AbstractParser<NameSystemSection>() {
      public NameSystemSection parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new NameSystemSection(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<NameSystemSection> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional uint32 namespaceId = 1;
    public static final int NAMESPACEID_FIELD_NUMBER = 1;
    private int namespaceId_;
    /**
     * <code>optional uint32 namespaceId = 1;</code>
     */
    public boolean hasNamespaceId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional uint32 namespaceId = 1;</code>
     */
    public int getNamespaceId() {
      return namespaceId_;
    }

    // optional uint64 genstampV1 = 2;
    public static final int GENSTAMPV1_FIELD_NUMBER = 2;
    private long genstampV1_;
    /**
     * <code>optional uint64 genstampV1 = 2;</code>
     */
    public boolean hasGenstampV1() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional uint64 genstampV1 = 2;</code>
     */
    public long getGenstampV1() {
      return genstampV1_;
    }

    // optional uint64 genstampV2 = 3;
    public static final int GENSTAMPV2_FIELD_NUMBER = 3;
    private long genstampV2_;
    /**
     * <code>optional uint64 genstampV2 = 3;</code>
     */
    public boolean hasGenstampV2() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional uint64 genstampV2 = 3;</code>
     */
    public long getGenstampV2() {
      return genstampV2_;
    }

    // optional uint64 genstampV1Limit = 4;
    public static final int GENSTAMPV1LIMIT_FIELD_NUMBER = 4;
    private long genstampV1Limit_;
    /**
     * <code>optional uint64 genstampV1Limit = 4;</code>
     */
    public boolean hasGenstampV1Limit() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional uint64 genstampV1Limit = 4;</code>
     */
    public long getGenstampV1Limit() {
      return genstampV1Limit_;
    }

    // optional uint64 lastAllocatedBlockId = 5;
    public static final int LASTALLOCATEDBLOCKID_FIELD_NUMBER = 5;
    private long lastAllocatedBlockId_;
    /**
     * <code>optional uint64 lastAllocatedBlockId = 5;</code>
     */
    public boolean hasLastAllocatedBlockId() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>optional uint64 lastAllocatedBlockId = 5;</code>
     */
    public long getLastAllocatedBlockId() {
      return lastAllocatedBlockId_;
    }

    // optional uint64 transactionId = 6;
    public static final int TRANSACTIONID_FIELD_NUMBER = 6;
    private long transactionId_;
    /**
     * <code>optional uint64 transactionId = 6;</code>
     */
    public boolean hasTransactionId() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    /**
     * <code>optional uint64 transactionId = 6;</code>
     */
    public long getTransactionId() {
      return transactionId_;
    }

    // optional uint64 rollingUpgradeStartTime = 7;
    public static final int ROLLINGUPGRADESTARTTIME_FIELD_NUMBER = 7;
    private long rollingUpgradeStartTime_;
    /**
     * <code>optional uint64 rollingUpgradeStartTime = 7;</code>
     */
    public boolean hasRollingUpgradeStartTime() {
      return ((bitField0_ & 0x00000040) == 0x00000040);
    }
    /**
     * <code>optional uint64 rollingUpgradeStartTime = 7;</code>
     */
    public long getRollingUpgradeStartTime() {
      return rollingUpgradeStartTime_;
    }

    private void initFields() {
      namespaceId_ = 0;
      genstampV1_ = 0L;
      genstampV2_ = 0L;
      genstampV1Limit_ = 0L;
      lastAllocatedBlockId_ = 0L;
      transactionId_ = 0L;
      rollingUpgradeStartTime_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeUInt32(1, namespaceId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt64(2, genstampV1_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeUInt64(3, genstampV2_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeUInt64(4, genstampV1Limit_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeUInt64(5, lastAllocatedBlockId_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeUInt64(6, transactionId_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        output.writeUInt64(7, rollingUpgradeStartTime_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(1, namespaceId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, genstampV1_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(3, genstampV2_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(4, genstampV1Limit_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(5, lastAllocatedBlockId_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(6, transactionId_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(7, rollingUpgradeStartTime_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.NameSystemSection}
     *
     * <pre>
     **
     * Name: NS_INFO
     * </pre>
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSectionOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_NameSystemSection_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_NameSystemSection_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        namespaceId_ = 0;
        bitField0_ = (bitField0_ & ~0x00000001);
        genstampV1_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        genstampV2_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        genstampV1Limit_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        lastAllocatedBlockId_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000010);
        transactionId_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000020);
        rollingUpgradeStartTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000040);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_NameSystemSection_descriptor;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection build() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection buildPartial() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.namespaceId_ = namespaceId_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.genstampV1_ = genstampV1_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.genstampV2_ = genstampV2_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.genstampV1Limit_ = genstampV1Limit_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000010;
        }
        result.lastAllocatedBlockId_ = lastAllocatedBlockId_;
        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
          to_bitField0_ |= 0x00000020;
        }
        result.transactionId_ = transactionId_;
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000040;
        }
        result.rollingUpgradeStartTime_ = rollingUpgradeStartTime_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection) {
          return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection other) {
        if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection.getDefaultInstance()) return this;
        if (other.hasNamespaceId()) {
          setNamespaceId(other.getNamespaceId());
        }
        if (other.hasGenstampV1()) {
          setGenstampV1(other.getGenstampV1());
        }
        if (other.hasGenstampV2()) {
          setGenstampV2(other.getGenstampV2());
        }
        if (other.hasGenstampV1Limit()) {
          setGenstampV1Limit(other.getGenstampV1Limit());
        }
        if (other.hasLastAllocatedBlockId()) {
          setLastAllocatedBlockId(other.getLastAllocatedBlockId());
        }
        if (other.hasTransactionId()) {
          setTransactionId(other.getTransactionId());
        }
        if (other.hasRollingUpgradeStartTime()) {
          setRollingUpgradeStartTime(other.getRollingUpgradeStartTime());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.NameSystemSection) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional uint32 namespaceId = 1;
      private int namespaceId_ ;
      /**
       * <code>optional uint32 namespaceId = 1;</code>
       */
      public boolean hasNamespaceId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint32 namespaceId = 1;</code>
       */
      public int getNamespaceId() {
        return namespaceId_;
      }
      /**
       * <code>optional uint32 namespaceId = 1;</code>
       */
      public Builder setNamespaceId(int value) {
        bitField0_ |= 0x00000001;
        namespaceId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 namespaceId = 1;</code>
       */
      public Builder clearNamespaceId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        namespaceId_ = 0;
        onChanged();
        return this;
      }

      // optional uint64 genstampV1 = 2;
      private long genstampV1_ ;
      /**
       * <code>optional uint64 genstampV1 = 2;</code>
       */
      public boolean hasGenstampV1() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional uint64 genstampV1 = 2;</code>
       */
      public long getGenstampV1() {
        return genstampV1_;
      }
      /**
       * <code>optional uint64 genstampV1 = 2;</code>
       */
      public Builder setGenstampV1(long value) {
        bitField0_ |= 0x00000002;
        genstampV1_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 genstampV1 = 2;</code>
       */
      public Builder clearGenstampV1() {
        bitField0_ = (bitField0_ & ~0x00000002);
        genstampV1_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 genstampV2 = 3;
      private long genstampV2_ ;
      /**
       * <code>optional uint64 genstampV2 = 3;</code>
       */
      public boolean hasGenstampV2() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional uint64 genstampV2 = 3;</code>
       */
      public long getGenstampV2() {
        return genstampV2_;
      }
      /**
       * <code>optional uint64 genstampV2 = 3;</code>
       */
      public Builder setGenstampV2(long value) {
        bitField0_ |= 0x00000004;
        genstampV2_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 genstampV2 = 3;</code>
       */
      public Builder clearGenstampV2() {
        bitField0_ = (bitField0_ & ~0x00000004);
        genstampV2_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 genstampV1Limit = 4;
      private long genstampV1Limit_ ;
      /**
       * <code>optional uint64 genstampV1Limit = 4;</code>
       */
      public boolean hasGenstampV1Limit() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional uint64 genstampV1Limit = 4;</code>
       */
      public long getGenstampV1Limit() {
        return genstampV1Limit_;
      }
      /**
       * <code>optional uint64 genstampV1Limit = 4;</code>
       */
      public Builder setGenstampV1Limit(long value) {
        bitField0_ |= 0x00000008;
        genstampV1Limit_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 genstampV1Limit = 4;</code>
       */
      public Builder clearGenstampV1Limit() {
        bitField0_ = (bitField0_ & ~0x00000008);
        genstampV1Limit_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 lastAllocatedBlockId = 5;
      private long lastAllocatedBlockId_ ;
      /**
       * <code>optional uint64 lastAllocatedBlockId = 5;</code>
       */
      public boolean hasLastAllocatedBlockId() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional uint64 lastAllocatedBlockId = 5;</code>
       */
      public long getLastAllocatedBlockId() {
        return lastAllocatedBlockId_;
      }
      /**
       * <code>optional uint64 lastAllocatedBlockId = 5;</code>
       */
      public Builder setLastAllocatedBlockId(long value) {
        bitField0_ |= 0x00000010;
        lastAllocatedBlockId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 lastAllocatedBlockId = 5;</code>
       */
      public Builder clearLastAllocatedBlockId() {
        bitField0_ = (bitField0_ & ~0x00000010);
        lastAllocatedBlockId_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 transactionId = 6;
      private long transactionId_ ;
      /**
       * <code>optional uint64 transactionId = 6;</code>
       */
      public boolean hasTransactionId() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      /**
       * <code>optional uint64 transactionId = 6;</code>
       */
      public long getTransactionId() {
        return transactionId_;
      }
      /**
       * <code>optional uint64 transactionId = 6;</code>
       */
      public Builder setTransactionId(long value) {
        bitField0_ |= 0x00000020;
        transactionId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 transactionId = 6;</code>
       */
      public Builder clearTransactionId() {
        bitField0_ = (bitField0_ & ~0x00000020);
        transactionId_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 rollingUpgradeStartTime = 7;
      private long rollingUpgradeStartTime_ ;
      /**
       * <code>optional uint64 rollingUpgradeStartTime = 7;</code>
       */
      public boolean hasRollingUpgradeStartTime() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional uint64 rollingUpgradeStartTime = 7;</code>
       */
      public long getRollingUpgradeStartTime() {
        return rollingUpgradeStartTime_;
      }
      /**
       * <code>optional uint64 rollingUpgradeStartTime = 7;</code>
       */
      public Builder setRollingUpgradeStartTime(long value) {
        bitField0_ |= 0x00000040;
        rollingUpgradeStartTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 rollingUpgradeStartTime = 7;</code>
       */
      public Builder clearRollingUpgradeStartTime() {
        bitField0_ = (bitField0_ & ~0x00000040);
        rollingUpgradeStartTime_ = 0L;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.NameSystemSection)
    }

    static {
      defaultInstance = new NameSystemSection(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.NameSystemSection)
  }

  public interface INodeSectionOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional uint64 lastInodeId = 1;
    /**
     * <code>optional uint64 lastInodeId = 1;</code>
     */
    boolean hasLastInodeId();
    /**
     * <code>optional uint64 lastInodeId = 1;</code>
     */
    long getLastInodeId();

    // optional uint64 numInodes = 2;
    /**
     * <code>optional uint64 numInodes = 2;</code>
     *
     * <pre>
     * repeated INodes..
     * </pre>
     */
    boolean hasNumInodes();
    /**
     * <code>optional uint64 numInodes = 2;</code>
     *
     * <pre>
     * repeated INodes..
     * </pre>
     */
    long getNumInodes();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.fsimage.INodeSection}
   *
   * <pre>
   **
   * Permission is serialized as a 64-bit long. [0:24):[25:48):[48:64) (in Big Endian).
   * The first and the second parts are the string ids of the user and
   * group name, and the last 16 bits are the permission bits.
   *
   * Name: INODE
   * </pre>
   */
  public static final class INodeSection extends
      com.google.protobuf.GeneratedMessage
      implements INodeSectionOrBuilder {
    // Use INodeSection.newBuilder() to construct.
    private INodeSection(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private INodeSection(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final INodeSection defaultInstance;
    public static INodeSection getDefaultInstance() {
      return defaultInstance;
    }

    public INodeSection getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private INodeSection(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              lastInodeId_ = input.readUInt64();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              numInodes_ = input.readUInt64();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.Builder.class);
    }

    public static com.google.protobuf.Parser<INodeSection> PARSER =
        new com.google.protobuf.AbstractParser<INodeSection>() {
      public INodeSection parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new INodeSection(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<INodeSection> getParserForType() {
      return PARSER;
    }

    public interface FileUnderConstructionFeatureOrBuilder
        extends com.google.protobuf.MessageOrBuilder {

      // optional string clientName = 1;
      /**
       * <code>optional string clientName = 1;</code>
       */
      boolean hasClientName();
      /**
       * <code>optional string clientName = 1;</code>
       */
      java.lang.String getClientName();
      /**
       * <code>optional string clientName = 1;</code>
       */
      com.google.protobuf.ByteString
          getClientNameBytes();

      // optional string clientMachine = 2;
      /**
       * <code>optional string clientMachine = 2;</code>
       */
      boolean hasClientMachine();
      /**
       * <code>optional string clientMachine = 2;</code>
       */
      java.lang.String getClientMachine();
      /**
       * <code>optional string clientMachine = 2;</code>
       */
      com.google.protobuf.ByteString
          getClientMachineBytes();
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature}
     *
     * <pre>
     **
     * under-construction feature for INodeFile
     * </pre>
     */
    public static final class FileUnderConstructionFeature extends
        com.google.protobuf.GeneratedMessage
        implements FileUnderConstructionFeatureOrBuilder {
      // Use FileUnderConstructionFeature.newBuilder() to construct.
      private FileUnderConstructionFeature(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
        super(builder);
        this.unknownFields = builder.getUnknownFields();
      }
      private FileUnderConstructionFeature(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

      private static final FileUnderConstructionFeature defaultInstance;
      public static FileUnderConstructionFeature getDefaultInstance() {
        return defaultInstance;
      }

      public FileUnderConstructionFeature getDefaultInstanceForType() {
        return defaultInstance;
      }

      private final com.google.protobuf.UnknownFieldSet unknownFields;
      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
          getUnknownFields() {
        return this.unknownFields;
      }
      private FileUnderConstructionFeature(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        initFields();
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
              case 10: {
                bitField0_ |= 0x00000001;
                clientName_ = input.readBytes();
                break;
              }
              case 18: {
                bitField0_ |= 0x00000002;
                clientMachine_ = input.readBytes();
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e.getMessage()).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_FileUnderConstructionFeature_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_FileUnderConstructionFeature_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature.Builder.class);
      }

      public static com.google.protobuf.Parser<FileUnderConstructionFeature> PARSER =
          new com.google.protobuf.AbstractParser<FileUnderConstructionFeature>() {
        public FileUnderConstructionFeature parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new FileUnderConstructionFeature(input, extensionRegistry);
        }
      };

      @java.lang.Override
      public com.google.protobuf.Parser<FileUnderConstructionFeature> getParserForType() {
        return PARSER;
      }

      private int bitField0_;
      // optional string clientName = 1;
      public static final int CLIENTNAME_FIELD_NUMBER = 1;
      private java.lang.Object clientName_;
      /**
       * <code>optional string clientName = 1;</code>
       */
      public boolean hasClientName() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string clientName = 1;</code>
       */
      public java.lang.String getClientName() {
        java.lang.Object ref = clientName_;
        if (ref instanceof java.lang.String) {
          return (java.lang.String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            clientName_ = s;
          }
          return s;
        }
      }
      /**
       * <code>optional string clientName = 1;</code>
       */
      public com.google.protobuf.ByteString
          getClientNameBytes() {
        java.lang.Object ref = clientName_;
        if (ref instanceof java.lang.String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          clientName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      // optional string clientMachine = 2;
      public static final int CLIENTMACHINE_FIELD_NUMBER = 2;
      private java.lang.Object clientMachine_;
      /**
       * <code>optional string clientMachine = 2;</code>
       */
      public boolean hasClientMachine() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional string clientMachine = 2;</code>
       */
      public java.lang.String getClientMachine() {
        java.lang.Object ref = clientMachine_;
        if (ref instanceof java.lang.String) {
          return (java.lang.String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            clientMachine_ = s;
          }
          return s;
        }
      }
      /**
       * <code>optional string clientMachine = 2;</code>
       */
      public com.google.protobuf.ByteString
          getClientMachineBytes() {
        java.lang.Object ref = clientMachine_;
        if (ref instanceof java.lang.String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          clientMachine_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      private void initFields() {
        clientName_ = "";
        clientMachine_ = "";
      }
      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized != -1) return isInitialized == 1;

        memoizedIsInitialized = 1;
        return true;
      }

      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          output.writeBytes(1, getClientNameBytes());
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          output.writeBytes(2, getClientMachineBytes());
        }
        getUnknownFields().writeTo(output);
      }

      private int memoizedSerializedSize = -1;
      public int getSerializedSize() {
        int size = memoizedSerializedSize;
        if (size != -1) return size;

        size = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          size += com.google.protobuf.CodedOutputStream
            .computeBytesSize(1, getClientNameBytes());
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          size += com.google.protobuf.CodedOutputStream
            .computeBytesSize(2, getClientMachineBytes());
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSerializedSize = size;
        return size;
      }

      private static final long serialVersionUID = 0L;
      @java.lang.Override
      protected java.lang.Object writeReplace()
          throws java.io.ObjectStreamException {
        return super.writeReplace();
      }

      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }

      public static Builder newBuilder() { return Builder.create(); }
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature prototype) {
        return newBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() { return newBuilder(this); }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature}
       *
       * <pre>
       **
       * under-construction feature for INodeFile
       * </pre>
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder>
         implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeatureOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_FileUnderConstructionFeature_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_FileUnderConstructionFeature_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature.Builder.class);
        }

        // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          }
        }
        private static Builder create() {
          return new Builder();
        }

        public Builder clear() {
          super.clear();
          clientName_ = "";
          bitField0_ = (bitField0_ & ~0x00000001);
          clientMachine_ = "";
          bitField0_ = (bitField0_ & ~0x00000002);
          return this;
        }

        public Builder clone() {
          return create().mergeFrom(buildPartial());
        }

        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_FileUnderConstructionFeature_descriptor;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature getDefaultInstanceForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature.getDefaultInstance();
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature build() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature buildPartial() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
            to_bitField0_ |= 0x00000001;
          }
          result.clientName_ = clientName_;
          if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
            to_bitField0_ |= 0x00000002;
          }
          result.clientMachine_ = clientMachine_;
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature) {
            return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature other) {
          if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature.getDefaultInstance()) return this;
          if (other.hasClientName()) {
            bitField0_ |= 0x00000001;
            clientName_ = other.clientName_;
            onChanged();
          }
          if (other.hasClientMachine()) {
            bitField0_ |= 0x00000002;
            clientMachine_ = other.clientMachine_;
            onChanged();
          }
          this.mergeUnknownFields(other.getUnknownFields());
          return this;
        }

        public final boolean isInitialized() {
          return true;
        }

        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature) e.getUnfinishedMessage();
            throw e;
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        // optional string clientName = 1;
        private java.lang.Object clientName_ = "";
        /**
         * <code>optional string clientName = 1;</code>
         */
        public boolean hasClientName() {
          return ((bitField0_ & 0x00000001) == 0x00000001);
        }
        /**
         * <code>optional string clientName = 1;</code>
         */
        public java.lang.String getClientName() {
          java.lang.Object ref = clientName_;
          if (!(ref instanceof java.lang.String)) {
            java.lang.String s = ((com.google.protobuf.ByteString) ref)
                .toStringUtf8();
            clientName_ = s;
            return s;
          } else {
            return (java.lang.String) ref;
          }
        }
        /**
         * <code>optional string clientName = 1;</code>
         */
        public com.google.protobuf.ByteString
            getClientNameBytes() {
          java.lang.Object ref = clientName_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            clientName_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <code>optional string clientName = 1;</code>
         */
        public Builder setClientName(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
          clientName_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional string clientName = 1;</code>
         */
        public Builder clearClientName() {
          bitField0_ = (bitField0_ & ~0x00000001);
          clientName_ = getDefaultInstance().getClientName();
          onChanged();
          return this;
        }
        /**
         * <code>optional string clientName = 1;</code>
         */
        public Builder setClientNameBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
          clientName_ = value;
          onChanged();
          return this;
        }

        // optional string clientMachine = 2;
        private java.lang.Object clientMachine_ = "";
        /**
         * <code>optional string clientMachine = 2;</code>
         */
        public boolean hasClientMachine() {
          return ((bitField0_ & 0x00000002) == 0x00000002);
        }
        /**
         * <code>optional string clientMachine = 2;</code>
         */
        public java.lang.String getClientMachine() {
          java.lang.Object ref = clientMachine_;
          if (!(ref instanceof java.lang.String)) {
            java.lang.String s = ((com.google.protobuf.ByteString) ref)
                .toStringUtf8();
            clientMachine_ = s;
            return s;
          } else {
            return (java.lang.String) ref;
          }
        }
        /**
         * <code>optional string clientMachine = 2;</code>
         */
        public com.google.protobuf.ByteString
            getClientMachineBytes() {
          java.lang.Object ref = clientMachine_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            clientMachine_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <code>optional string clientMachine = 2;</code>
         */
        public Builder setClientMachine(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
          clientMachine_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional string clientMachine = 2;</code>
         */
        public Builder clearClientMachine() {
          bitField0_ = (bitField0_ & ~0x00000002);
          clientMachine_ = getDefaultInstance().getClientMachine();
          onChanged();
          return this;
        }
        /**
         * <code>optional string clientMachine = 2;</code>
         */
        public Builder setClientMachineBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
          clientMachine_ = value;
          onChanged();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature)
      }

      static {
        defaultInstance = new FileUnderConstructionFeature(true);
        defaultInstance.initFields();
      }

      // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature)
    }

    public interface AclFeatureProtoOrBuilder
        extends com.google.protobuf.MessageOrBuilder {

      // repeated fixed32 entries = 2 [packed = true];
      /**
       * <code>repeated fixed32 entries = 2 [packed = true];</code>
       *
       * <pre>
       **
       * An ACL entry is represented by a 32-bit integer in Big Endian
       * format. The bits can be divided in four segments:
       * [0:2) || [2:26) || [26:27) || [27:29) || [29:32)
       *
       * [0:2) -- reserved for futute uses.
       * [2:26) -- the name of the entry, which is an ID that points to a
       * string in the StringTableSection.
       * [26:27) -- the scope of the entry (AclEntryScopeProto)
       * [27:29) -- the type of the entry (AclEntryTypeProto)
       * [29:32) -- the permission of the entry (FsActionProto)
       * </pre>
       */
      java.util.List<java.lang.Integer> getEntriesList();
      /**
       * <code>repeated fixed32 entries = 2 [packed = true];</code>
       *
       * <pre>
       **
       * An ACL entry is represented by a 32-bit integer in Big Endian
       * format. The bits can be divided in four segments:
       * [0:2) || [2:26) || [26:27) || [27:29) || [29:32)
       *
       * [0:2) -- reserved for futute uses.
       * [2:26) -- the name of the entry, which is an ID that points to a
       * string in the StringTableSection.
       * [26:27) -- the scope of the entry (AclEntryScopeProto)
       * [27:29) -- the type of the entry (AclEntryTypeProto)
       * [29:32) -- the permission of the entry (FsActionProto)
       * </pre>
       */
      int getEntriesCount();
      /**
       * <code>repeated fixed32 entries = 2 [packed = true];</code>
       *
       * <pre>
       **
       * An ACL entry is represented by a 32-bit integer in Big Endian
       * format. The bits can be divided in four segments:
       * [0:2) || [2:26) || [26:27) || [27:29) || [29:32)
       *
       * [0:2) -- reserved for futute uses.
       * [2:26) -- the name of the entry, which is an ID that points to a
       * string in the StringTableSection.
       * [26:27) -- the scope of the entry (AclEntryScopeProto)
       * [27:29) -- the type of the entry (AclEntryTypeProto)
       * [29:32) -- the permission of the entry (FsActionProto)
       * </pre>
       */
      int getEntries(int index);
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.INodeSection.AclFeatureProto}
     */
    public static final class AclFeatureProto extends
        com.google.protobuf.GeneratedMessage
        implements AclFeatureProtoOrBuilder {
      // Use AclFeatureProto.newBuilder() to construct.
      private AclFeatureProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
        super(builder);
        this.unknownFields = builder.getUnknownFields();
      }
      private AclFeatureProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

      private static final AclFeatureProto defaultInstance;
      public static AclFeatureProto getDefaultInstance() {
        return defaultInstance;
      }

      public AclFeatureProto getDefaultInstanceForType() {
        return defaultInstance;
      }

      private final com.google.protobuf.UnknownFieldSet unknownFields;
      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
          getUnknownFields() {
        return this.unknownFields;
      }
      private AclFeatureProto(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        initFields();
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
              case 21: {
                if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                  entries_ = new java.util.ArrayList<java.lang.Integer>();
                  mutable_bitField0_ |= 0x00000001;
                }
                entries_.add(input.readFixed32());
                break;
              }
              case 18: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000001) == 0x00000001) && input.getBytesUntilLimit() > 0) {
                  entries_ = new java.util.ArrayList<java.lang.Integer>();
                  mutable_bitField0_ |= 0x00000001;
                }
                while (input.getBytesUntilLimit() > 0) {
                  entries_.add(input.readFixed32());
                }
                input.popLimit(limit);
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e.getMessage()).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
            entries_ = java.util.Collections.unmodifiableList(entries_);
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_AclFeatureProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_AclFeatureProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.Builder.class);
      }

      public static com.google.protobuf.Parser<AclFeatureProto> PARSER =
          new com.google.protobuf.AbstractParser<AclFeatureProto>() {
        public AclFeatureProto parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new AclFeatureProto(input, extensionRegistry);
        }
      };

      @java.lang.Override
      public com.google.protobuf.Parser<AclFeatureProto> getParserForType() {
        return PARSER;
      }

      // repeated fixed32 entries = 2 [packed = true];
      public static final int ENTRIES_FIELD_NUMBER = 2;
      private java.util.List<java.lang.Integer> entries_;
      /**
       * <code>repeated fixed32 entries = 2 [packed = true];</code>
       *
       * <pre>
       **
       * An ACL entry is represented by a 32-bit integer in Big Endian
       * format. The bits can be divided in four segments:
       * [0:2) || [2:26) || [26:27) || [27:29) || [29:32)
       *
       * [0:2) -- reserved for futute uses.
       * [2:26) -- the name of the entry, which is an ID that points to a
       * string in the StringTableSection.
       * [26:27) -- the scope of the entry (AclEntryScopeProto)
       * [27:29) -- the type of the entry (AclEntryTypeProto)
       * [29:32) -- the permission of the entry (FsActionProto)
       * </pre>
       */
      public java.util.List<java.lang.Integer>
          getEntriesList() {
        return entries_;
      }
      /**
       * <code>repeated fixed32 entries = 2 [packed = true];</code>
       *
       * <pre>
       **
       * An ACL entry is represented by a 32-bit integer in Big Endian
       * format. The bits can be divided in four segments:
       * [0:2) || [2:26) || [26:27) || [27:29) || [29:32)
       *
       * [0:2) -- reserved for futute uses.
       * [2:26) -- the name of the entry, which is an ID that points to a
       * string in the StringTableSection.
       * [26:27) -- the scope of the entry (AclEntryScopeProto)
       * [27:29) -- the type of the entry (AclEntryTypeProto)
       * [29:32) -- the permission of the entry (FsActionProto)
       * </pre>
       */
      public int getEntriesCount() {
        return entries_.size();
      }
      /**
       * <code>repeated fixed32 entries = 2 [packed = true];</code>
       *
       * <pre>
       **
       * An ACL entry is represented by a 32-bit integer in Big Endian
       * format. The bits can be divided in four segments:
       * [0:2) || [2:26) || [26:27) || [27:29) || [29:32)
       *
       * [0:2) -- reserved for futute uses.
       * [2:26) -- the name of the entry, which is an ID that points to a
       * string in the StringTableSection.
       * [26:27) -- the scope of the entry (AclEntryScopeProto)
       * [27:29) -- the type of the entry (AclEntryTypeProto)
       * [29:32) -- the permission of the entry (FsActionProto)
       * </pre>
       */
      public int getEntries(int index) {
        return entries_.get(index);
      }
      private int entriesMemoizedSerializedSize = -1;

      private void initFields() {
        entries_ = java.util.Collections.emptyList();
      }
      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized != -1) return isInitialized == 1;

        memoizedIsInitialized = 1;
        return true;
      }

      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (getEntriesList().size() > 0) {
          output.writeRawVarint32(18);
          output.writeRawVarint32(entriesMemoizedSerializedSize);
        }
        for (int i = 0; i < entries_.size(); i++) {
          output.writeFixed32NoTag(entries_.get(i));
        }
        getUnknownFields().writeTo(output);
      }

      private int memoizedSerializedSize = -1;
      public int getSerializedSize() {
        int size = memoizedSerializedSize;
        if (size != -1) return size;

        size = 0;
        {
          int dataSize = 0;
          dataSize = 4 * getEntriesList().size();
          size += dataSize;
          if (!getEntriesList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          entriesMemoizedSerializedSize = dataSize;
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSerializedSize = size;
        return size;
      }

      private static final long serialVersionUID = 0L;
      @java.lang.Override
      protected java.lang.Object writeReplace()
          throws java.io.ObjectStreamException {
        return super.writeReplace();
      }

      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }

      public static Builder newBuilder() { return Builder.create(); }
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto prototype) {
        return newBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() { return newBuilder(this); }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code hadoop.hdfs.fsimage.INodeSection.AclFeatureProto}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder>
         implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProtoOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_AclFeatureProto_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_AclFeatureProto_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.Builder.class);
        }

        // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          }
        }
        private static Builder create() {
          return new Builder();
        }

        public Builder clear() {
          super.clear();
          entries_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          return this;
        }

        public Builder clone() {
          return create().mergeFrom(buildPartial());
        }

        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_AclFeatureProto_descriptor;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto getDefaultInstanceForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.getDefaultInstance();
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto build() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto buildPartial() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto(this);
          int from_bitField0_ = bitField0_;
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            entries_ = java.util.Collections.unmodifiableList(entries_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.entries_ = entries_;
          onBuilt();
          return result;
        }

        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto) {
            return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto other) {
          if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.getDefaultInstance()) return this;
          if (!other.entries_.isEmpty()) {
            if (entries_.isEmpty()) {
              entries_ = other.entries_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureEntriesIsMutable();
              entries_.addAll(other.entries_);
            }
            onChanged();
          }
          this.mergeUnknownFields(other.getUnknownFields());
          return this;
        }

        public final boolean isInitialized() {
          return true;
        }

        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto) e.getUnfinishedMessage();
            throw e;
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        // repeated fixed32 entries = 2 [packed = true];
        private java.util.List<java.lang.Integer> entries_ = java.util.Collections.emptyList();
        private void ensureEntriesIsMutable() {
          if (!((bitField0_ & 0x00000001) == 0x00000001)) {
            entries_ = new java.util.ArrayList<java.lang.Integer>(entries_);
            bitField0_ |= 0x00000001;
           }
        }
        /**
         * <code>repeated fixed32 entries = 2 [packed = true];</code>
         *
         * <pre>
         **
         * An ACL entry is represented by a 32-bit integer in Big Endian
         * format. The bits can be divided in four segments:
         * [0:2) || [2:26) || [26:27) || [27:29) || [29:32)
         *
         * [0:2) -- reserved for futute uses.
         * [2:26) -- the name of the entry, which is an ID that points to a
         * string in the StringTableSection.
         * [26:27) -- the scope of the entry (AclEntryScopeProto)
         * [27:29) -- the type of the entry (AclEntryTypeProto)
         * [29:32) -- the permission of the entry (FsActionProto)
         * </pre>
         */
        public java.util.List<java.lang.Integer>
            getEntriesList() {
          return java.util.Collections.unmodifiableList(entries_);
        }
        /**
         * <code>repeated fixed32 entries = 2 [packed = true];</code>
         *
         * <pre>
         **
         * An ACL entry is represented by a 32-bit integer in Big Endian
         * format. The bits can be divided in four segments:
         * [0:2) || [2:26) || [26:27) || [27:29) || [29:32)
         *
         * [0:2) -- reserved for futute uses.
         * [2:26) -- the name of the entry, which is an ID that points to a
         * string in the StringTableSection.
         * [26:27) -- the scope of the entry (AclEntryScopeProto)
         * [27:29) -- the type of the entry (AclEntryTypeProto)
         * [29:32) -- the permission of the entry (FsActionProto)
         * </pre>
         */
        public int getEntriesCount() {
          return entries_.size();
        }
        /**
         * <code>repeated fixed32 entries = 2 [packed = true];</code>
         *
         * <pre>
         **
         * An ACL entry is represented by a 32-bit integer in Big Endian
         * format. The bits can be divided in four segments:
         * [0:2) || [2:26) || [26:27) || [27:29) || [29:32)
         *
         * [0:2) -- reserved for futute uses.
         * [2:26) -- the name of the entry, which is an ID that points to a
         * string in the StringTableSection.
         * [26:27) -- the scope of the entry (AclEntryScopeProto)
         * [27:29) -- the type of the entry (AclEntryTypeProto)
         * [29:32) -- the permission of the entry (FsActionProto)
         * </pre>
         */
        public int getEntries(int index) {
          return entries_.get(index);
        }
        /**
         * <code>repeated fixed32 entries = 2 [packed = true];</code>
         *
         * <pre>
         **
         * An ACL entry is represented by a 32-bit integer in Big Endian
         * format. The bits can be divided in four segments:
         * [0:2) || [2:26) || [26:27) || [27:29) || [29:32)
         *
         * [0:2) -- reserved for futute uses.
         * [2:26) -- the name of the entry, which is an ID that points to a
         * string in the StringTableSection.
         * [26:27) -- the scope of the entry (AclEntryScopeProto)
         * [27:29) -- the type of the entry (AclEntryTypeProto)
         * [29:32) -- the permission of the entry (FsActionProto)
         * </pre>
         */
        public Builder setEntries(
            int index, int value) {
          ensureEntriesIsMutable();
          entries_.set(index, value);
          onChanged();
          return this;
        }
        /**
         * <code>repeated fixed32 entries = 2 [packed = true];</code>
         *
         * <pre>
         **
         * An ACL entry is represented by a 32-bit integer in Big Endian
         * format. The bits can be divided in four segments:
         * [0:2) || [2:26) || [26:27) || [27:29) || [29:32)
         *
         * [0:2) -- reserved for futute uses.
         * [2:26) -- the name of the entry, which is an ID that points to a
         * string in the StringTableSection.
         * [26:27) -- the scope of the entry (AclEntryScopeProto)
         * [27:29) -- the type of the entry (AclEntryTypeProto)
         * [29:32) -- the permission of the entry (FsActionProto)
         * </pre>
         */
        public Builder addEntries(int value) {
          ensureEntriesIsMutable();
          entries_.add(value);
          onChanged();
          return this;
        }
        /**
         * <code>repeated fixed32 entries = 2 [packed = true];</code>
         *
         * <pre>
         **
         * An ACL entry is represented by a 32-bit integer in Big Endian
         * format. The bits can be divided in four segments:
         * [0:2) || [2:26) || [26:27) || [27:29) || [29:32)
         *
         * [0:2) -- reserved for futute uses.
         * [2:26) -- the name of the entry, which is an ID that points to a
         * string in the StringTableSection.
         * [26:27) -- the scope of the entry (AclEntryScopeProto)
         * [27:29) -- the type of the entry (AclEntryTypeProto)
         * [29:32) -- the permission of the entry (FsActionProto)
         * </pre>
         */
        public Builder addAllEntries(
            java.lang.Iterable<? extends java.lang.Integer> values) {
          ensureEntriesIsMutable();
          super.addAll(values, entries_);
          onChanged();
          return this;
        }
        /**
         * <code>repeated fixed32 entries = 2 [packed = true];</code>
         *
         * <pre>
         **
         * An ACL entry is represented by a 32-bit integer in Big Endian
         * format. The bits can be divided in four segments:
         * [0:2) || [2:26) || [26:27) || [27:29) || [29:32)
         *
         * [0:2) -- reserved for futute uses.
         * [2:26) -- the name of the entry, which is an ID that points to a
         * string in the StringTableSection.
         * [26:27) -- the scope of the entry (AclEntryScopeProto)
         * [27:29) -- the type of the entry (AclEntryTypeProto)
         * [29:32) -- the permission of the entry (FsActionProto)
         * </pre>
         */
        public Builder clearEntries() {
          entries_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.INodeSection.AclFeatureProto)
      }

      static {
        defaultInstance = new AclFeatureProto(true);
        defaultInstance.initFields();
      }

      // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.INodeSection.AclFeatureProto)
    }

    public interface INodeFileOrBuilder
        extends com.google.protobuf.MessageOrBuilder {

      // optional uint32 replication = 1;
      /**
       * <code>optional uint32 replication = 1;</code>
       */
      boolean hasReplication();
      /**
       * <code>optional uint32 replication = 1;</code>
       */
      int getReplication();

      // optional uint64 modificationTime = 2;
      /**
       * <code>optional uint64 modificationTime = 2;</code>
       */
      boolean hasModificationTime();
      /**
       * <code>optional uint64 modificationTime = 2;</code>
       */
      long getModificationTime();

      // optional uint64 accessTime = 3;
      /**
       * <code>optional uint64 accessTime = 3;</code>
       */
      boolean hasAccessTime();
      /**
       * <code>optional uint64 accessTime = 3;</code>
       */
      long getAccessTime();

      // optional uint64 preferredBlockSize = 4;
      /**
       * <code>optional uint64 preferredBlockSize = 4;</code>
       */
      boolean hasPreferredBlockSize();
      /**
       * <code>optional uint64 preferredBlockSize = 4;</code>
       */
      long getPreferredBlockSize();

      // optional fixed64 permission = 5;
      /**
       * <code>optional fixed64 permission = 5;</code>
       */
      boolean hasPermission();
      /**
       * <code>optional fixed64 permission = 5;</code>
       */
      long getPermission();

      // repeated .hadoop.hdfs.BlockProto blocks = 6;
      /**
       * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
       */
      java.util.List<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto> 
          getBlocksList();
      /**
       * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
       */
      org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto getBlocks(int index);
      /**
       * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
       */
      int getBlocksCount();
      /**
       * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
       */
      java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProtoOrBuilder> 
          getBlocksOrBuilderList();
      /**
       * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
       */
      org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProtoOrBuilder getBlocksOrBuilder(
          int index);

      // optional .hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature fileUC = 7;
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature fileUC = 7;</code>
       */
      boolean hasFileUC();
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature fileUC = 7;</code>
       */
      org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature getFileUC();
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature fileUC = 7;</code>
       */
      org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeatureOrBuilder getFileUCOrBuilder();

      // optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 8;
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 8;</code>
       */
      boolean hasAcl();
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 8;</code>
       */
      org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto getAcl();
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 8;</code>
       */
      org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProtoOrBuilder getAclOrBuilder();
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.INodeSection.INodeFile}
     */
    public static final class INodeFile extends
        com.google.protobuf.GeneratedMessage
        implements INodeFileOrBuilder {
      // Use INodeFile.newBuilder() to construct.
      private INodeFile(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
        super(builder);
        this.unknownFields = builder.getUnknownFields();
      }
      private INodeFile(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

      private static final INodeFile defaultInstance;
      public static INodeFile getDefaultInstance() {
        return defaultInstance;
      }

      public INodeFile getDefaultInstanceForType() {
        return defaultInstance;
      }

      private final com.google.protobuf.UnknownFieldSet unknownFields;
      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
          getUnknownFields() {
        return this.unknownFields;
      }
      private INodeFile(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        initFields();
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
              case 8: {
                bitField0_ |= 0x00000001;
                replication_ = input.readUInt32();
                break;
              }
              case 16: {
                bitField0_ |= 0x00000002;
                modificationTime_ = input.readUInt64();
                break;
              }
              case 24: {
                bitField0_ |= 0x00000004;
                accessTime_ = input.readUInt64();
                break;
              }
              case 32: {
                bitField0_ |= 0x00000008;
                preferredBlockSize_ = input.readUInt64();
                break;
              }
              case 41: {
                bitField0_ |= 0x00000010;
                permission_ = input.readFixed64();
                break;
              }
              case 50: {
                if (!((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
                  blocks_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto>();
                  mutable_bitField0_ |= 0x00000020;
                }
                blocks_.add(input.readMessage(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto.PARSER, extensionRegistry));
                break;
              }
              case 58: {
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature.Builder subBuilder = null;
                if (((bitField0_ & 0x00000020) == 0x00000020)) {
                  subBuilder = fileUC_.toBuilder();
                }
                fileUC_ = input.readMessage(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature.PARSER, extensionRegistry);
                if (subBuilder != null) {
                  subBuilder.mergeFrom(fileUC_);
                  fileUC_ = subBuilder.buildPartial();
                }
                bitField0_ |= 0x00000020;
                break;
              }
              case 66: {
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.Builder subBuilder = null;
                if (((bitField0_ & 0x00000040) == 0x00000040)) {
                  subBuilder = acl_.toBuilder();
                }
                acl_ = input.readMessage(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.PARSER, extensionRegistry);
                if (subBuilder != null) {
                  subBuilder.mergeFrom(acl_);
                  acl_ = subBuilder.buildPartial();
                }
                bitField0_ |= 0x00000040;
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e.getMessage()).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
            blocks_ = java.util.Collections.unmodifiableList(blocks_);
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_INodeFile_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_INodeFile_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.Builder.class);
      }

      public static com.google.protobuf.Parser<INodeFile> PARSER =
          new com.google.protobuf.AbstractParser<INodeFile>() {
        public INodeFile parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new INodeFile(input, extensionRegistry);
        }
      };

      @java.lang.Override
      public com.google.protobuf.Parser<INodeFile> getParserForType() {
        return PARSER;
      }

      private int bitField0_;
      // optional uint32 replication = 1;
      public static final int REPLICATION_FIELD_NUMBER = 1;
      private int replication_;
      /**
       * <code>optional uint32 replication = 1;</code>
       */
      public boolean hasReplication() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint32 replication = 1;</code>
       */
      public int getReplication() {
        return replication_;
      }

      // optional uint64 modificationTime = 2;
      public static final int MODIFICATIONTIME_FIELD_NUMBER = 2;
      private long modificationTime_;
      /**
       * <code>optional uint64 modificationTime = 2;</code>
       */
      public boolean hasModificationTime() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional uint64 modificationTime = 2;</code>
       */
      public long getModificationTime() {
        return modificationTime_;
      }

      // optional uint64 accessTime = 3;
      public static final int ACCESSTIME_FIELD_NUMBER = 3;
      private long accessTime_;
      /**
       * <code>optional uint64 accessTime = 3;</code>
       */
      public boolean hasAccessTime() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional uint64 accessTime = 3;</code>
       */
      public long getAccessTime() {
        return accessTime_;
      }

      // optional uint64 preferredBlockSize = 4;
      public static final int PREFERREDBLOCKSIZE_FIELD_NUMBER = 4;
      private long preferredBlockSize_;
      /**
       * <code>optional uint64 preferredBlockSize = 4;</code>
       */
      public boolean hasPreferredBlockSize() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional uint64 preferredBlockSize = 4;</code>
       */
      public long getPreferredBlockSize() {
        return preferredBlockSize_;
      }

      // optional fixed64 permission = 5;
      public static final int PERMISSION_FIELD_NUMBER = 5;
      private long permission_;
      /**
       * <code>optional fixed64 permission = 5;</code>
       */
      public boolean hasPermission() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional fixed64 permission = 5;</code>
       */
      public long getPermission() {
        return permission_;
      }

      // repeated .hadoop.hdfs.BlockProto blocks = 6;
      public static final int BLOCKS_FIELD_NUMBER = 6;
      private java.util.List<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto> blocks_;
      /**
       * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto> getBlocksList() {
        return blocks_;
      }
      /**
       * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProtoOrBuilder> 
          getBlocksOrBuilderList() {
        return blocks_;
      }
      /**
       * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
       */
      public int getBlocksCount() {
        return blocks_.size();
      }
      /**
       * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto getBlocks(int index) {
        return blocks_.get(index);
      }
      /**
       * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProtoOrBuilder getBlocksOrBuilder(
          int index) {
        return blocks_.get(index);
      }

      // optional .hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature fileUC = 7;
      public static final int FILEUC_FIELD_NUMBER = 7;
      private org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature fileUC_;
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature fileUC = 7;</code>
       */
      public boolean hasFileUC() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature fileUC = 7;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature getFileUC() {
        return fileUC_;
      }
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature fileUC = 7;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeatureOrBuilder getFileUCOrBuilder() {
        return fileUC_;
      }

      // optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 8;
      public static final int ACL_FIELD_NUMBER = 8;
      private org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto acl_;
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 8;</code>
       */
      public boolean hasAcl() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 8;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto getAcl() {
        return acl_;
      }
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 8;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProtoOrBuilder getAclOrBuilder() {
        return acl_;
      }

      private void initFields() {
        replication_ = 0;
        modificationTime_ = 0L;
        accessTime_ = 0L;
        preferredBlockSize_ = 0L;
        permission_ = 0L;
        blocks_ = java.util.Collections.emptyList();
        fileUC_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature.getDefaultInstance();
        acl_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.getDefaultInstance();
      }
      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized != -1) return isInitialized == 1;

        for (int i = 0; i < getBlocksCount(); i++) {
          if (!getBlocks(i).isInitialized()) {
            memoizedIsInitialized = 0;
            return false;
          }
        }
        memoizedIsInitialized = 1;
        return true;
      }

      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          output.writeUInt32(1, replication_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          output.writeUInt64(2, modificationTime_);
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          output.writeUInt64(3, accessTime_);
        }
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          output.writeUInt64(4, preferredBlockSize_);
        }
        if (((bitField0_ & 0x00000010) == 0x00000010)) {
          output.writeFixed64(5, permission_);
        }
        for (int i = 0; i < blocks_.size(); i++) {
          output.writeMessage(6, blocks_.get(i));
        }
        if (((bitField0_ & 0x00000020) == 0x00000020)) {
          output.writeMessage(7, fileUC_);
        }
        if (((bitField0_ & 0x00000040) == 0x00000040)) {
          output.writeMessage(8, acl_);
        }
        getUnknownFields().writeTo(output);
      }

      private int memoizedSerializedSize = -1;
      public int getSerializedSize() {
        int size = memoizedSerializedSize;
        if (size != -1) return size;

        size = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt32Size(1, replication_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(2, modificationTime_);
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(3, accessTime_);
        }
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(4, preferredBlockSize_);
        }
        if (((bitField0_ & 0x00000010) == 0x00000010)) {
          size += com.google.protobuf.CodedOutputStream
            .computeFixed64Size(5, permission_);
        }
        for (int i = 0; i < blocks_.size(); i++) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(6, blocks_.get(i));
        }
        if (((bitField0_ & 0x00000020) == 0x00000020)) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(7, fileUC_);
        }
        if (((bitField0_ & 0x00000040) == 0x00000040)) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(8, acl_);
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSerializedSize = size;
        return size;
      }

      private static final long serialVersionUID = 0L;
      @java.lang.Override
      protected java.lang.Object writeReplace()
          throws java.io.ObjectStreamException {
        return super.writeReplace();
      }

      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }

      public static Builder newBuilder() { return Builder.create(); }
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile prototype) {
        return newBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() { return newBuilder(this); }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code hadoop.hdfs.fsimage.INodeSection.INodeFile}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder>
         implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFileOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_INodeFile_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_INodeFile_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.Builder.class);
        }

        // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
            getBlocksFieldBuilder();
            getFileUCFieldBuilder();
            getAclFieldBuilder();
          }
        }
        private static Builder create() {
          return new Builder();
        }

        public Builder clear() {
          super.clear();
          replication_ = 0;
          bitField0_ = (bitField0_ & ~0x00000001);
          modificationTime_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000002);
          accessTime_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000004);
          preferredBlockSize_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000008);
          permission_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000010);
          if (blocksBuilder_ == null) {
            blocks_ = java.util.Collections.emptyList();
            bitField0_ = (bitField0_ & ~0x00000020);
          } else {
            blocksBuilder_.clear();
          }
          if (fileUCBuilder_ == null) {
            fileUC_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature.getDefaultInstance();
          } else {
            fileUCBuilder_.clear();
          }
          bitField0_ = (bitField0_ & ~0x00000040);
          if (aclBuilder_ == null) {
            acl_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.getDefaultInstance();
          } else {
            aclBuilder_.clear();
          }
          bitField0_ = (bitField0_ & ~0x00000080);
          return this;
        }

        public Builder clone() {
          return create().mergeFrom(buildPartial());
        }

        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_INodeFile_descriptor;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile getDefaultInstanceForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.getDefaultInstance();
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile build() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile buildPartial() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
            to_bitField0_ |= 0x00000001;
          }
          result.replication_ = replication_;
          if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
            to_bitField0_ |= 0x00000002;
          }
          result.modificationTime_ = modificationTime_;
          if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
            to_bitField0_ |= 0x00000004;
          }
          result.accessTime_ = accessTime_;
          if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
            to_bitField0_ |= 0x00000008;
          }
          result.preferredBlockSize_ = preferredBlockSize_;
          if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
            to_bitField0_ |= 0x00000010;
          }
          result.permission_ = permission_;
          if (blocksBuilder_ == null) {
            if (((bitField0_ & 0x00000020) == 0x00000020)) {
              blocks_ = java.util.Collections.unmodifiableList(blocks_);
              bitField0_ = (bitField0_ & ~0x00000020);
            }
            result.blocks_ = blocks_;
          } else {
            result.blocks_ = blocksBuilder_.build();
          }
          if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
            to_bitField0_ |= 0x00000020;
          }
          if (fileUCBuilder_ == null) {
            result.fileUC_ = fileUC_;
          } else {
            result.fileUC_ = fileUCBuilder_.build();
          }
          if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
            to_bitField0_ |= 0x00000040;
          }
          if (aclBuilder_ == null) {
            result.acl_ = acl_;
          } else {
            result.acl_ = aclBuilder_.build();
          }
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile) {
            return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile other) {
          if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.getDefaultInstance()) return this;
          if (other.hasReplication()) {
            setReplication(other.getReplication());
          }
          if (other.hasModificationTime()) {
            setModificationTime(other.getModificationTime());
          }
          if (other.hasAccessTime()) {
            setAccessTime(other.getAccessTime());
          }
          if (other.hasPreferredBlockSize()) {
            setPreferredBlockSize(other.getPreferredBlockSize());
          }
          if (other.hasPermission()) {
            setPermission(other.getPermission());
          }
          if (blocksBuilder_ == null) {
            if (!other.blocks_.isEmpty()) {
              if (blocks_.isEmpty()) {
                blocks_ = other.blocks_;
                bitField0_ = (bitField0_ & ~0x00000020);
              } else {
                ensureBlocksIsMutable();
                blocks_.addAll(other.blocks_);
              }
              onChanged();
            }
          } else {
            if (!other.blocks_.isEmpty()) {
              if (blocksBuilder_.isEmpty()) {
                blocksBuilder_.dispose();
                blocksBuilder_ = null;
                blocks_ = other.blocks_;
                bitField0_ = (bitField0_ & ~0x00000020);
                blocksBuilder_ = 
                  com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                     getBlocksFieldBuilder() : null;
              } else {
                blocksBuilder_.addAllMessages(other.blocks_);
              }
            }
          }
          if (other.hasFileUC()) {
            mergeFileUC(other.getFileUC());
          }
          if (other.hasAcl()) {
            mergeAcl(other.getAcl());
          }
          this.mergeUnknownFields(other.getUnknownFields());
          return this;
        }

        public final boolean isInitialized() {
          for (int i = 0; i < getBlocksCount(); i++) {
            if (!getBlocks(i).isInitialized()) {
              
              return false;
            }
          }
          return true;
        }

        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile) e.getUnfinishedMessage();
            throw e;
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        // optional uint32 replication = 1;
        private int replication_ ;
        /**
         * <code>optional uint32 replication = 1;</code>
         */
        public boolean hasReplication() {
          return ((bitField0_ & 0x00000001) == 0x00000001);
        }
        /**
         * <code>optional uint32 replication = 1;</code>
         */
        public int getReplication() {
          return replication_;
        }
        /**
         * <code>optional uint32 replication = 1;</code>
         */
        public Builder setReplication(int value) {
          bitField0_ |= 0x00000001;
          replication_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint32 replication = 1;</code>
         */
        public Builder clearReplication() {
          bitField0_ = (bitField0_ & ~0x00000001);
          replication_ = 0;
          onChanged();
          return this;
        }

        // optional uint64 modificationTime = 2;
        private long modificationTime_ ;
        /**
         * <code>optional uint64 modificationTime = 2;</code>
         */
        public boolean hasModificationTime() {
          return ((bitField0_ & 0x00000002) == 0x00000002);
        }
        /**
         * <code>optional uint64 modificationTime = 2;</code>
         */
        public long getModificationTime() {
          return modificationTime_;
        }
        /**
         * <code>optional uint64 modificationTime = 2;</code>
         */
        public Builder setModificationTime(long value) {
          bitField0_ |= 0x00000002;
          modificationTime_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 modificationTime = 2;</code>
         */
        public Builder clearModificationTime() {
          bitField0_ = (bitField0_ & ~0x00000002);
          modificationTime_ = 0L;
          onChanged();
          return this;
        }

        // optional uint64 accessTime = 3;
        private long accessTime_ ;
        /**
         * <code>optional uint64 accessTime = 3;</code>
         */
        public boolean hasAccessTime() {
          return ((bitField0_ & 0x00000004) == 0x00000004);
        }
        /**
         * <code>optional uint64 accessTime = 3;</code>
         */
        public long getAccessTime() {
          return accessTime_;
        }
        /**
         * <code>optional uint64 accessTime = 3;</code>
         */
        public Builder setAccessTime(long value) {
          bitField0_ |= 0x00000004;
          accessTime_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 accessTime = 3;</code>
         */
        public Builder clearAccessTime() {
          bitField0_ = (bitField0_ & ~0x00000004);
          accessTime_ = 0L;
          onChanged();
          return this;
        }

        // optional uint64 preferredBlockSize = 4;
        private long preferredBlockSize_ ;
        /**
         * <code>optional uint64 preferredBlockSize = 4;</code>
         */
        public boolean hasPreferredBlockSize() {
          return ((bitField0_ & 0x00000008) == 0x00000008);
        }
        /**
         * <code>optional uint64 preferredBlockSize = 4;</code>
         */
        public long getPreferredBlockSize() {
          return preferredBlockSize_;
        }
        /**
         * <code>optional uint64 preferredBlockSize = 4;</code>
         */
        public Builder setPreferredBlockSize(long value) {
          bitField0_ |= 0x00000008;
          preferredBlockSize_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 preferredBlockSize = 4;</code>
         */
        public Builder clearPreferredBlockSize() {
          bitField0_ = (bitField0_ & ~0x00000008);
          preferredBlockSize_ = 0L;
          onChanged();
          return this;
        }

        // optional fixed64 permission = 5;
        private long permission_ ;
        /**
         * <code>optional fixed64 permission = 5;</code>
         */
        public boolean hasPermission() {
          return ((bitField0_ & 0x00000010) == 0x00000010);
        }
        /**
         * <code>optional fixed64 permission = 5;</code>
         */
        public long getPermission() {
          return permission_;
        }
        /**
         * <code>optional fixed64 permission = 5;</code>
         */
        public Builder setPermission(long value) {
          bitField0_ |= 0x00000010;
          permission_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional fixed64 permission = 5;</code>
         */
        public Builder clearPermission() {
          bitField0_ = (bitField0_ & ~0x00000010);
          permission_ = 0L;
          onChanged();
          return this;
        }

        // repeated .hadoop.hdfs.BlockProto blocks = 6;
        private java.util.List<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto> blocks_ =
          java.util.Collections.emptyList();
        private void ensureBlocksIsMutable() {
          if (!((bitField0_ & 0x00000020) == 0x00000020)) {
            blocks_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto>(blocks_);
            bitField0_ |= 0x00000020;
           }
        }

        private com.google.protobuf.RepeatedFieldBuilder<
            org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto.Builder, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProtoOrBuilder> blocksBuilder_;

        /**
         * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
         */
        public java.util.List<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto> getBlocksList() {
          if (blocksBuilder_ == null) {
            return java.util.Collections.unmodifiableList(blocks_);
          } else {
            return blocksBuilder_.getMessageList();
          }
        }
        /**
         * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
         */
        public int getBlocksCount() {
          if (blocksBuilder_ == null) {
            return blocks_.size();
          } else {
            return blocksBuilder_.getCount();
          }
        }
        /**
         * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
         */
        public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto getBlocks(int index) {
          if (blocksBuilder_ == null) {
            return blocks_.get(index);
          } else {
            return blocksBuilder_.getMessage(index);
          }
        }
        /**
         * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
         */
        public Builder setBlocks(
            int index, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto value) {
          if (blocksBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureBlocksIsMutable();
            blocks_.set(index, value);
            onChanged();
          } else {
            blocksBuilder_.setMessage(index, value);
          }
          return this;
        }
        /**
         * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
         */
        public Builder setBlocks(
            int index, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto.Builder builderForValue) {
          if (blocksBuilder_ == null) {
            ensureBlocksIsMutable();
            blocks_.set(index, builderForValue.build());
            onChanged();
          } else {
            blocksBuilder_.setMessage(index, builderForValue.build());
          }
          return this;
        }
        /**
         * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
         */
        public Builder addBlocks(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto value) {
          if (blocksBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureBlocksIsMutable();
            blocks_.add(value);
            onChanged();
          } else {
            blocksBuilder_.addMessage(value);
          }
          return this;
        }
        /**
         * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
         */
        public Builder addBlocks(
            int index, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto value) {
          if (blocksBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureBlocksIsMutable();
            blocks_.add(index, value);
            onChanged();
          } else {
            blocksBuilder_.addMessage(index, value);
          }
          return this;
        }
        /**
         * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
         */
        public Builder addBlocks(
            org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto.Builder builderForValue) {
          if (blocksBuilder_ == null) {
            ensureBlocksIsMutable();
            blocks_.add(builderForValue.build());
            onChanged();
          } else {
            blocksBuilder_.addMessage(builderForValue.build());
          }
          return this;
        }
        /**
         * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
         */
        public Builder addBlocks(
            int index, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto.Builder builderForValue) {
          if (blocksBuilder_ == null) {
            ensureBlocksIsMutable();
            blocks_.add(index, builderForValue.build());
            onChanged();
          } else {
            blocksBuilder_.addMessage(index, builderForValue.build());
          }
          return this;
        }
        /**
         * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
         */
        public Builder addAllBlocks(
            java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto> values) {
          if (blocksBuilder_ == null) {
            ensureBlocksIsMutable();
            super.addAll(values, blocks_);
            onChanged();
          } else {
            blocksBuilder_.addAllMessages(values);
          }
          return this;
        }
        /**
         * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
         */
        public Builder clearBlocks() {
          if (blocksBuilder_ == null) {
            blocks_ = java.util.Collections.emptyList();
            bitField0_ = (bitField0_ & ~0x00000020);
            onChanged();
          } else {
            blocksBuilder_.clear();
          }
          return this;
        }
        /**
         * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
         */
        public Builder removeBlocks(int index) {
          if (blocksBuilder_ == null) {
            ensureBlocksIsMutable();
            blocks_.remove(index);
            onChanged();
          } else {
            blocksBuilder_.remove(index);
          }
          return this;
        }
        /**
         * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
         */
        public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto.Builder getBlocksBuilder(
            int index) {
          return getBlocksFieldBuilder().getBuilder(index);
        }
        /**
         * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
         */
        public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProtoOrBuilder getBlocksOrBuilder(
            int index) {
          if (blocksBuilder_ == null) {
            return blocks_.get(index);  } else {
            return blocksBuilder_.getMessageOrBuilder(index);
          }
        }
        /**
         * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
         */
        public java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProtoOrBuilder> 
             getBlocksOrBuilderList() {
          if (blocksBuilder_ != null) {
            return blocksBuilder_.getMessageOrBuilderList();
          } else {
            return java.util.Collections.unmodifiableList(blocks_);
          }
        }
        /**
         * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
         */
        public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto.Builder addBlocksBuilder() {
          return getBlocksFieldBuilder().addBuilder(
              org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto.getDefaultInstance());
        }
        /**
         * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
         */
        public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto.Builder addBlocksBuilder(
            int index) {
          return getBlocksFieldBuilder().addBuilder(
              index, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto.getDefaultInstance());
        }
        /**
         * <code>repeated .hadoop.hdfs.BlockProto blocks = 6;</code>
         */
        public java.util.List<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto.Builder> 
             getBlocksBuilderList() {
          return getBlocksFieldBuilder().getBuilderList();
        }
        private com.google.protobuf.RepeatedFieldBuilder<
            org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto.Builder, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProtoOrBuilder> 
            getBlocksFieldBuilder() {
          if (blocksBuilder_ == null) {
            blocksBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
                org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProto.Builder, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.BlockProtoOrBuilder>(
                    blocks_,
                    ((bitField0_ & 0x00000020) == 0x00000020),
                    getParentForChildren(),
                    isClean());
            blocks_ = null;
          }
          return blocksBuilder_;
        }

        // optional .hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature fileUC = 7;
        private org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature fileUC_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature.getDefaultInstance();
        private com.google.protobuf.SingleFieldBuilder<
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeatureOrBuilder> fileUCBuilder_;
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature fileUC = 7;</code>
         */
        public boolean hasFileUC() {
          return ((bitField0_ & 0x00000040) == 0x00000040);
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature fileUC = 7;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature getFileUC() {
          if (fileUCBuilder_ == null) {
            return fileUC_;
          } else {
            return fileUCBuilder_.getMessage();
          }
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature fileUC = 7;</code>
         */
        public Builder setFileUC(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature value) {
          if (fileUCBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            fileUC_ = value;
            onChanged();
          } else {
            fileUCBuilder_.setMessage(value);
          }
          bitField0_ |= 0x00000040;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature fileUC = 7;</code>
         */
        public Builder setFileUC(
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature.Builder builderForValue) {
          if (fileUCBuilder_ == null) {
            fileUC_ = builderForValue.build();
            onChanged();
          } else {
            fileUCBuilder_.setMessage(builderForValue.build());
          }
          bitField0_ |= 0x00000040;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature fileUC = 7;</code>
         */
        public Builder mergeFileUC(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature value) {
          if (fileUCBuilder_ == null) {
            if (((bitField0_ & 0x00000040) == 0x00000040) &&
                fileUC_ != org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature.getDefaultInstance()) {
              fileUC_ =
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature.newBuilder(fileUC_).mergeFrom(value).buildPartial();
            } else {
              fileUC_ = value;
            }
            onChanged();
          } else {
            fileUCBuilder_.mergeFrom(value);
          }
          bitField0_ |= 0x00000040;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature fileUC = 7;</code>
         */
        public Builder clearFileUC() {
          if (fileUCBuilder_ == null) {
            fileUC_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature.getDefaultInstance();
            onChanged();
          } else {
            fileUCBuilder_.clear();
          }
          bitField0_ = (bitField0_ & ~0x00000040);
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature fileUC = 7;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature.Builder getFileUCBuilder() {
          bitField0_ |= 0x00000040;
          onChanged();
          return getFileUCFieldBuilder().getBuilder();
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature fileUC = 7;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeatureOrBuilder getFileUCOrBuilder() {
          if (fileUCBuilder_ != null) {
            return fileUCBuilder_.getMessageOrBuilder();
          } else {
            return fileUC_;
          }
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.FileUnderConstructionFeature fileUC = 7;</code>
         */
        private com.google.protobuf.SingleFieldBuilder<
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeatureOrBuilder> 
            getFileUCFieldBuilder() {
          if (fileUCBuilder_ == null) {
            fileUCBuilder_ = new com.google.protobuf.SingleFieldBuilder<
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeature.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.FileUnderConstructionFeatureOrBuilder>(
                    fileUC_,
                    getParentForChildren(),
                    isClean());
            fileUC_ = null;
          }
          return fileUCBuilder_;
        }

        // optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 8;
        private org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto acl_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.getDefaultInstance();
        private com.google.protobuf.SingleFieldBuilder<
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProtoOrBuilder> aclBuilder_;
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 8;</code>
         */
        public boolean hasAcl() {
          return ((bitField0_ & 0x00000080) == 0x00000080);
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 8;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto getAcl() {
          if (aclBuilder_ == null) {
            return acl_;
          } else {
            return aclBuilder_.getMessage();
          }
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 8;</code>
         */
        public Builder setAcl(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto value) {
          if (aclBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            acl_ = value;
            onChanged();
          } else {
            aclBuilder_.setMessage(value);
          }
          bitField0_ |= 0x00000080;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 8;</code>
         */
        public Builder setAcl(
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.Builder builderForValue) {
          if (aclBuilder_ == null) {
            acl_ = builderForValue.build();
            onChanged();
          } else {
            aclBuilder_.setMessage(builderForValue.build());
          }
          bitField0_ |= 0x00000080;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 8;</code>
         */
        public Builder mergeAcl(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto value) {
          if (aclBuilder_ == null) {
            if (((bitField0_ & 0x00000080) == 0x00000080) &&
                acl_ != org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.getDefaultInstance()) {
              acl_ =
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.newBuilder(acl_).mergeFrom(value).buildPartial();
            } else {
              acl_ = value;
            }
            onChanged();
          } else {
            aclBuilder_.mergeFrom(value);
          }
          bitField0_ |= 0x00000080;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 8;</code>
         */
        public Builder clearAcl() {
          if (aclBuilder_ == null) {
            acl_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.getDefaultInstance();
            onChanged();
          } else {
            aclBuilder_.clear();
          }
          bitField0_ = (bitField0_ & ~0x00000080);
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 8;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.Builder getAclBuilder() {
          bitField0_ |= 0x00000080;
          onChanged();
          return getAclFieldBuilder().getBuilder();
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 8;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProtoOrBuilder getAclOrBuilder() {
          if (aclBuilder_ != null) {
            return aclBuilder_.getMessageOrBuilder();
          } else {
            return acl_;
          }
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 8;</code>
         */
        private com.google.protobuf.SingleFieldBuilder<
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProtoOrBuilder> 
            getAclFieldBuilder() {
          if (aclBuilder_ == null) {
            aclBuilder_ = new com.google.protobuf.SingleFieldBuilder<
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProtoOrBuilder>(
                    acl_,
                    getParentForChildren(),
                    isClean());
            acl_ = null;
          }
          return aclBuilder_;
        }

        // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.INodeSection.INodeFile)
      }

      static {
        defaultInstance = new INodeFile(true);
        defaultInstance.initFields();
      }

      // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.INodeSection.INodeFile)
    }

    public interface INodeDirectoryOrBuilder
        extends com.google.protobuf.MessageOrBuilder {

      // optional uint64 modificationTime = 1;
      /**
       * <code>optional uint64 modificationTime = 1;</code>
       */
      boolean hasModificationTime();
      /**
       * <code>optional uint64 modificationTime = 1;</code>
       */
      long getModificationTime();

      // optional uint64 nsQuota = 2;
      /**
       * <code>optional uint64 nsQuota = 2;</code>
       *
       * <pre>
       * namespace quota
       * </pre>
       */
      boolean hasNsQuota();
      /**
       * <code>optional uint64 nsQuota = 2;</code>
       *
       * <pre>
       * namespace quota
       * </pre>
       */
      long getNsQuota();

      // optional uint64 dsQuota = 3;
      /**
       * <code>optional uint64 dsQuota = 3;</code>
       *
       * <pre>
       * diskspace quota
       * </pre>
       */
      boolean hasDsQuota();
      /**
       * <code>optional uint64 dsQuota = 3;</code>
       *
       * <pre>
       * diskspace quota
       * </pre>
       */
      long getDsQuota();

      // optional fixed64 permission = 4;
      /**
       * <code>optional fixed64 permission = 4;</code>
       */
      boolean hasPermission();
      /**
       * <code>optional fixed64 permission = 4;</code>
       */
      long getPermission();

      // optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 5;
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 5;</code>
       */
      boolean hasAcl();
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 5;</code>
       */
      org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto getAcl();
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 5;</code>
       */
      org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProtoOrBuilder getAclOrBuilder();
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.INodeSection.INodeDirectory}
     */
    public static final class INodeDirectory extends
        com.google.protobuf.GeneratedMessage
        implements INodeDirectoryOrBuilder {
      // Use INodeDirectory.newBuilder() to construct.
      private INodeDirectory(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
        super(builder);
        this.unknownFields = builder.getUnknownFields();
      }
      private INodeDirectory(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

      private static final INodeDirectory defaultInstance;
      public static INodeDirectory getDefaultInstance() {
        return defaultInstance;
      }

      public INodeDirectory getDefaultInstanceForType() {
        return defaultInstance;
      }

      private final com.google.protobuf.UnknownFieldSet unknownFields;
      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
          getUnknownFields() {
        return this.unknownFields;
      }
      private INodeDirectory(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        initFields();
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
              case 8: {
                bitField0_ |= 0x00000001;
                modificationTime_ = input.readUInt64();
                break;
              }
              case 16: {
                bitField0_ |= 0x00000002;
                nsQuota_ = input.readUInt64();
                break;
              }
              case 24: {
                bitField0_ |= 0x00000004;
                dsQuota_ = input.readUInt64();
                break;
              }
              case 33: {
                bitField0_ |= 0x00000008;
                permission_ = input.readFixed64();
                break;
              }
              case 42: {
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.Builder subBuilder = null;
                if (((bitField0_ & 0x00000010) == 0x00000010)) {
                  subBuilder = acl_.toBuilder();
                }
                acl_ = input.readMessage(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.PARSER, extensionRegistry);
                if (subBuilder != null) {
                  subBuilder.mergeFrom(acl_);
                  acl_ = subBuilder.buildPartial();
                }
                bitField0_ |= 0x00000010;
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e.getMessage()).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_INodeDirectory_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_INodeDirectory_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.Builder.class);
      }

      public static com.google.protobuf.Parser<INodeDirectory> PARSER =
          new com.google.protobuf.AbstractParser<INodeDirectory>() {
        public INodeDirectory parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new INodeDirectory(input, extensionRegistry);
        }
      };

      @java.lang.Override
      public com.google.protobuf.Parser<INodeDirectory> getParserForType() {
        return PARSER;
      }

      private int bitField0_;
      // optional uint64 modificationTime = 1;
      public static final int MODIFICATIONTIME_FIELD_NUMBER = 1;
      private long modificationTime_;
      /**
       * <code>optional uint64 modificationTime = 1;</code>
       */
      public boolean hasModificationTime() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint64 modificationTime = 1;</code>
       */
      public long getModificationTime() {
        return modificationTime_;
      }

      // optional uint64 nsQuota = 2;
      public static final int NSQUOTA_FIELD_NUMBER = 2;
      private long nsQuota_;
      /**
       * <code>optional uint64 nsQuota = 2;</code>
       *
       * <pre>
       * namespace quota
       * </pre>
       */
      public boolean hasNsQuota() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional uint64 nsQuota = 2;</code>
       *
       * <pre>
       * namespace quota
       * </pre>
       */
      public long getNsQuota() {
        return nsQuota_;
      }

      // optional uint64 dsQuota = 3;
      public static final int DSQUOTA_FIELD_NUMBER = 3;
      private long dsQuota_;
      /**
       * <code>optional uint64 dsQuota = 3;</code>
       *
       * <pre>
       * diskspace quota
       * </pre>
       */
      public boolean hasDsQuota() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional uint64 dsQuota = 3;</code>
       *
       * <pre>
       * diskspace quota
       * </pre>
       */
      public long getDsQuota() {
        return dsQuota_;
      }

      // optional fixed64 permission = 4;
      public static final int PERMISSION_FIELD_NUMBER = 4;
      private long permission_;
      /**
       * <code>optional fixed64 permission = 4;</code>
       */
      public boolean hasPermission() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional fixed64 permission = 4;</code>
       */
      public long getPermission() {
        return permission_;
      }

      // optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 5;
      public static final int ACL_FIELD_NUMBER = 5;
      private org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto acl_;
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 5;</code>
       */
      public boolean hasAcl() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 5;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto getAcl() {
        return acl_;
      }
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 5;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProtoOrBuilder getAclOrBuilder() {
        return acl_;
      }

      private void initFields() {
        modificationTime_ = 0L;
        nsQuota_ = 0L;
        dsQuota_ = 0L;
        permission_ = 0L;
        acl_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.getDefaultInstance();
      }
      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized != -1) return isInitialized == 1;

        memoizedIsInitialized = 1;
        return true;
      }

      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          output.writeUInt64(1, modificationTime_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          output.writeUInt64(2, nsQuota_);
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          output.writeUInt64(3, dsQuota_);
        }
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          output.writeFixed64(4, permission_);
        }
        if (((bitField0_ & 0x00000010) == 0x00000010)) {
          output.writeMessage(5, acl_);
        }
        getUnknownFields().writeTo(output);
      }

      private int memoizedSerializedSize = -1;
      public int getSerializedSize() {
        int size = memoizedSerializedSize;
        if (size != -1) return size;

        size = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(1, modificationTime_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(2, nsQuota_);
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(3, dsQuota_);
        }
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          size += com.google.protobuf.CodedOutputStream
            .computeFixed64Size(4, permission_);
        }
        if (((bitField0_ & 0x00000010) == 0x00000010)) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(5, acl_);
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSerializedSize = size;
        return size;
      }

      private static final long serialVersionUID = 0L;
      @java.lang.Override
      protected java.lang.Object writeReplace()
          throws java.io.ObjectStreamException {
        return super.writeReplace();
      }

      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }

      public static Builder newBuilder() { return Builder.create(); }
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory prototype) {
        return newBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() { return newBuilder(this); }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code hadoop.hdfs.fsimage.INodeSection.INodeDirectory}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder>
         implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectoryOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_INodeDirectory_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_INodeDirectory_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.Builder.class);
        }

        // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
            getAclFieldBuilder();
          }
        }
        private static Builder create() {
          return new Builder();
        }

        public Builder clear() {
          super.clear();
          modificationTime_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000001);
          nsQuota_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000002);
          dsQuota_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000004);
          permission_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000008);
          if (aclBuilder_ == null) {
            acl_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.getDefaultInstance();
          } else {
            aclBuilder_.clear();
          }
          bitField0_ = (bitField0_ & ~0x00000010);
          return this;
        }

        public Builder clone() {
          return create().mergeFrom(buildPartial());
        }

        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_INodeDirectory_descriptor;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory getDefaultInstanceForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.getDefaultInstance();
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory build() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory buildPartial() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
            to_bitField0_ |= 0x00000001;
          }
          result.modificationTime_ = modificationTime_;
          if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
            to_bitField0_ |= 0x00000002;
          }
          result.nsQuota_ = nsQuota_;
          if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
            to_bitField0_ |= 0x00000004;
          }
          result.dsQuota_ = dsQuota_;
          if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
            to_bitField0_ |= 0x00000008;
          }
          result.permission_ = permission_;
          if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
            to_bitField0_ |= 0x00000010;
          }
          if (aclBuilder_ == null) {
            result.acl_ = acl_;
          } else {
            result.acl_ = aclBuilder_.build();
          }
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory) {
            return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory other) {
          if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.getDefaultInstance()) return this;
          if (other.hasModificationTime()) {
            setModificationTime(other.getModificationTime());
          }
          if (other.hasNsQuota()) {
            setNsQuota(other.getNsQuota());
          }
          if (other.hasDsQuota()) {
            setDsQuota(other.getDsQuota());
          }
          if (other.hasPermission()) {
            setPermission(other.getPermission());
          }
          if (other.hasAcl()) {
            mergeAcl(other.getAcl());
          }
          this.mergeUnknownFields(other.getUnknownFields());
          return this;
        }

        public final boolean isInitialized() {
          return true;
        }

        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory) e.getUnfinishedMessage();
            throw e;
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        // optional uint64 modificationTime = 1;
        private long modificationTime_ ;
        /**
         * <code>optional uint64 modificationTime = 1;</code>
         */
        public boolean hasModificationTime() {
          return ((bitField0_ & 0x00000001) == 0x00000001);
        }
        /**
         * <code>optional uint64 modificationTime = 1;</code>
         */
        public long getModificationTime() {
          return modificationTime_;
        }
        /**
         * <code>optional uint64 modificationTime = 1;</code>
         */
        public Builder setModificationTime(long value) {
          bitField0_ |= 0x00000001;
          modificationTime_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 modificationTime = 1;</code>
         */
        public Builder clearModificationTime() {
          bitField0_ = (bitField0_ & ~0x00000001);
          modificationTime_ = 0L;
          onChanged();
          return this;
        }

        // optional uint64 nsQuota = 2;
        private long nsQuota_ ;
        /**
         * <code>optional uint64 nsQuota = 2;</code>
         *
         * <pre>
         * namespace quota
         * </pre>
         */
        public boolean hasNsQuota() {
          return ((bitField0_ & 0x00000002) == 0x00000002);
        }
        /**
         * <code>optional uint64 nsQuota = 2;</code>
         *
         * <pre>
         * namespace quota
         * </pre>
         */
        public long getNsQuota() {
          return nsQuota_;
        }
        /**
         * <code>optional uint64 nsQuota = 2;</code>
         *
         * <pre>
         * namespace quota
         * </pre>
         */
        public Builder setNsQuota(long value) {
          bitField0_ |= 0x00000002;
          nsQuota_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 nsQuota = 2;</code>
         *
         * <pre>
         * namespace quota
         * </pre>
         */
        public Builder clearNsQuota() {
          bitField0_ = (bitField0_ & ~0x00000002);
          nsQuota_ = 0L;
          onChanged();
          return this;
        }

        // optional uint64 dsQuota = 3;
        private long dsQuota_ ;
        /**
         * <code>optional uint64 dsQuota = 3;</code>
         *
         * <pre>
         * diskspace quota
         * </pre>
         */
        public boolean hasDsQuota() {
          return ((bitField0_ & 0x00000004) == 0x00000004);
        }
        /**
         * <code>optional uint64 dsQuota = 3;</code>
         *
         * <pre>
         * diskspace quota
         * </pre>
         */
        public long getDsQuota() {
          return dsQuota_;
        }
        /**
         * <code>optional uint64 dsQuota = 3;</code>
         *
         * <pre>
         * diskspace quota
         * </pre>
         */
        public Builder setDsQuota(long value) {
          bitField0_ |= 0x00000004;
          dsQuota_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 dsQuota = 3;</code>
         *
         * <pre>
         * diskspace quota
         * </pre>
         */
        public Builder clearDsQuota() {
          bitField0_ = (bitField0_ & ~0x00000004);
          dsQuota_ = 0L;
          onChanged();
          return this;
        }

        // optional fixed64 permission = 4;
        private long permission_ ;
        /**
         * <code>optional fixed64 permission = 4;</code>
         */
        public boolean hasPermission() {
          return ((bitField0_ & 0x00000008) == 0x00000008);
        }
        /**
         * <code>optional fixed64 permission = 4;</code>
         */
        public long getPermission() {
          return permission_;
        }
        /**
         * <code>optional fixed64 permission = 4;</code>
         */
        public Builder setPermission(long value) {
          bitField0_ |= 0x00000008;
          permission_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional fixed64 permission = 4;</code>
         */
        public Builder clearPermission() {
          bitField0_ = (bitField0_ & ~0x00000008);
          permission_ = 0L;
          onChanged();
          return this;
        }

        // optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 5;
        private org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto acl_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.getDefaultInstance();
        private com.google.protobuf.SingleFieldBuilder<
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProtoOrBuilder> aclBuilder_;
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 5;</code>
         */
        public boolean hasAcl() {
          return ((bitField0_ & 0x00000010) == 0x00000010);
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 5;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto getAcl() {
          if (aclBuilder_ == null) {
            return acl_;
          } else {
            return aclBuilder_.getMessage();
          }
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 5;</code>
         */
        public Builder setAcl(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto value) {
          if (aclBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            acl_ = value;
            onChanged();
          } else {
            aclBuilder_.setMessage(value);
          }
          bitField0_ |= 0x00000010;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 5;</code>
         */
        public Builder setAcl(
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.Builder builderForValue) {
          if (aclBuilder_ == null) {
            acl_ = builderForValue.build();
            onChanged();
          } else {
            aclBuilder_.setMessage(builderForValue.build());
          }
          bitField0_ |= 0x00000010;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 5;</code>
         */
        public Builder mergeAcl(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto value) {
          if (aclBuilder_ == null) {
            if (((bitField0_ & 0x00000010) == 0x00000010) &&
                acl_ != org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.getDefaultInstance()) {
              acl_ =
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.newBuilder(acl_).mergeFrom(value).buildPartial();
            } else {
              acl_ = value;
            }
            onChanged();
          } else {
            aclBuilder_.mergeFrom(value);
          }
          bitField0_ |= 0x00000010;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 5;</code>
         */
        public Builder clearAcl() {
          if (aclBuilder_ == null) {
            acl_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.getDefaultInstance();
            onChanged();
          } else {
            aclBuilder_.clear();
          }
          bitField0_ = (bitField0_ & ~0x00000010);
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 5;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.Builder getAclBuilder() {
          bitField0_ |= 0x00000010;
          onChanged();
          return getAclFieldBuilder().getBuilder();
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 5;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProtoOrBuilder getAclOrBuilder() {
          if (aclBuilder_ != null) {
            return aclBuilder_.getMessageOrBuilder();
          } else {
            return acl_;
          }
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.AclFeatureProto acl = 5;</code>
         */
        private com.google.protobuf.SingleFieldBuilder<
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProtoOrBuilder> 
            getAclFieldBuilder() {
          if (aclBuilder_ == null) {
            aclBuilder_ = new com.google.protobuf.SingleFieldBuilder<
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProto.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.AclFeatureProtoOrBuilder>(
                    acl_,
                    getParentForChildren(),
                    isClean());
            acl_ = null;
          }
          return aclBuilder_;
        }

        // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.INodeSection.INodeDirectory)
      }

      static {
        defaultInstance = new INodeDirectory(true);
        defaultInstance.initFields();
      }

      // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.INodeSection.INodeDirectory)
    }

    public interface INodeSymlinkOrBuilder
        extends com.google.protobuf.MessageOrBuilder {

      // optional fixed64 permission = 1;
      /**
       * <code>optional fixed64 permission = 1;</code>
       */
      boolean hasPermission();
      /**
       * <code>optional fixed64 permission = 1;</code>
       */
      long getPermission();

      // optional bytes target = 2;
      /**
       * <code>optional bytes target = 2;</code>
       */
      boolean hasTarget();
      /**
       * <code>optional bytes target = 2;</code>
       */
      com.google.protobuf.ByteString getTarget();

      // optional uint64 modificationTime = 3;
      /**
       * <code>optional uint64 modificationTime = 3;</code>
       */
      boolean hasModificationTime();
      /**
       * <code>optional uint64 modificationTime = 3;</code>
       */
      long getModificationTime();

      // optional uint64 accessTime = 4;
      /**
       * <code>optional uint64 accessTime = 4;</code>
       */
      boolean hasAccessTime();
      /**
       * <code>optional uint64 accessTime = 4;</code>
       */
      long getAccessTime();
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.INodeSection.INodeSymlink}
     */
    public static final class INodeSymlink extends
        com.google.protobuf.GeneratedMessage
        implements INodeSymlinkOrBuilder {
      // Use INodeSymlink.newBuilder() to construct.
      private INodeSymlink(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
        super(builder);
        this.unknownFields = builder.getUnknownFields();
      }
      private INodeSymlink(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

      private static final INodeSymlink defaultInstance;
      public static INodeSymlink getDefaultInstance() {
        return defaultInstance;
      }

      public INodeSymlink getDefaultInstanceForType() {
        return defaultInstance;
      }

      private final com.google.protobuf.UnknownFieldSet unknownFields;
      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
          getUnknownFields() {
        return this.unknownFields;
      }
      private INodeSymlink(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        initFields();
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
              case 9: {
                bitField0_ |= 0x00000001;
                permission_ = input.readFixed64();
                break;
              }
              case 18: {
                bitField0_ |= 0x00000002;
                target_ = input.readBytes();
                break;
              }
              case 24: {
                bitField0_ |= 0x00000004;
                modificationTime_ = input.readUInt64();
                break;
              }
              case 32: {
                bitField0_ |= 0x00000008;
                accessTime_ = input.readUInt64();
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e.getMessage()).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_INodeSymlink_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_INodeSymlink_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink.Builder.class);
      }

      public static com.google.protobuf.Parser<INodeSymlink> PARSER =
          new com.google.protobuf.AbstractParser<INodeSymlink>() {
        public INodeSymlink parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new INodeSymlink(input, extensionRegistry);
        }
      };

      @java.lang.Override
      public com.google.protobuf.Parser<INodeSymlink> getParserForType() {
        return PARSER;
      }

      private int bitField0_;
      // optional fixed64 permission = 1;
      public static final int PERMISSION_FIELD_NUMBER = 1;
      private long permission_;
      /**
       * <code>optional fixed64 permission = 1;</code>
       */
      public boolean hasPermission() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional fixed64 permission = 1;</code>
       */
      public long getPermission() {
        return permission_;
      }

      // optional bytes target = 2;
      public static final int TARGET_FIELD_NUMBER = 2;
      private com.google.protobuf.ByteString target_;
      /**
       * <code>optional bytes target = 2;</code>
       */
      public boolean hasTarget() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional bytes target = 2;</code>
       */
      public com.google.protobuf.ByteString getTarget() {
        return target_;
      }

      // optional uint64 modificationTime = 3;
      public static final int MODIFICATIONTIME_FIELD_NUMBER = 3;
      private long modificationTime_;
      /**
       * <code>optional uint64 modificationTime = 3;</code>
       */
      public boolean hasModificationTime() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional uint64 modificationTime = 3;</code>
       */
      public long getModificationTime() {
        return modificationTime_;
      }

      // optional uint64 accessTime = 4;
      public static final int ACCESSTIME_FIELD_NUMBER = 4;
      private long accessTime_;
      /**
       * <code>optional uint64 accessTime = 4;</code>
       */
      public boolean hasAccessTime() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional uint64 accessTime = 4;</code>
       */
      public long getAccessTime() {
        return accessTime_;
      }

      private void initFields() {
        permission_ = 0L;
        target_ = com.google.protobuf.ByteString.EMPTY;
        modificationTime_ = 0L;
        accessTime_ = 0L;
      }
      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized != -1) return isInitialized == 1;

        memoizedIsInitialized = 1;
        return true;
      }

      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          output.writeFixed64(1, permission_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          output.writeBytes(2, target_);
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          output.writeUInt64(3, modificationTime_);
        }
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          output.writeUInt64(4, accessTime_);
        }
        getUnknownFields().writeTo(output);
      }

      private int memoizedSerializedSize = -1;
      public int getSerializedSize() {
        int size = memoizedSerializedSize;
        if (size != -1) return size;

        size = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          size += com.google.protobuf.CodedOutputStream
            .computeFixed64Size(1, permission_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          size += com.google.protobuf.CodedOutputStream
            .computeBytesSize(2, target_);
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(3, modificationTime_);
        }
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(4, accessTime_);
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSerializedSize = size;
        return size;
      }

      private static final long serialVersionUID = 0L;
      @java.lang.Override
      protected java.lang.Object writeReplace()
          throws java.io.ObjectStreamException {
        return super.writeReplace();
      }

      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }

      public static Builder newBuilder() { return Builder.create(); }
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink prototype) {
        return newBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() { return newBuilder(this); }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code hadoop.hdfs.fsimage.INodeSection.INodeSymlink}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder>
         implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlinkOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_INodeSymlink_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_INodeSymlink_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink.Builder.class);
        }

        // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          }
        }
        private static Builder create() {
          return new Builder();
        }

        public Builder clear() {
          super.clear();
          permission_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000001);
          target_ = com.google.protobuf.ByteString.EMPTY;
          bitField0_ = (bitField0_ & ~0x00000002);
          modificationTime_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000004);
          accessTime_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000008);
          return this;
        }

        public Builder clone() {
          return create().mergeFrom(buildPartial());
        }

        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_INodeSymlink_descriptor;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink getDefaultInstanceForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink.getDefaultInstance();
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink build() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink buildPartial() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
            to_bitField0_ |= 0x00000001;
          }
          result.permission_ = permission_;
          if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
            to_bitField0_ |= 0x00000002;
          }
          result.target_ = target_;
          if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
            to_bitField0_ |= 0x00000004;
          }
          result.modificationTime_ = modificationTime_;
          if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
            to_bitField0_ |= 0x00000008;
          }
          result.accessTime_ = accessTime_;
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink) {
            return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink other) {
          if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink.getDefaultInstance()) return this;
          if (other.hasPermission()) {
            setPermission(other.getPermission());
          }
          if (other.hasTarget()) {
            setTarget(other.getTarget());
          }
          if (other.hasModificationTime()) {
            setModificationTime(other.getModificationTime());
          }
          if (other.hasAccessTime()) {
            setAccessTime(other.getAccessTime());
          }
          this.mergeUnknownFields(other.getUnknownFields());
          return this;
        }

        public final boolean isInitialized() {
          return true;
        }

        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink) e.getUnfinishedMessage();
            throw e;
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        // optional fixed64 permission = 1;
        private long permission_ ;
        /**
         * <code>optional fixed64 permission = 1;</code>
         */
        public boolean hasPermission() {
          return ((bitField0_ & 0x00000001) == 0x00000001);
        }
        /**
         * <code>optional fixed64 permission = 1;</code>
         */
        public long getPermission() {
          return permission_;
        }
        /**
         * <code>optional fixed64 permission = 1;</code>
         */
        public Builder setPermission(long value) {
          bitField0_ |= 0x00000001;
          permission_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional fixed64 permission = 1;</code>
         */
        public Builder clearPermission() {
          bitField0_ = (bitField0_ & ~0x00000001);
          permission_ = 0L;
          onChanged();
          return this;
        }

        // optional bytes target = 2;
        private com.google.protobuf.ByteString target_ = com.google.protobuf.ByteString.EMPTY;
        /**
         * <code>optional bytes target = 2;</code>
         */
        public boolean hasTarget() {
          return ((bitField0_ & 0x00000002) == 0x00000002);
        }
        /**
         * <code>optional bytes target = 2;</code>
         */
        public com.google.protobuf.ByteString getTarget() {
          return target_;
        }
        /**
         * <code>optional bytes target = 2;</code>
         */
        public Builder setTarget(com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
          target_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional bytes target = 2;</code>
         */
        public Builder clearTarget() {
          bitField0_ = (bitField0_ & ~0x00000002);
          target_ = getDefaultInstance().getTarget();
          onChanged();
          return this;
        }

        // optional uint64 modificationTime = 3;
        private long modificationTime_ ;
        /**
         * <code>optional uint64 modificationTime = 3;</code>
         */
        public boolean hasModificationTime() {
          return ((bitField0_ & 0x00000004) == 0x00000004);
        }
        /**
         * <code>optional uint64 modificationTime = 3;</code>
         */
        public long getModificationTime() {
          return modificationTime_;
        }
        /**
         * <code>optional uint64 modificationTime = 3;</code>
         */
        public Builder setModificationTime(long value) {
          bitField0_ |= 0x00000004;
          modificationTime_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 modificationTime = 3;</code>
         */
        public Builder clearModificationTime() {
          bitField0_ = (bitField0_ & ~0x00000004);
          modificationTime_ = 0L;
          onChanged();
          return this;
        }

        // optional uint64 accessTime = 4;
        private long accessTime_ ;
        /**
         * <code>optional uint64 accessTime = 4;</code>
         */
        public boolean hasAccessTime() {
          return ((bitField0_ & 0x00000008) == 0x00000008);
        }
        /**
         * <code>optional uint64 accessTime = 4;</code>
         */
        public long getAccessTime() {
          return accessTime_;
        }
        /**
         * <code>optional uint64 accessTime = 4;</code>
         */
        public Builder setAccessTime(long value) {
          bitField0_ |= 0x00000008;
          accessTime_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 accessTime = 4;</code>
         */
        public Builder clearAccessTime() {
          bitField0_ = (bitField0_ & ~0x00000008);
          accessTime_ = 0L;
          onChanged();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.INodeSection.INodeSymlink)
      }

      static {
        defaultInstance = new INodeSymlink(true);
        defaultInstance.initFields();
      }

      // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.INodeSection.INodeSymlink)
    }

    public interface INodeOrBuilder
        extends com.google.protobuf.MessageOrBuilder {

      // required .hadoop.hdfs.fsimage.INodeSection.INode.Type type = 1;
      /**
       * <code>required .hadoop.hdfs.fsimage.INodeSection.INode.Type type = 1;</code>
       */
      boolean hasType();
      /**
       * <code>required .hadoop.hdfs.fsimage.INodeSection.INode.Type type = 1;</code>
       */
      org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.Type getType();

      // required uint64 id = 2;
      /**
       * <code>required uint64 id = 2;</code>
       */
      boolean hasId();
      /**
       * <code>required uint64 id = 2;</code>
       */
      long getId();

      // optional bytes name = 3;
      /**
       * <code>optional bytes name = 3;</code>
       */
      boolean hasName();
      /**
       * <code>optional bytes name = 3;</code>
       */
      com.google.protobuf.ByteString getName();

      // optional .hadoop.hdfs.fsimage.INodeSection.INodeFile file = 4;
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile file = 4;</code>
       */
      boolean hasFile();
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile file = 4;</code>
       */
      org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile getFile();
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile file = 4;</code>
       */
      org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFileOrBuilder getFileOrBuilder();

      // optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory directory = 5;
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory directory = 5;</code>
       */
      boolean hasDirectory();
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory directory = 5;</code>
       */
      org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory getDirectory();
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory directory = 5;</code>
       */
      org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectoryOrBuilder getDirectoryOrBuilder();

      // optional .hadoop.hdfs.fsimage.INodeSection.INodeSymlink symlink = 6;
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeSymlink symlink = 6;</code>
       */
      boolean hasSymlink();
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeSymlink symlink = 6;</code>
       */
      org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink getSymlink();
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeSymlink symlink = 6;</code>
       */
      org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlinkOrBuilder getSymlinkOrBuilder();
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.INodeSection.INode}
     */
    public static final class INode extends
        com.google.protobuf.GeneratedMessage
        implements INodeOrBuilder {
      // Use INode.newBuilder() to construct.
      private INode(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
        super(builder);
        this.unknownFields = builder.getUnknownFields();
      }
      private INode(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

      private static final INode defaultInstance;
      public static INode getDefaultInstance() {
        return defaultInstance;
      }

      public INode getDefaultInstanceForType() {
        return defaultInstance;
      }

      private final com.google.protobuf.UnknownFieldSet unknownFields;
      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
          getUnknownFields() {
        return this.unknownFields;
      }
      private INode(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        initFields();
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
              case 8: {
                int rawValue = input.readEnum();
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.Type value = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.Type.valueOf(rawValue);
                if (value == null) {
                  unknownFields.mergeVarintField(1, rawValue);
                } else {
                  bitField0_ |= 0x00000001;
                  type_ = value;
                }
                break;
              }
              case 16: {
                bitField0_ |= 0x00000002;
                id_ = input.readUInt64();
                break;
              }
              case 26: {
                bitField0_ |= 0x00000004;
                name_ = input.readBytes();
                break;
              }
              case 34: {
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.Builder subBuilder = null;
                if (((bitField0_ & 0x00000008) == 0x00000008)) {
                  subBuilder = file_.toBuilder();
                }
                file_ = input.readMessage(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.PARSER, extensionRegistry);
                if (subBuilder != null) {
                  subBuilder.mergeFrom(file_);
                  file_ = subBuilder.buildPartial();
                }
                bitField0_ |= 0x00000008;
                break;
              }
              case 42: {
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.Builder subBuilder = null;
                if (((bitField0_ & 0x00000010) == 0x00000010)) {
                  subBuilder = directory_.toBuilder();
                }
                directory_ = input.readMessage(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.PARSER, extensionRegistry);
                if (subBuilder != null) {
                  subBuilder.mergeFrom(directory_);
                  directory_ = subBuilder.buildPartial();
                }
                bitField0_ |= 0x00000010;
                break;
              }
              case 50: {
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink.Builder subBuilder = null;
                if (((bitField0_ & 0x00000020) == 0x00000020)) {
                  subBuilder = symlink_.toBuilder();
                }
                symlink_ = input.readMessage(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink.PARSER, extensionRegistry);
                if (subBuilder != null) {
                  subBuilder.mergeFrom(symlink_);
                  symlink_ = subBuilder.buildPartial();
                }
                bitField0_ |= 0x00000020;
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e.getMessage()).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_INode_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_INode_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.Builder.class);
      }

      public static com.google.protobuf.Parser<INode> PARSER =
          new com.google.protobuf.AbstractParser<INode>() {
        public INode parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new INode(input, extensionRegistry);
        }
      };

      @java.lang.Override
      public com.google.protobuf.Parser<INode> getParserForType() {
        return PARSER;
      }

      /**
       * Protobuf enum {@code hadoop.hdfs.fsimage.INodeSection.INode.Type}
       */
      public enum Type
          implements com.google.protobuf.ProtocolMessageEnum {
        /**
         * <code>FILE = 1;</code>
         */
        FILE(0, 1),
        /**
         * <code>DIRECTORY = 2;</code>
         */
        DIRECTORY(1, 2),
        /**
         * <code>SYMLINK = 3;</code>
         */
        SYMLINK(2, 3),
        ;

        /**
         * <code>FILE = 1;</code>
         */
        public static final int FILE_VALUE = 1;
        /**
         * <code>DIRECTORY = 2;</code>
         */
        public static final int DIRECTORY_VALUE = 2;
        /**
         * <code>SYMLINK = 3;</code>
         */
        public static final int SYMLINK_VALUE = 3;


        public final int getNumber() { return value; }

        public static Type valueOf(int value) {
          switch (value) {
            case 1: return FILE;
            case 2: return DIRECTORY;
            case 3: return SYMLINK;
            default: return null;
          }
        }

        public static com.google.protobuf.Internal.EnumLiteMap<Type>
            internalGetValueMap() {
          return internalValueMap;
        }
        private static com.google.protobuf.Internal.EnumLiteMap<Type>
            internalValueMap =
              new com.google.protobuf.Internal.EnumLiteMap<Type>() {
                public Type findValueByNumber(int number) {
                  return Type.valueOf(number);
                }
              };

        public final com.google.protobuf.Descriptors.EnumValueDescriptor
            getValueDescriptor() {
          return getDescriptor().getValues().get(index);
        }
        public final com.google.protobuf.Descriptors.EnumDescriptor
            getDescriptorForType() {
          return getDescriptor();
        }
        public static final com.google.protobuf.Descriptors.EnumDescriptor
            getDescriptor() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.getDescriptor().getEnumTypes().get(0);
        }

        private static final Type[] VALUES = values();

        public static Type valueOf(
            com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
          if (desc.getType() != getDescriptor()) {
            throw new java.lang.IllegalArgumentException(
              "EnumValueDescriptor is not for this type.");
          }
          return VALUES[desc.getIndex()];
        }

        private final int index;
        private final int value;

        private Type(int index, int value) {
          this.index = index;
          this.value = value;
        }

        // @@protoc_insertion_point(enum_scope:hadoop.hdfs.fsimage.INodeSection.INode.Type)
      }

      private int bitField0_;
      // required .hadoop.hdfs.fsimage.INodeSection.INode.Type type = 1;
      public static final int TYPE_FIELD_NUMBER = 1;
      private org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.Type type_;
      /**
       * <code>required .hadoop.hdfs.fsimage.INodeSection.INode.Type type = 1;</code>
       */
      public boolean hasType() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .hadoop.hdfs.fsimage.INodeSection.INode.Type type = 1;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.Type getType() {
        return type_;
      }

      // required uint64 id = 2;
      public static final int ID_FIELD_NUMBER = 2;
      private long id_;
      /**
       * <code>required uint64 id = 2;</code>
       */
      public boolean hasId() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required uint64 id = 2;</code>
       */
      public long getId() {
        return id_;
      }

      // optional bytes name = 3;
      public static final int NAME_FIELD_NUMBER = 3;
      private com.google.protobuf.ByteString name_;
      /**
       * <code>optional bytes name = 3;</code>
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional bytes name = 3;</code>
       */
      public com.google.protobuf.ByteString getName() {
        return name_;
      }

      // optional .hadoop.hdfs.fsimage.INodeSection.INodeFile file = 4;
      public static final int FILE_FIELD_NUMBER = 4;
      private org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile file_;
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile file = 4;</code>
       */
      public boolean hasFile() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile file = 4;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile getFile() {
        return file_;
      }
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile file = 4;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFileOrBuilder getFileOrBuilder() {
        return file_;
      }

      // optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory directory = 5;
      public static final int DIRECTORY_FIELD_NUMBER = 5;
      private org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory directory_;
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory directory = 5;</code>
       */
      public boolean hasDirectory() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory directory = 5;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory getDirectory() {
        return directory_;
      }
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory directory = 5;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectoryOrBuilder getDirectoryOrBuilder() {
        return directory_;
      }

      // optional .hadoop.hdfs.fsimage.INodeSection.INodeSymlink symlink = 6;
      public static final int SYMLINK_FIELD_NUMBER = 6;
      private org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink symlink_;
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeSymlink symlink = 6;</code>
       */
      public boolean hasSymlink() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeSymlink symlink = 6;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink getSymlink() {
        return symlink_;
      }
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeSymlink symlink = 6;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlinkOrBuilder getSymlinkOrBuilder() {
        return symlink_;
      }

      private void initFields() {
        type_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.Type.FILE;
        id_ = 0L;
        name_ = com.google.protobuf.ByteString.EMPTY;
        file_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.getDefaultInstance();
        directory_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.getDefaultInstance();
        symlink_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink.getDefaultInstance();
      }
      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized != -1) return isInitialized == 1;

        if (!hasType()) {
          memoizedIsInitialized = 0;
          return false;
        }
        if (!hasId()) {
          memoizedIsInitialized = 0;
          return false;
        }
        if (hasFile()) {
          if (!getFile().isInitialized()) {
            memoizedIsInitialized = 0;
            return false;
          }
        }
        memoizedIsInitialized = 1;
        return true;
      }

      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          output.writeEnum(1, type_.getNumber());
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          output.writeUInt64(2, id_);
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          output.writeBytes(3, name_);
        }
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          output.writeMessage(4, file_);
        }
        if (((bitField0_ & 0x00000010) == 0x00000010)) {
          output.writeMessage(5, directory_);
        }
        if (((bitField0_ & 0x00000020) == 0x00000020)) {
          output.writeMessage(6, symlink_);
        }
        getUnknownFields().writeTo(output);
      }

      private int memoizedSerializedSize = -1;
      public int getSerializedSize() {
        int size = memoizedSerializedSize;
        if (size != -1) return size;

        size = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          size += com.google.protobuf.CodedOutputStream
            .computeEnumSize(1, type_.getNumber());
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(2, id_);
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          size += com.google.protobuf.CodedOutputStream
            .computeBytesSize(3, name_);
        }
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(4, file_);
        }
        if (((bitField0_ & 0x00000010) == 0x00000010)) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(5, directory_);
        }
        if (((bitField0_ & 0x00000020) == 0x00000020)) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(6, symlink_);
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSerializedSize = size;
        return size;
      }

      private static final long serialVersionUID = 0L;
      @java.lang.Override
      protected java.lang.Object writeReplace()
          throws java.io.ObjectStreamException {
        return super.writeReplace();
      }

      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }

      public static Builder newBuilder() { return Builder.create(); }
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode prototype) {
        return newBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() { return newBuilder(this); }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code hadoop.hdfs.fsimage.INodeSection.INode}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder>
         implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_INode_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_INode_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.Builder.class);
        }

        // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
            getFileFieldBuilder();
            getDirectoryFieldBuilder();
            getSymlinkFieldBuilder();
          }
        }
        private static Builder create() {
          return new Builder();
        }

        public Builder clear() {
          super.clear();
          type_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.Type.FILE;
          bitField0_ = (bitField0_ & ~0x00000001);
          id_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000002);
          name_ = com.google.protobuf.ByteString.EMPTY;
          bitField0_ = (bitField0_ & ~0x00000004);
          if (fileBuilder_ == null) {
            file_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.getDefaultInstance();
          } else {
            fileBuilder_.clear();
          }
          bitField0_ = (bitField0_ & ~0x00000008);
          if (directoryBuilder_ == null) {
            directory_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.getDefaultInstance();
          } else {
            directoryBuilder_.clear();
          }
          bitField0_ = (bitField0_ & ~0x00000010);
          if (symlinkBuilder_ == null) {
            symlink_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink.getDefaultInstance();
          } else {
            symlinkBuilder_.clear();
          }
          bitField0_ = (bitField0_ & ~0x00000020);
          return this;
        }

        public Builder clone() {
          return create().mergeFrom(buildPartial());
        }

        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_INode_descriptor;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode getDefaultInstanceForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.getDefaultInstance();
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode build() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode buildPartial() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
            to_bitField0_ |= 0x00000001;
          }
          result.type_ = type_;
          if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
            to_bitField0_ |= 0x00000002;
          }
          result.id_ = id_;
          if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
            to_bitField0_ |= 0x00000004;
          }
          result.name_ = name_;
          if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
            to_bitField0_ |= 0x00000008;
          }
          if (fileBuilder_ == null) {
            result.file_ = file_;
          } else {
            result.file_ = fileBuilder_.build();
          }
          if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
            to_bitField0_ |= 0x00000010;
          }
          if (directoryBuilder_ == null) {
            result.directory_ = directory_;
          } else {
            result.directory_ = directoryBuilder_.build();
          }
          if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
            to_bitField0_ |= 0x00000020;
          }
          if (symlinkBuilder_ == null) {
            result.symlink_ = symlink_;
          } else {
            result.symlink_ = symlinkBuilder_.build();
          }
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode) {
            return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode other) {
          if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.getDefaultInstance()) return this;
          if (other.hasType()) {
            setType(other.getType());
          }
          if (other.hasId()) {
            setId(other.getId());
          }
          if (other.hasName()) {
            setName(other.getName());
          }
          if (other.hasFile()) {
            mergeFile(other.getFile());
          }
          if (other.hasDirectory()) {
            mergeDirectory(other.getDirectory());
          }
          if (other.hasSymlink()) {
            mergeSymlink(other.getSymlink());
          }
          this.mergeUnknownFields(other.getUnknownFields());
          return this;
        }

        public final boolean isInitialized() {
          if (!hasType()) {
            
            return false;
          }
          if (!hasId()) {
            
            return false;
          }
          if (hasFile()) {
            if (!getFile().isInitialized()) {
              
              return false;
            }
          }
          return true;
        }

        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode) e.getUnfinishedMessage();
            throw e;
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        // required .hadoop.hdfs.fsimage.INodeSection.INode.Type type = 1;
        private org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.Type type_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.Type.FILE;
        /**
         * <code>required .hadoop.hdfs.fsimage.INodeSection.INode.Type type = 1;</code>
         */
        public boolean hasType() {
          return ((bitField0_ & 0x00000001) == 0x00000001);
        }
        /**
         * <code>required .hadoop.hdfs.fsimage.INodeSection.INode.Type type = 1;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.Type getType() {
          return type_;
        }
        /**
         * <code>required .hadoop.hdfs.fsimage.INodeSection.INode.Type type = 1;</code>
         */
        public Builder setType(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.Type value) {
          if (value == null) {
            throw new NullPointerException();
          }
          bitField0_ |= 0x00000001;
          type_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>required .hadoop.hdfs.fsimage.INodeSection.INode.Type type = 1;</code>
         */
        public Builder clearType() {
          bitField0_ = (bitField0_ & ~0x00000001);
          type_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.Type.FILE;
          onChanged();
          return this;
        }

        // required uint64 id = 2;
        private long id_ ;
        /**
         * <code>required uint64 id = 2;</code>
         */
        public boolean hasId() {
          return ((bitField0_ & 0x00000002) == 0x00000002);
        }
        /**
         * <code>required uint64 id = 2;</code>
         */
        public long getId() {
          return id_;
        }
        /**
         * <code>required uint64 id = 2;</code>
         */
        public Builder setId(long value) {
          bitField0_ |= 0x00000002;
          id_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>required uint64 id = 2;</code>
         */
        public Builder clearId() {
          bitField0_ = (bitField0_ & ~0x00000002);
          id_ = 0L;
          onChanged();
          return this;
        }

        // optional bytes name = 3;
        private com.google.protobuf.ByteString name_ = com.google.protobuf.ByteString.EMPTY;
        /**
         * <code>optional bytes name = 3;</code>
         */
        public boolean hasName() {
          return ((bitField0_ & 0x00000004) == 0x00000004);
        }
        /**
         * <code>optional bytes name = 3;</code>
         */
        public com.google.protobuf.ByteString getName() {
          return name_;
        }
        /**
         * <code>optional bytes name = 3;</code>
         */
        public Builder setName(com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
          name_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional bytes name = 3;</code>
         */
        public Builder clearName() {
          bitField0_ = (bitField0_ & ~0x00000004);
          name_ = getDefaultInstance().getName();
          onChanged();
          return this;
        }

        // optional .hadoop.hdfs.fsimage.INodeSection.INodeFile file = 4;
        private org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile file_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.getDefaultInstance();
        private com.google.protobuf.SingleFieldBuilder<
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFileOrBuilder> fileBuilder_;
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile file = 4;</code>
         */
        public boolean hasFile() {
          return ((bitField0_ & 0x00000008) == 0x00000008);
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile file = 4;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile getFile() {
          if (fileBuilder_ == null) {
            return file_;
          } else {
            return fileBuilder_.getMessage();
          }
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile file = 4;</code>
         */
        public Builder setFile(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile value) {
          if (fileBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            file_ = value;
            onChanged();
          } else {
            fileBuilder_.setMessage(value);
          }
          bitField0_ |= 0x00000008;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile file = 4;</code>
         */
        public Builder setFile(
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.Builder builderForValue) {
          if (fileBuilder_ == null) {
            file_ = builderForValue.build();
            onChanged();
          } else {
            fileBuilder_.setMessage(builderForValue.build());
          }
          bitField0_ |= 0x00000008;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile file = 4;</code>
         */
        public Builder mergeFile(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile value) {
          if (fileBuilder_ == null) {
            if (((bitField0_ & 0x00000008) == 0x00000008) &&
                file_ != org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.getDefaultInstance()) {
              file_ =
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.newBuilder(file_).mergeFrom(value).buildPartial();
            } else {
              file_ = value;
            }
            onChanged();
          } else {
            fileBuilder_.mergeFrom(value);
          }
          bitField0_ |= 0x00000008;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile file = 4;</code>
         */
        public Builder clearFile() {
          if (fileBuilder_ == null) {
            file_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.getDefaultInstance();
            onChanged();
          } else {
            fileBuilder_.clear();
          }
          bitField0_ = (bitField0_ & ~0x00000008);
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile file = 4;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.Builder getFileBuilder() {
          bitField0_ |= 0x00000008;
          onChanged();
          return getFileFieldBuilder().getBuilder();
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile file = 4;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFileOrBuilder getFileOrBuilder() {
          if (fileBuilder_ != null) {
            return fileBuilder_.getMessageOrBuilder();
          } else {
            return file_;
          }
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile file = 4;</code>
         */
        private com.google.protobuf.SingleFieldBuilder<
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFileOrBuilder> 
            getFileFieldBuilder() {
          if (fileBuilder_ == null) {
            fileBuilder_ = new com.google.protobuf.SingleFieldBuilder<
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFileOrBuilder>(
                    file_,
                    getParentForChildren(),
                    isClean());
            file_ = null;
          }
          return fileBuilder_;
        }

        // optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory directory = 5;
        private org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory directory_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.getDefaultInstance();
        private com.google.protobuf.SingleFieldBuilder<
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectoryOrBuilder> directoryBuilder_;
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory directory = 5;</code>
         */
        public boolean hasDirectory() {
          return ((bitField0_ & 0x00000010) == 0x00000010);
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory directory = 5;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory getDirectory() {
          if (directoryBuilder_ == null) {
            return directory_;
          } else {
            return directoryBuilder_.getMessage();
          }
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory directory = 5;</code>
         */
        public Builder setDirectory(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory value) {
          if (directoryBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            directory_ = value;
            onChanged();
          } else {
            directoryBuilder_.setMessage(value);
          }
          bitField0_ |= 0x00000010;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory directory = 5;</code>
         */
        public Builder setDirectory(
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.Builder builderForValue) {
          if (directoryBuilder_ == null) {
            directory_ = builderForValue.build();
            onChanged();
          } else {
            directoryBuilder_.setMessage(builderForValue.build());
          }
          bitField0_ |= 0x00000010;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory directory = 5;</code>
         */
        public Builder mergeDirectory(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory value) {
          if (directoryBuilder_ == null) {
            if (((bitField0_ & 0x00000010) == 0x00000010) &&
                directory_ != org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.getDefaultInstance()) {
              directory_ =
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.newBuilder(directory_).mergeFrom(value).buildPartial();
            } else {
              directory_ = value;
            }
            onChanged();
          } else {
            directoryBuilder_.mergeFrom(value);
          }
          bitField0_ |= 0x00000010;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory directory = 5;</code>
         */
        public Builder clearDirectory() {
          if (directoryBuilder_ == null) {
            directory_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.getDefaultInstance();
            onChanged();
          } else {
            directoryBuilder_.clear();
          }
          bitField0_ = (bitField0_ & ~0x00000010);
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory directory = 5;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.Builder getDirectoryBuilder() {
          bitField0_ |= 0x00000010;
          onChanged();
          return getDirectoryFieldBuilder().getBuilder();
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory directory = 5;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectoryOrBuilder getDirectoryOrBuilder() {
          if (directoryBuilder_ != null) {
            return directoryBuilder_.getMessageOrBuilder();
          } else {
            return directory_;
          }
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory directory = 5;</code>
         */
        private com.google.protobuf.SingleFieldBuilder<
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectoryOrBuilder> 
            getDirectoryFieldBuilder() {
          if (directoryBuilder_ == null) {
            directoryBuilder_ = new com.google.protobuf.SingleFieldBuilder<
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectoryOrBuilder>(
                    directory_,
                    getParentForChildren(),
                    isClean());
            directory_ = null;
          }
          return directoryBuilder_;
        }

        // optional .hadoop.hdfs.fsimage.INodeSection.INodeSymlink symlink = 6;
        private org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink symlink_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink.getDefaultInstance();
        private com.google.protobuf.SingleFieldBuilder<
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlinkOrBuilder> symlinkBuilder_;
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeSymlink symlink = 6;</code>
         */
        public boolean hasSymlink() {
          return ((bitField0_ & 0x00000020) == 0x00000020);
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeSymlink symlink = 6;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink getSymlink() {
          if (symlinkBuilder_ == null) {
            return symlink_;
          } else {
            return symlinkBuilder_.getMessage();
          }
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeSymlink symlink = 6;</code>
         */
        public Builder setSymlink(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink value) {
          if (symlinkBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            symlink_ = value;
            onChanged();
          } else {
            symlinkBuilder_.setMessage(value);
          }
          bitField0_ |= 0x00000020;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeSymlink symlink = 6;</code>
         */
        public Builder setSymlink(
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink.Builder builderForValue) {
          if (symlinkBuilder_ == null) {
            symlink_ = builderForValue.build();
            onChanged();
          } else {
            symlinkBuilder_.setMessage(builderForValue.build());
          }
          bitField0_ |= 0x00000020;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeSymlink symlink = 6;</code>
         */
        public Builder mergeSymlink(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink value) {
          if (symlinkBuilder_ == null) {
            if (((bitField0_ & 0x00000020) == 0x00000020) &&
                symlink_ != org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink.getDefaultInstance()) {
              symlink_ =
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink.newBuilder(symlink_).mergeFrom(value).buildPartial();
            } else {
              symlink_ = value;
            }
            onChanged();
          } else {
            symlinkBuilder_.mergeFrom(value);
          }
          bitField0_ |= 0x00000020;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeSymlink symlink = 6;</code>
         */
        public Builder clearSymlink() {
          if (symlinkBuilder_ == null) {
            symlink_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink.getDefaultInstance();
            onChanged();
          } else {
            symlinkBuilder_.clear();
          }
          bitField0_ = (bitField0_ & ~0x00000020);
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeSymlink symlink = 6;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink.Builder getSymlinkBuilder() {
          bitField0_ |= 0x00000020;
          onChanged();
          return getSymlinkFieldBuilder().getBuilder();
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeSymlink symlink = 6;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlinkOrBuilder getSymlinkOrBuilder() {
          if (symlinkBuilder_ != null) {
            return symlinkBuilder_.getMessageOrBuilder();
          } else {
            return symlink_;
          }
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeSymlink symlink = 6;</code>
         */
        private com.google.protobuf.SingleFieldBuilder<
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlinkOrBuilder> 
            getSymlinkFieldBuilder() {
          if (symlinkBuilder_ == null) {
            symlinkBuilder_ = new com.google.protobuf.SingleFieldBuilder<
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlink.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeSymlinkOrBuilder>(
                    symlink_,
                    getParentForChildren(),
                    isClean());
            symlink_ = null;
          }
          return symlinkBuilder_;
        }

        // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.INodeSection.INode)
      }

      static {
        defaultInstance = new INode(true);
        defaultInstance.initFields();
      }

      // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.INodeSection.INode)
    }

    private int bitField0_;
    // optional uint64 lastInodeId = 1;
    public static final int LASTINODEID_FIELD_NUMBER = 1;
    private long lastInodeId_;
    /**
     * <code>optional uint64 lastInodeId = 1;</code>
     */
    public boolean hasLastInodeId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional uint64 lastInodeId = 1;</code>
     */
    public long getLastInodeId() {
      return lastInodeId_;
    }

    // optional uint64 numInodes = 2;
    public static final int NUMINODES_FIELD_NUMBER = 2;
    private long numInodes_;
    /**
     * <code>optional uint64 numInodes = 2;</code>
     *
     * <pre>
     * repeated INodes..
     * </pre>
     */
    public boolean hasNumInodes() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional uint64 numInodes = 2;</code>
     *
     * <pre>
     * repeated INodes..
     * </pre>
     */
    public long getNumInodes() {
      return numInodes_;
    }

    private void initFields() {
      lastInodeId_ = 0L;
      numInodes_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeUInt64(1, lastInodeId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt64(2, numInodes_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(1, lastInodeId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, numInodes_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.INodeSection}
     *
     * <pre>
     **
     * Permission is serialized as a 64-bit long. [0:24):[25:48):[48:64) (in Big Endian).
     * The first and the second parts are the string ids of the user and
     * group name, and the last 16 bits are the permission bits.
     *
     * Name: INODE
     * </pre>
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSectionOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        lastInodeId_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        numInodes_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeSection_descriptor;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection build() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection buildPartial() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.lastInodeId_ = lastInodeId_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.numInodes_ = numInodes_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection) {
          return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection other) {
        if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.getDefaultInstance()) return this;
        if (other.hasLastInodeId()) {
          setLastInodeId(other.getLastInodeId());
        }
        if (other.hasNumInodes()) {
          setNumInodes(other.getNumInodes());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional uint64 lastInodeId = 1;
      private long lastInodeId_ ;
      /**
       * <code>optional uint64 lastInodeId = 1;</code>
       */
      public boolean hasLastInodeId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint64 lastInodeId = 1;</code>
       */
      public long getLastInodeId() {
        return lastInodeId_;
      }
      /**
       * <code>optional uint64 lastInodeId = 1;</code>
       */
      public Builder setLastInodeId(long value) {
        bitField0_ |= 0x00000001;
        lastInodeId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 lastInodeId = 1;</code>
       */
      public Builder clearLastInodeId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        lastInodeId_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 numInodes = 2;
      private long numInodes_ ;
      /**
       * <code>optional uint64 numInodes = 2;</code>
       *
       * <pre>
       * repeated INodes..
       * </pre>
       */
      public boolean hasNumInodes() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional uint64 numInodes = 2;</code>
       *
       * <pre>
       * repeated INodes..
       * </pre>
       */
      public long getNumInodes() {
        return numInodes_;
      }
      /**
       * <code>optional uint64 numInodes = 2;</code>
       *
       * <pre>
       * repeated INodes..
       * </pre>
       */
      public Builder setNumInodes(long value) {
        bitField0_ |= 0x00000002;
        numInodes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 numInodes = 2;</code>
       *
       * <pre>
       * repeated INodes..
       * </pre>
       */
      public Builder clearNumInodes() {
        bitField0_ = (bitField0_ & ~0x00000002);
        numInodes_ = 0L;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.INodeSection)
    }

    static {
      defaultInstance = new INodeSection(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.INodeSection)
  }

  public interface FilesUnderConstructionSectionOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.hdfs.fsimage.FilesUnderConstructionSection}
   *
   * <pre>
   **
   * This section records information about under-construction files for
   * reconstructing the lease map.
   * NAME: FILES_UNDERCONSTRUCTION
   * </pre>
   */
  public static final class FilesUnderConstructionSection extends
      com.google.protobuf.GeneratedMessage
      implements FilesUnderConstructionSectionOrBuilder {
    // Use FilesUnderConstructionSection.newBuilder() to construct.
    private FilesUnderConstructionSection(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private FilesUnderConstructionSection(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final FilesUnderConstructionSection defaultInstance;
    public static FilesUnderConstructionSection getDefaultInstance() {
      return defaultInstance;
    }

    public FilesUnderConstructionSection getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private FilesUnderConstructionSection(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_FilesUnderConstructionSection_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_FilesUnderConstructionSection_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.Builder.class);
    }

    public static com.google.protobuf.Parser<FilesUnderConstructionSection> PARSER =
        new com.google.protobuf.AbstractParser<FilesUnderConstructionSection>() {
      public FilesUnderConstructionSection parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new FilesUnderConstructionSection(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<FilesUnderConstructionSection> getParserForType() {
      return PARSER;
    }

    public interface FileUnderConstructionEntryOrBuilder
        extends com.google.protobuf.MessageOrBuilder {

      // optional uint64 inodeId = 1;
      /**
       * <code>optional uint64 inodeId = 1;</code>
       */
      boolean hasInodeId();
      /**
       * <code>optional uint64 inodeId = 1;</code>
       */
      long getInodeId();

      // optional string fullPath = 2;
      /**
       * <code>optional string fullPath = 2;</code>
       */
      boolean hasFullPath();
      /**
       * <code>optional string fullPath = 2;</code>
       */
      java.lang.String getFullPath();
      /**
       * <code>optional string fullPath = 2;</code>
       */
      com.google.protobuf.ByteString
          getFullPathBytes();
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.FilesUnderConstructionSection.FileUnderConstructionEntry}
     */
    public static final class FileUnderConstructionEntry extends
        com.google.protobuf.GeneratedMessage
        implements FileUnderConstructionEntryOrBuilder {
      // Use FileUnderConstructionEntry.newBuilder() to construct.
      private FileUnderConstructionEntry(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
        super(builder);
        this.unknownFields = builder.getUnknownFields();
      }
      private FileUnderConstructionEntry(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

      private static final FileUnderConstructionEntry defaultInstance;
      public static FileUnderConstructionEntry getDefaultInstance() {
        return defaultInstance;
      }

      public FileUnderConstructionEntry getDefaultInstanceForType() {
        return defaultInstance;
      }

      private final com.google.protobuf.UnknownFieldSet unknownFields;
      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
          getUnknownFields() {
        return this.unknownFields;
      }
      private FileUnderConstructionEntry(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        initFields();
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
              case 8: {
                bitField0_ |= 0x00000001;
                inodeId_ = input.readUInt64();
                break;
              }
              case 18: {
                bitField0_ |= 0x00000002;
                fullPath_ = input.readBytes();
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e.getMessage()).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_FilesUnderConstructionSection_FileUnderConstructionEntry_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_FilesUnderConstructionSection_FileUnderConstructionEntry_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry.Builder.class);
      }

      public static com.google.protobuf.Parser<FileUnderConstructionEntry> PARSER =
          new com.google.protobuf.AbstractParser<FileUnderConstructionEntry>() {
        public FileUnderConstructionEntry parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new FileUnderConstructionEntry(input, extensionRegistry);
        }
      };

      @java.lang.Override
      public com.google.protobuf.Parser<FileUnderConstructionEntry> getParserForType() {
        return PARSER;
      }

      private int bitField0_;
      // optional uint64 inodeId = 1;
      public static final int INODEID_FIELD_NUMBER = 1;
      private long inodeId_;
      /**
       * <code>optional uint64 inodeId = 1;</code>
       */
      public boolean hasInodeId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint64 inodeId = 1;</code>
       */
      public long getInodeId() {
        return inodeId_;
      }

      // optional string fullPath = 2;
      public static final int FULLPATH_FIELD_NUMBER = 2;
      private java.lang.Object fullPath_;
      /**
       * <code>optional string fullPath = 2;</code>
       */
      public boolean hasFullPath() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional string fullPath = 2;</code>
       */
      public java.lang.String getFullPath() {
        java.lang.Object ref = fullPath_;
        if (ref instanceof java.lang.String) {
          return (java.lang.String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            fullPath_ = s;
          }
          return s;
        }
      }
      /**
       * <code>optional string fullPath = 2;</code>
       */
      public com.google.protobuf.ByteString
          getFullPathBytes() {
        java.lang.Object ref = fullPath_;
        if (ref instanceof java.lang.String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          fullPath_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      private void initFields() {
        inodeId_ = 0L;
        fullPath_ = "";
      }
      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized != -1) return isInitialized == 1;

        memoizedIsInitialized = 1;
        return true;
      }

      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          output.writeUInt64(1, inodeId_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          output.writeBytes(2, getFullPathBytes());
        }
        getUnknownFields().writeTo(output);
      }

      private int memoizedSerializedSize = -1;
      public int getSerializedSize() {
        int size = memoizedSerializedSize;
        if (size != -1) return size;

        size = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(1, inodeId_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          size += com.google.protobuf.CodedOutputStream
            .computeBytesSize(2, getFullPathBytes());
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSerializedSize = size;
        return size;
      }

      private static final long serialVersionUID = 0L;
      @java.lang.Override
      protected java.lang.Object writeReplace()
          throws java.io.ObjectStreamException {
        return super.writeReplace();
      }

      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }

      public static Builder newBuilder() { return Builder.create(); }
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry prototype) {
        return newBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() { return newBuilder(this); }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code hadoop.hdfs.fsimage.FilesUnderConstructionSection.FileUnderConstructionEntry}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder>
         implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntryOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_FilesUnderConstructionSection_FileUnderConstructionEntry_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_FilesUnderConstructionSection_FileUnderConstructionEntry_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry.Builder.class);
        }

        // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          }
        }
        private static Builder create() {
          return new Builder();
        }

        public Builder clear() {
          super.clear();
          inodeId_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000001);
          fullPath_ = "";
          bitField0_ = (bitField0_ & ~0x00000002);
          return this;
        }

        public Builder clone() {
          return create().mergeFrom(buildPartial());
        }

        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_FilesUnderConstructionSection_FileUnderConstructionEntry_descriptor;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry getDefaultInstanceForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry.getDefaultInstance();
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry build() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry buildPartial() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
            to_bitField0_ |= 0x00000001;
          }
          result.inodeId_ = inodeId_;
          if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
            to_bitField0_ |= 0x00000002;
          }
          result.fullPath_ = fullPath_;
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry) {
            return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry other) {
          if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry.getDefaultInstance()) return this;
          if (other.hasInodeId()) {
            setInodeId(other.getInodeId());
          }
          if (other.hasFullPath()) {
            bitField0_ |= 0x00000002;
            fullPath_ = other.fullPath_;
            onChanged();
          }
          this.mergeUnknownFields(other.getUnknownFields());
          return this;
        }

        public final boolean isInitialized() {
          return true;
        }

        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry) e.getUnfinishedMessage();
            throw e;
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        // optional uint64 inodeId = 1;
        private long inodeId_ ;
        /**
         * <code>optional uint64 inodeId = 1;</code>
         */
        public boolean hasInodeId() {
          return ((bitField0_ & 0x00000001) == 0x00000001);
        }
        /**
         * <code>optional uint64 inodeId = 1;</code>
         */
        public long getInodeId() {
          return inodeId_;
        }
        /**
         * <code>optional uint64 inodeId = 1;</code>
         */
        public Builder setInodeId(long value) {
          bitField0_ |= 0x00000001;
          inodeId_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 inodeId = 1;</code>
         */
        public Builder clearInodeId() {
          bitField0_ = (bitField0_ & ~0x00000001);
          inodeId_ = 0L;
          onChanged();
          return this;
        }

        // optional string fullPath = 2;
        private java.lang.Object fullPath_ = "";
        /**
         * <code>optional string fullPath = 2;</code>
         */
        public boolean hasFullPath() {
          return ((bitField0_ & 0x00000002) == 0x00000002);
        }
        /**
         * <code>optional string fullPath = 2;</code>
         */
        public java.lang.String getFullPath() {
          java.lang.Object ref = fullPath_;
          if (!(ref instanceof java.lang.String)) {
            java.lang.String s = ((com.google.protobuf.ByteString) ref)
                .toStringUtf8();
            fullPath_ = s;
            return s;
          } else {
            return (java.lang.String) ref;
          }
        }
        /**
         * <code>optional string fullPath = 2;</code>
         */
        public com.google.protobuf.ByteString
            getFullPathBytes() {
          java.lang.Object ref = fullPath_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            fullPath_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <code>optional string fullPath = 2;</code>
         */
        public Builder setFullPath(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
          fullPath_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional string fullPath = 2;</code>
         */
        public Builder clearFullPath() {
          bitField0_ = (bitField0_ & ~0x00000002);
          fullPath_ = getDefaultInstance().getFullPath();
          onChanged();
          return this;
        }
        /**
         * <code>optional string fullPath = 2;</code>
         */
        public Builder setFullPathBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
          fullPath_ = value;
          onChanged();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.FilesUnderConstructionSection.FileUnderConstructionEntry)
      }

      static {
        defaultInstance = new FileUnderConstructionEntry(true);
        defaultInstance.initFields();
      }

      // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.FilesUnderConstructionSection.FileUnderConstructionEntry)
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.FilesUnderConstructionSection}
     *
     * <pre>
     **
     * This section records information about under-construction files for
     * reconstructing the lease map.
     * NAME: FILES_UNDERCONSTRUCTION
     * </pre>
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSectionOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_FilesUnderConstructionSection_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_FilesUnderConstructionSection_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_FilesUnderConstructionSection_descriptor;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection build() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection buildPartial() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection) {
          return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection other) {
        if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.FilesUnderConstructionSection) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.FilesUnderConstructionSection)
    }

    static {
      defaultInstance = new FilesUnderConstructionSection(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.FilesUnderConstructionSection)
  }

  public interface INodeDirectorySectionOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.hdfs.fsimage.INodeDirectorySection}
   *
   * <pre>
   **
   * This section records the children of each directories
   * NAME: INODE_DIR
   * </pre>
   */
  public static final class INodeDirectorySection extends
      com.google.protobuf.GeneratedMessage
      implements INodeDirectorySectionOrBuilder {
    // Use INodeDirectorySection.newBuilder() to construct.
    private INodeDirectorySection(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private INodeDirectorySection(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final INodeDirectorySection defaultInstance;
    public static INodeDirectorySection getDefaultInstance() {
      return defaultInstance;
    }

    public INodeDirectorySection getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private INodeDirectorySection(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeDirectorySection_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeDirectorySection_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.Builder.class);
    }

    public static com.google.protobuf.Parser<INodeDirectorySection> PARSER =
        new com.google.protobuf.AbstractParser<INodeDirectorySection>() {
      public INodeDirectorySection parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new INodeDirectorySection(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<INodeDirectorySection> getParserForType() {
      return PARSER;
    }

    public interface DirEntryOrBuilder
        extends com.google.protobuf.MessageOrBuilder {

      // optional uint64 parent = 1;
      /**
       * <code>optional uint64 parent = 1;</code>
       */
      boolean hasParent();
      /**
       * <code>optional uint64 parent = 1;</code>
       */
      long getParent();

      // repeated uint64 children = 2 [packed = true];
      /**
       * <code>repeated uint64 children = 2 [packed = true];</code>
       *
       * <pre>
       * children that are not reference nodes
       * </pre>
       */
      java.util.List<java.lang.Long> getChildrenList();
      /**
       * <code>repeated uint64 children = 2 [packed = true];</code>
       *
       * <pre>
       * children that are not reference nodes
       * </pre>
       */
      int getChildrenCount();
      /**
       * <code>repeated uint64 children = 2 [packed = true];</code>
       *
       * <pre>
       * children that are not reference nodes
       * </pre>
       */
      long getChildren(int index);

      // repeated uint32 refChildren = 3 [packed = true];
      /**
       * <code>repeated uint32 refChildren = 3 [packed = true];</code>
       *
       * <pre>
       * children that are reference nodes, each element is a reference node id
       * </pre>
       */
      java.util.List<java.lang.Integer> getRefChildrenList();
      /**
       * <code>repeated uint32 refChildren = 3 [packed = true];</code>
       *
       * <pre>
       * children that are reference nodes, each element is a reference node id
       * </pre>
       */
      int getRefChildrenCount();
      /**
       * <code>repeated uint32 refChildren = 3 [packed = true];</code>
       *
       * <pre>
       * children that are reference nodes, each element is a reference node id
       * </pre>
       */
      int getRefChildren(int index);
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.INodeDirectorySection.DirEntry}
     *
     * <pre>
     **
     * A single DirEntry needs to fit in the default PB max message size of
     * 64MB. Please be careful when adding more fields to a DirEntry!
     * </pre>
     */
    public static final class DirEntry extends
        com.google.protobuf.GeneratedMessage
        implements DirEntryOrBuilder {
      // Use DirEntry.newBuilder() to construct.
      private DirEntry(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
        super(builder);
        this.unknownFields = builder.getUnknownFields();
      }
      private DirEntry(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

      private static final DirEntry defaultInstance;
      public static DirEntry getDefaultInstance() {
        return defaultInstance;
      }

      public DirEntry getDefaultInstanceForType() {
        return defaultInstance;
      }

      private final com.google.protobuf.UnknownFieldSet unknownFields;
      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
          getUnknownFields() {
        return this.unknownFields;
      }
      private DirEntry(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        initFields();
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
              case 8: {
                bitField0_ |= 0x00000001;
                parent_ = input.readUInt64();
                break;
              }
              case 16: {
                if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                  children_ = new java.util.ArrayList<java.lang.Long>();
                  mutable_bitField0_ |= 0x00000002;
                }
                children_.add(input.readUInt64());
                break;
              }
              case 18: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000002) == 0x00000002) && input.getBytesUntilLimit() > 0) {
                  children_ = new java.util.ArrayList<java.lang.Long>();
                  mutable_bitField0_ |= 0x00000002;
                }
                while (input.getBytesUntilLimit() > 0) {
                  children_.add(input.readUInt64());
                }
                input.popLimit(limit);
                break;
              }
              case 24: {
                if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
                  refChildren_ = new java.util.ArrayList<java.lang.Integer>();
                  mutable_bitField0_ |= 0x00000004;
                }
                refChildren_.add(input.readUInt32());
                break;
              }
              case 26: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000004) == 0x00000004) && input.getBytesUntilLimit() > 0) {
                  refChildren_ = new java.util.ArrayList<java.lang.Integer>();
                  mutable_bitField0_ |= 0x00000004;
                }
                while (input.getBytesUntilLimit() > 0) {
                  refChildren_.add(input.readUInt32());
                }
                input.popLimit(limit);
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e.getMessage()).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
            children_ = java.util.Collections.unmodifiableList(children_);
          }
          if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
            refChildren_ = java.util.Collections.unmodifiableList(refChildren_);
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeDirectorySection_DirEntry_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeDirectorySection_DirEntry_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry.Builder.class);
      }

      public static com.google.protobuf.Parser<DirEntry> PARSER =
          new com.google.protobuf.AbstractParser<DirEntry>() {
        public DirEntry parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new DirEntry(input, extensionRegistry);
        }
      };

      @java.lang.Override
      public com.google.protobuf.Parser<DirEntry> getParserForType() {
        return PARSER;
      }

      private int bitField0_;
      // optional uint64 parent = 1;
      public static final int PARENT_FIELD_NUMBER = 1;
      private long parent_;
      /**
       * <code>optional uint64 parent = 1;</code>
       */
      public boolean hasParent() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint64 parent = 1;</code>
       */
      public long getParent() {
        return parent_;
      }

      // repeated uint64 children = 2 [packed = true];
      public static final int CHILDREN_FIELD_NUMBER = 2;
      private java.util.List<java.lang.Long> children_;
      /**
       * <code>repeated uint64 children = 2 [packed = true];</code>
       *
       * <pre>
       * children that are not reference nodes
       * </pre>
       */
      public java.util.List<java.lang.Long>
          getChildrenList() {
        return children_;
      }
      /**
       * <code>repeated uint64 children = 2 [packed = true];</code>
       *
       * <pre>
       * children that are not reference nodes
       * </pre>
       */
      public int getChildrenCount() {
        return children_.size();
      }
      /**
       * <code>repeated uint64 children = 2 [packed = true];</code>
       *
       * <pre>
       * children that are not reference nodes
       * </pre>
       */
      public long getChildren(int index) {
        return children_.get(index);
      }
      private int childrenMemoizedSerializedSize = -1;

      // repeated uint32 refChildren = 3 [packed = true];
      public static final int REFCHILDREN_FIELD_NUMBER = 3;
      private java.util.List<java.lang.Integer> refChildren_;
      /**
       * <code>repeated uint32 refChildren = 3 [packed = true];</code>
       *
       * <pre>
       * children that are reference nodes, each element is a reference node id
       * </pre>
       */
      public java.util.List<java.lang.Integer>
          getRefChildrenList() {
        return refChildren_;
      }
      /**
       * <code>repeated uint32 refChildren = 3 [packed = true];</code>
       *
       * <pre>
       * children that are reference nodes, each element is a reference node id
       * </pre>
       */
      public int getRefChildrenCount() {
        return refChildren_.size();
      }
      /**
       * <code>repeated uint32 refChildren = 3 [packed = true];</code>
       *
       * <pre>
       * children that are reference nodes, each element is a reference node id
       * </pre>
       */
      public int getRefChildren(int index) {
        return refChildren_.get(index);
      }
      private int refChildrenMemoizedSerializedSize = -1;

      private void initFields() {
        parent_ = 0L;
        children_ = java.util.Collections.emptyList();
        refChildren_ = java.util.Collections.emptyList();
      }
      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized != -1) return isInitialized == 1;

        memoizedIsInitialized = 1;
        return true;
      }

      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          output.writeUInt64(1, parent_);
        }
        if (getChildrenList().size() > 0) {
          output.writeRawVarint32(18);
          output.writeRawVarint32(childrenMemoizedSerializedSize);
        }
        for (int i = 0; i < children_.size(); i++) {
          output.writeUInt64NoTag(children_.get(i));
        }
        if (getRefChildrenList().size() > 0) {
          output.writeRawVarint32(26);
          output.writeRawVarint32(refChildrenMemoizedSerializedSize);
        }
        for (int i = 0; i < refChildren_.size(); i++) {
          output.writeUInt32NoTag(refChildren_.get(i));
        }
        getUnknownFields().writeTo(output);
      }

      private int memoizedSerializedSize = -1;
      public int getSerializedSize() {
        int size = memoizedSerializedSize;
        if (size != -1) return size;

        size = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(1, parent_);
        }
        {
          int dataSize = 0;
          for (int i = 0; i < children_.size(); i++) {
            dataSize += com.google.protobuf.CodedOutputStream
              .computeUInt64SizeNoTag(children_.get(i));
          }
          size += dataSize;
          if (!getChildrenList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          childrenMemoizedSerializedSize = dataSize;
        }
        {
          int dataSize = 0;
          for (int i = 0; i < refChildren_.size(); i++) {
            dataSize += com.google.protobuf.CodedOutputStream
              .computeUInt32SizeNoTag(refChildren_.get(i));
          }
          size += dataSize;
          if (!getRefChildrenList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          refChildrenMemoizedSerializedSize = dataSize;
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSerializedSize = size;
        return size;
      }

      private static final long serialVersionUID = 0L;
      @java.lang.Override
      protected java.lang.Object writeReplace()
          throws java.io.ObjectStreamException {
        return super.writeReplace();
      }

      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }

      public static Builder newBuilder() { return Builder.create(); }
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry prototype) {
        return newBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() { return newBuilder(this); }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code hadoop.hdfs.fsimage.INodeDirectorySection.DirEntry}
       *
       * <pre>
       **
       * A single DirEntry needs to fit in the default PB max message size of
       * 64MB. Please be careful when adding more fields to a DirEntry!
       * </pre>
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder>
         implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntryOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeDirectorySection_DirEntry_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeDirectorySection_DirEntry_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry.Builder.class);
        }

        // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          }
        }
        private static Builder create() {
          return new Builder();
        }

        public Builder clear() {
          super.clear();
          parent_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000001);
          children_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          refChildren_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          return this;
        }

        public Builder clone() {
          return create().mergeFrom(buildPartial());
        }

        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeDirectorySection_DirEntry_descriptor;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry getDefaultInstanceForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry.getDefaultInstance();
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry build() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry buildPartial() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
            to_bitField0_ |= 0x00000001;
          }
          result.parent_ = parent_;
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            children_ = java.util.Collections.unmodifiableList(children_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.children_ = children_;
          if (((bitField0_ & 0x00000004) == 0x00000004)) {
            refChildren_ = java.util.Collections.unmodifiableList(refChildren_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.refChildren_ = refChildren_;
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry) {
            return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry other) {
          if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry.getDefaultInstance()) return this;
          if (other.hasParent()) {
            setParent(other.getParent());
          }
          if (!other.children_.isEmpty()) {
            if (children_.isEmpty()) {
              children_ = other.children_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureChildrenIsMutable();
              children_.addAll(other.children_);
            }
            onChanged();
          }
          if (!other.refChildren_.isEmpty()) {
            if (refChildren_.isEmpty()) {
              refChildren_ = other.refChildren_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureRefChildrenIsMutable();
              refChildren_.addAll(other.refChildren_);
            }
            onChanged();
          }
          this.mergeUnknownFields(other.getUnknownFields());
          return this;
        }

        public final boolean isInitialized() {
          return true;
        }

        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.DirEntry) e.getUnfinishedMessage();
            throw e;
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        // optional uint64 parent = 1;
        private long parent_ ;
        /**
         * <code>optional uint64 parent = 1;</code>
         */
        public boolean hasParent() {
          return ((bitField0_ & 0x00000001) == 0x00000001);
        }
        /**
         * <code>optional uint64 parent = 1;</code>
         */
        public long getParent() {
          return parent_;
        }
        /**
         * <code>optional uint64 parent = 1;</code>
         */
        public Builder setParent(long value) {
          bitField0_ |= 0x00000001;
          parent_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 parent = 1;</code>
         */
        public Builder clearParent() {
          bitField0_ = (bitField0_ & ~0x00000001);
          parent_ = 0L;
          onChanged();
          return this;
        }

        // repeated uint64 children = 2 [packed = true];
        private java.util.List<java.lang.Long> children_ = java.util.Collections.emptyList();
        private void ensureChildrenIsMutable() {
          if (!((bitField0_ & 0x00000002) == 0x00000002)) {
            children_ = new java.util.ArrayList<java.lang.Long>(children_);
            bitField0_ |= 0x00000002;
           }
        }
        /**
         * <code>repeated uint64 children = 2 [packed = true];</code>
         *
         * <pre>
         * children that are not reference nodes
         * </pre>
         */
        public java.util.List<java.lang.Long>
            getChildrenList() {
          return java.util.Collections.unmodifiableList(children_);
        }
        /**
         * <code>repeated uint64 children = 2 [packed = true];</code>
         *
         * <pre>
         * children that are not reference nodes
         * </pre>
         */
        public int getChildrenCount() {
          return children_.size();
        }
        /**
         * <code>repeated uint64 children = 2 [packed = true];</code>
         *
         * <pre>
         * children that are not reference nodes
         * </pre>
         */
        public long getChildren(int index) {
          return children_.get(index);
        }
        /**
         * <code>repeated uint64 children = 2 [packed = true];</code>
         *
         * <pre>
         * children that are not reference nodes
         * </pre>
         */
        public Builder setChildren(
            int index, long value) {
          ensureChildrenIsMutable();
          children_.set(index, value);
          onChanged();
          return this;
        }
        /**
         * <code>repeated uint64 children = 2 [packed = true];</code>
         *
         * <pre>
         * children that are not reference nodes
         * </pre>
         */
        public Builder addChildren(long value) {
          ensureChildrenIsMutable();
          children_.add(value);
          onChanged();
          return this;
        }
        /**
         * <code>repeated uint64 children = 2 [packed = true];</code>
         *
         * <pre>
         * children that are not reference nodes
         * </pre>
         */
        public Builder addAllChildren(
            java.lang.Iterable<? extends java.lang.Long> values) {
          ensureChildrenIsMutable();
          super.addAll(values, children_);
          onChanged();
          return this;
        }
        /**
         * <code>repeated uint64 children = 2 [packed = true];</code>
         *
         * <pre>
         * children that are not reference nodes
         * </pre>
         */
        public Builder clearChildren() {
          children_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
          return this;
        }

        // repeated uint32 refChildren = 3 [packed = true];
        private java.util.List<java.lang.Integer> refChildren_ = java.util.Collections.emptyList();
        private void ensureRefChildrenIsMutable() {
          if (!((bitField0_ & 0x00000004) == 0x00000004)) {
            refChildren_ = new java.util.ArrayList<java.lang.Integer>(refChildren_);
            bitField0_ |= 0x00000004;
           }
        }
        /**
         * <code>repeated uint32 refChildren = 3 [packed = true];</code>
         *
         * <pre>
         * children that are reference nodes, each element is a reference node id
         * </pre>
         */
        public java.util.List<java.lang.Integer>
            getRefChildrenList() {
          return java.util.Collections.unmodifiableList(refChildren_);
        }
        /**
         * <code>repeated uint32 refChildren = 3 [packed = true];</code>
         *
         * <pre>
         * children that are reference nodes, each element is a reference node id
         * </pre>
         */
        public int getRefChildrenCount() {
          return refChildren_.size();
        }
        /**
         * <code>repeated uint32 refChildren = 3 [packed = true];</code>
         *
         * <pre>
         * children that are reference nodes, each element is a reference node id
         * </pre>
         */
        public int getRefChildren(int index) {
          return refChildren_.get(index);
        }
        /**
         * <code>repeated uint32 refChildren = 3 [packed = true];</code>
         *
         * <pre>
         * children that are reference nodes, each element is a reference node id
         * </pre>
         */
        public Builder setRefChildren(
            int index, int value) {
          ensureRefChildrenIsMutable();
          refChildren_.set(index, value);
          onChanged();
          return this;
        }
        /**
         * <code>repeated uint32 refChildren = 3 [packed = true];</code>
         *
         * <pre>
         * children that are reference nodes, each element is a reference node id
         * </pre>
         */
        public Builder addRefChildren(int value) {
          ensureRefChildrenIsMutable();
          refChildren_.add(value);
          onChanged();
          return this;
        }
        /**
         * <code>repeated uint32 refChildren = 3 [packed = true];</code>
         *
         * <pre>
         * children that are reference nodes, each element is a reference node id
         * </pre>
         */
        public Builder addAllRefChildren(
            java.lang.Iterable<? extends java.lang.Integer> values) {
          ensureRefChildrenIsMutable();
          super.addAll(values, refChildren_);
          onChanged();
          return this;
        }
        /**
         * <code>repeated uint32 refChildren = 3 [packed = true];</code>
         *
         * <pre>
         * children that are reference nodes, each element is a reference node id
         * </pre>
         */
        public Builder clearRefChildren() {
          refChildren_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.INodeDirectorySection.DirEntry)
      }

      static {
        defaultInstance = new DirEntry(true);
        defaultInstance.initFields();
      }

      // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.INodeDirectorySection.DirEntry)
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.INodeDirectorySection}
     *
     * <pre>
     **
     * This section records the children of each directories
     * NAME: INODE_DIR
     * </pre>
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySectionOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeDirectorySection_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeDirectorySection_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeDirectorySection_descriptor;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection build() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection buildPartial() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection) {
          return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection other) {
        if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeDirectorySection) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.INodeDirectorySection)
    }

    static {
      defaultInstance = new INodeDirectorySection(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.INodeDirectorySection)
  }

  public interface INodeReferenceSectionOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.hdfs.fsimage.INodeReferenceSection}
   */
  public static final class INodeReferenceSection extends
      com.google.protobuf.GeneratedMessage
      implements INodeReferenceSectionOrBuilder {
    // Use INodeReferenceSection.newBuilder() to construct.
    private INodeReferenceSection(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private INodeReferenceSection(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final INodeReferenceSection defaultInstance;
    public static INodeReferenceSection getDefaultInstance() {
      return defaultInstance;
    }

    public INodeReferenceSection getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private INodeReferenceSection(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeReferenceSection_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeReferenceSection_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.Builder.class);
    }

    public static com.google.protobuf.Parser<INodeReferenceSection> PARSER =
        new com.google.protobuf.AbstractParser<INodeReferenceSection>() {
      public INodeReferenceSection parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new INodeReferenceSection(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<INodeReferenceSection> getParserForType() {
      return PARSER;
    }

    public interface INodeReferenceOrBuilder
        extends com.google.protobuf.MessageOrBuilder {

      // optional uint64 referredId = 1;
      /**
       * <code>optional uint64 referredId = 1;</code>
       *
       * <pre>
       * id of the referred inode
       * </pre>
       */
      boolean hasReferredId();
      /**
       * <code>optional uint64 referredId = 1;</code>
       *
       * <pre>
       * id of the referred inode
       * </pre>
       */
      long getReferredId();

      // optional bytes name = 2;
      /**
       * <code>optional bytes name = 2;</code>
       *
       * <pre>
       * local name recorded in WithName
       * </pre>
       */
      boolean hasName();
      /**
       * <code>optional bytes name = 2;</code>
       *
       * <pre>
       * local name recorded in WithName
       * </pre>
       */
      com.google.protobuf.ByteString getName();

      // optional uint64 dstSnapshotId = 3;
      /**
       * <code>optional uint64 dstSnapshotId = 3;</code>
       *
       * <pre>
       * recorded in DstReference
       * </pre>
       */
      boolean hasDstSnapshotId();
      /**
       * <code>optional uint64 dstSnapshotId = 3;</code>
       *
       * <pre>
       * recorded in DstReference
       * </pre>
       */
      long getDstSnapshotId();

      // optional uint64 lastSnapshotId = 4;
      /**
       * <code>optional uint64 lastSnapshotId = 4;</code>
       *
       * <pre>
       * recorded in WithName
       * </pre>
       */
      boolean hasLastSnapshotId();
      /**
       * <code>optional uint64 lastSnapshotId = 4;</code>
       *
       * <pre>
       * recorded in WithName
       * </pre>
       */
      long getLastSnapshotId();
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.INodeReferenceSection.INodeReference}
     */
    public static final class INodeReference extends
        com.google.protobuf.GeneratedMessage
        implements INodeReferenceOrBuilder {
      // Use INodeReference.newBuilder() to construct.
      private INodeReference(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
        super(builder);
        this.unknownFields = builder.getUnknownFields();
      }
      private INodeReference(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

      private static final INodeReference defaultInstance;
      public static INodeReference getDefaultInstance() {
        return defaultInstance;
      }

      public INodeReference getDefaultInstanceForType() {
        return defaultInstance;
      }

      private final com.google.protobuf.UnknownFieldSet unknownFields;
      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
          getUnknownFields() {
        return this.unknownFields;
      }
      private INodeReference(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        initFields();
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
              case 8: {
                bitField0_ |= 0x00000001;
                referredId_ = input.readUInt64();
                break;
              }
              case 18: {
                bitField0_ |= 0x00000002;
                name_ = input.readBytes();
                break;
              }
              case 24: {
                bitField0_ |= 0x00000004;
                dstSnapshotId_ = input.readUInt64();
                break;
              }
              case 32: {
                bitField0_ |= 0x00000008;
                lastSnapshotId_ = input.readUInt64();
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e.getMessage()).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeReferenceSection_INodeReference_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeReferenceSection_INodeReference_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference.Builder.class);
      }

      public static com.google.protobuf.Parser<INodeReference> PARSER =
          new com.google.protobuf.AbstractParser<INodeReference>() {
        public INodeReference parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new INodeReference(input, extensionRegistry);
        }
      };

      @java.lang.Override
      public com.google.protobuf.Parser<INodeReference> getParserForType() {
        return PARSER;
      }

      private int bitField0_;
      // optional uint64 referredId = 1;
      public static final int REFERREDID_FIELD_NUMBER = 1;
      private long referredId_;
      /**
       * <code>optional uint64 referredId = 1;</code>
       *
       * <pre>
       * id of the referred inode
       * </pre>
       */
      public boolean hasReferredId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint64 referredId = 1;</code>
       *
       * <pre>
       * id of the referred inode
       * </pre>
       */
      public long getReferredId() {
        return referredId_;
      }

      // optional bytes name = 2;
      public static final int NAME_FIELD_NUMBER = 2;
      private com.google.protobuf.ByteString name_;
      /**
       * <code>optional bytes name = 2;</code>
       *
       * <pre>
       * local name recorded in WithName
       * </pre>
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional bytes name = 2;</code>
       *
       * <pre>
       * local name recorded in WithName
       * </pre>
       */
      public com.google.protobuf.ByteString getName() {
        return name_;
      }

      // optional uint64 dstSnapshotId = 3;
      public static final int DSTSNAPSHOTID_FIELD_NUMBER = 3;
      private long dstSnapshotId_;
      /**
       * <code>optional uint64 dstSnapshotId = 3;</code>
       *
       * <pre>
       * recorded in DstReference
       * </pre>
       */
      public boolean hasDstSnapshotId() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional uint64 dstSnapshotId = 3;</code>
       *
       * <pre>
       * recorded in DstReference
       * </pre>
       */
      public long getDstSnapshotId() {
        return dstSnapshotId_;
      }

      // optional uint64 lastSnapshotId = 4;
      public static final int LASTSNAPSHOTID_FIELD_NUMBER = 4;
      private long lastSnapshotId_;
      /**
       * <code>optional uint64 lastSnapshotId = 4;</code>
       *
       * <pre>
       * recorded in WithName
       * </pre>
       */
      public boolean hasLastSnapshotId() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional uint64 lastSnapshotId = 4;</code>
       *
       * <pre>
       * recorded in WithName
       * </pre>
       */
      public long getLastSnapshotId() {
        return lastSnapshotId_;
      }

      private void initFields() {
        referredId_ = 0L;
        name_ = com.google.protobuf.ByteString.EMPTY;
        dstSnapshotId_ = 0L;
        lastSnapshotId_ = 0L;
      }
      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized != -1) return isInitialized == 1;

        memoizedIsInitialized = 1;
        return true;
      }

      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          output.writeUInt64(1, referredId_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          output.writeBytes(2, name_);
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          output.writeUInt64(3, dstSnapshotId_);
        }
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          output.writeUInt64(4, lastSnapshotId_);
        }
        getUnknownFields().writeTo(output);
      }

      private int memoizedSerializedSize = -1;
      public int getSerializedSize() {
        int size = memoizedSerializedSize;
        if (size != -1) return size;

        size = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(1, referredId_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          size += com.google.protobuf.CodedOutputStream
            .computeBytesSize(2, name_);
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(3, dstSnapshotId_);
        }
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(4, lastSnapshotId_);
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSerializedSize = size;
        return size;
      }

      private static final long serialVersionUID = 0L;
      @java.lang.Override
      protected java.lang.Object writeReplace()
          throws java.io.ObjectStreamException {
        return super.writeReplace();
      }

      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }

      public static Builder newBuilder() { return Builder.create(); }
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference prototype) {
        return newBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() { return newBuilder(this); }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code hadoop.hdfs.fsimage.INodeReferenceSection.INodeReference}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder>
         implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReferenceOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeReferenceSection_INodeReference_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeReferenceSection_INodeReference_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference.Builder.class);
        }

        // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          }
        }
        private static Builder create() {
          return new Builder();
        }

        public Builder clear() {
          super.clear();
          referredId_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000001);
          name_ = com.google.protobuf.ByteString.EMPTY;
          bitField0_ = (bitField0_ & ~0x00000002);
          dstSnapshotId_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000004);
          lastSnapshotId_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000008);
          return this;
        }

        public Builder clone() {
          return create().mergeFrom(buildPartial());
        }

        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeReferenceSection_INodeReference_descriptor;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference getDefaultInstanceForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference.getDefaultInstance();
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference build() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference buildPartial() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
            to_bitField0_ |= 0x00000001;
          }
          result.referredId_ = referredId_;
          if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
            to_bitField0_ |= 0x00000002;
          }
          result.name_ = name_;
          if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
            to_bitField0_ |= 0x00000004;
          }
          result.dstSnapshotId_ = dstSnapshotId_;
          if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
            to_bitField0_ |= 0x00000008;
          }
          result.lastSnapshotId_ = lastSnapshotId_;
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference) {
            return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference other) {
          if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference.getDefaultInstance()) return this;
          if (other.hasReferredId()) {
            setReferredId(other.getReferredId());
          }
          if (other.hasName()) {
            setName(other.getName());
          }
          if (other.hasDstSnapshotId()) {
            setDstSnapshotId(other.getDstSnapshotId());
          }
          if (other.hasLastSnapshotId()) {
            setLastSnapshotId(other.getLastSnapshotId());
          }
          this.mergeUnknownFields(other.getUnknownFields());
          return this;
        }

        public final boolean isInitialized() {
          return true;
        }

        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.INodeReference) e.getUnfinishedMessage();
            throw e;
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        // optional uint64 referredId = 1;
        private long referredId_ ;
        /**
         * <code>optional uint64 referredId = 1;</code>
         *
         * <pre>
         * id of the referred inode
         * </pre>
         */
        public boolean hasReferredId() {
          return ((bitField0_ & 0x00000001) == 0x00000001);
        }
        /**
         * <code>optional uint64 referredId = 1;</code>
         *
         * <pre>
         * id of the referred inode
         * </pre>
         */
        public long getReferredId() {
          return referredId_;
        }
        /**
         * <code>optional uint64 referredId = 1;</code>
         *
         * <pre>
         * id of the referred inode
         * </pre>
         */
        public Builder setReferredId(long value) {
          bitField0_ |= 0x00000001;
          referredId_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 referredId = 1;</code>
         *
         * <pre>
         * id of the referred inode
         * </pre>
         */
        public Builder clearReferredId() {
          bitField0_ = (bitField0_ & ~0x00000001);
          referredId_ = 0L;
          onChanged();
          return this;
        }

        // optional bytes name = 2;
        private com.google.protobuf.ByteString name_ = com.google.protobuf.ByteString.EMPTY;
        /**
         * <code>optional bytes name = 2;</code>
         *
         * <pre>
         * local name recorded in WithName
         * </pre>
         */
        public boolean hasName() {
          return ((bitField0_ & 0x00000002) == 0x00000002);
        }
        /**
         * <code>optional bytes name = 2;</code>
         *
         * <pre>
         * local name recorded in WithName
         * </pre>
         */
        public com.google.protobuf.ByteString getName() {
          return name_;
        }
        /**
         * <code>optional bytes name = 2;</code>
         *
         * <pre>
         * local name recorded in WithName
         * </pre>
         */
        public Builder setName(com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
          name_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional bytes name = 2;</code>
         *
         * <pre>
         * local name recorded in WithName
         * </pre>
         */
        public Builder clearName() {
          bitField0_ = (bitField0_ & ~0x00000002);
          name_ = getDefaultInstance().getName();
          onChanged();
          return this;
        }

        // optional uint64 dstSnapshotId = 3;
        private long dstSnapshotId_ ;
        /**
         * <code>optional uint64 dstSnapshotId = 3;</code>
         *
         * <pre>
         * recorded in DstReference
         * </pre>
         */
        public boolean hasDstSnapshotId() {
          return ((bitField0_ & 0x00000004) == 0x00000004);
        }
        /**
         * <code>optional uint64 dstSnapshotId = 3;</code>
         *
         * <pre>
         * recorded in DstReference
         * </pre>
         */
        public long getDstSnapshotId() {
          return dstSnapshotId_;
        }
        /**
         * <code>optional uint64 dstSnapshotId = 3;</code>
         *
         * <pre>
         * recorded in DstReference
         * </pre>
         */
        public Builder setDstSnapshotId(long value) {
          bitField0_ |= 0x00000004;
          dstSnapshotId_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 dstSnapshotId = 3;</code>
         *
         * <pre>
         * recorded in DstReference
         * </pre>
         */
        public Builder clearDstSnapshotId() {
          bitField0_ = (bitField0_ & ~0x00000004);
          dstSnapshotId_ = 0L;
          onChanged();
          return this;
        }

        // optional uint64 lastSnapshotId = 4;
        private long lastSnapshotId_ ;
        /**
         * <code>optional uint64 lastSnapshotId = 4;</code>
         *
         * <pre>
         * recorded in WithName
         * </pre>
         */
        public boolean hasLastSnapshotId() {
          return ((bitField0_ & 0x00000008) == 0x00000008);
        }
        /**
         * <code>optional uint64 lastSnapshotId = 4;</code>
         *
         * <pre>
         * recorded in WithName
         * </pre>
         */
        public long getLastSnapshotId() {
          return lastSnapshotId_;
        }
        /**
         * <code>optional uint64 lastSnapshotId = 4;</code>
         *
         * <pre>
         * recorded in WithName
         * </pre>
         */
        public Builder setLastSnapshotId(long value) {
          bitField0_ |= 0x00000008;
          lastSnapshotId_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 lastSnapshotId = 4;</code>
         *
         * <pre>
         * recorded in WithName
         * </pre>
         */
        public Builder clearLastSnapshotId() {
          bitField0_ = (bitField0_ & ~0x00000008);
          lastSnapshotId_ = 0L;
          onChanged();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.INodeReferenceSection.INodeReference)
      }

      static {
        defaultInstance = new INodeReference(true);
        defaultInstance.initFields();
      }

      // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.INodeReferenceSection.INodeReference)
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.INodeReferenceSection}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSectionOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeReferenceSection_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeReferenceSection_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_INodeReferenceSection_descriptor;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection build() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection buildPartial() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection) {
          return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection other) {
        if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeReferenceSection) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.INodeReferenceSection)
    }

    static {
      defaultInstance = new INodeReferenceSection(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.INodeReferenceSection)
  }

  public interface SnapshotSectionOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional uint32 snapshotCounter = 1;
    /**
     * <code>optional uint32 snapshotCounter = 1;</code>
     */
    boolean hasSnapshotCounter();
    /**
     * <code>optional uint32 snapshotCounter = 1;</code>
     */
    int getSnapshotCounter();

    // repeated uint64 snapshottableDir = 2 [packed = true];
    /**
     * <code>repeated uint64 snapshottableDir = 2 [packed = true];</code>
     */
    java.util.List<java.lang.Long> getSnapshottableDirList();
    /**
     * <code>repeated uint64 snapshottableDir = 2 [packed = true];</code>
     */
    int getSnapshottableDirCount();
    /**
     * <code>repeated uint64 snapshottableDir = 2 [packed = true];</code>
     */
    long getSnapshottableDir(int index);

    // optional uint32 numSnapshots = 3;
    /**
     * <code>optional uint32 numSnapshots = 3;</code>
     *
     * <pre>
     * total number of snapshots
     * </pre>
     */
    boolean hasNumSnapshots();
    /**
     * <code>optional uint32 numSnapshots = 3;</code>
     *
     * <pre>
     * total number of snapshots
     * </pre>
     */
    int getNumSnapshots();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.fsimage.SnapshotSection}
   *
   * <pre>
   **
   * This section records the information about snapshot
   * NAME: SNAPSHOT
   * </pre>
   */
  public static final class SnapshotSection extends
      com.google.protobuf.GeneratedMessage
      implements SnapshotSectionOrBuilder {
    // Use SnapshotSection.newBuilder() to construct.
    private SnapshotSection(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private SnapshotSection(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final SnapshotSection defaultInstance;
    public static SnapshotSection getDefaultInstance() {
      return defaultInstance;
    }

    public SnapshotSection getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private SnapshotSection(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              snapshotCounter_ = input.readUInt32();
              break;
            }
            case 16: {
              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                snapshottableDir_ = new java.util.ArrayList<java.lang.Long>();
                mutable_bitField0_ |= 0x00000002;
              }
              snapshottableDir_.add(input.readUInt64());
              break;
            }
            case 18: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002) && input.getBytesUntilLimit() > 0) {
                snapshottableDir_ = new java.util.ArrayList<java.lang.Long>();
                mutable_bitField0_ |= 0x00000002;
              }
              while (input.getBytesUntilLimit() > 0) {
                snapshottableDir_.add(input.readUInt64());
              }
              input.popLimit(limit);
              break;
            }
            case 24: {
              bitField0_ |= 0x00000002;
              numSnapshots_ = input.readUInt32();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
          snapshottableDir_ = java.util.Collections.unmodifiableList(snapshottableDir_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotSection_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotSection_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Builder.class);
    }

    public static com.google.protobuf.Parser<SnapshotSection> PARSER =
        new com.google.protobuf.AbstractParser<SnapshotSection>() {
      public SnapshotSection parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SnapshotSection(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<SnapshotSection> getParserForType() {
      return PARSER;
    }

    public interface SnapshotOrBuilder
        extends com.google.protobuf.MessageOrBuilder {

      // optional uint64 snapshotId = 1;
      /**
       * <code>optional uint64 snapshotId = 1;</code>
       */
      boolean hasSnapshotId();
      /**
       * <code>optional uint64 snapshotId = 1;</code>
       */
      long getSnapshotId();

      // optional .hadoop.hdfs.fsimage.INodeSection.INode root = 2;
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INode root = 2;</code>
       *
       * <pre>
       * Snapshot root
       * </pre>
       */
      boolean hasRoot();
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INode root = 2;</code>
       *
       * <pre>
       * Snapshot root
       * </pre>
       */
      org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode getRoot();
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INode root = 2;</code>
       *
       * <pre>
       * Snapshot root
       * </pre>
       */
      org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeOrBuilder getRootOrBuilder();
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.SnapshotSection.Snapshot}
     */
    public static final class Snapshot extends
        com.google.protobuf.GeneratedMessage
        implements SnapshotOrBuilder {
      // Use Snapshot.newBuilder() to construct.
      private Snapshot(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
        super(builder);
        this.unknownFields = builder.getUnknownFields();
      }
      private Snapshot(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

      private static final Snapshot defaultInstance;
      public static Snapshot getDefaultInstance() {
        return defaultInstance;
      }

      public Snapshot getDefaultInstanceForType() {
        return defaultInstance;
      }

      private final com.google.protobuf.UnknownFieldSet unknownFields;
      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
          getUnknownFields() {
        return this.unknownFields;
      }
      private Snapshot(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        initFields();
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
              case 8: {
                bitField0_ |= 0x00000001;
                snapshotId_ = input.readUInt64();
                break;
              }
              case 18: {
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.Builder subBuilder = null;
                if (((bitField0_ & 0x00000002) == 0x00000002)) {
                  subBuilder = root_.toBuilder();
                }
                root_ = input.readMessage(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.PARSER, extensionRegistry);
                if (subBuilder != null) {
                  subBuilder.mergeFrom(root_);
                  root_ = subBuilder.buildPartial();
                }
                bitField0_ |= 0x00000002;
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e.getMessage()).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotSection_Snapshot_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotSection_Snapshot_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot.Builder.class);
      }

      public static com.google.protobuf.Parser<Snapshot> PARSER =
          new com.google.protobuf.AbstractParser<Snapshot>() {
        public Snapshot parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Snapshot(input, extensionRegistry);
        }
      };

      @java.lang.Override
      public com.google.protobuf.Parser<Snapshot> getParserForType() {
        return PARSER;
      }

      private int bitField0_;
      // optional uint64 snapshotId = 1;
      public static final int SNAPSHOTID_FIELD_NUMBER = 1;
      private long snapshotId_;
      /**
       * <code>optional uint64 snapshotId = 1;</code>
       */
      public boolean hasSnapshotId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint64 snapshotId = 1;</code>
       */
      public long getSnapshotId() {
        return snapshotId_;
      }

      // optional .hadoop.hdfs.fsimage.INodeSection.INode root = 2;
      public static final int ROOT_FIELD_NUMBER = 2;
      private org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode root_;
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INode root = 2;</code>
       *
       * <pre>
       * Snapshot root
       * </pre>
       */
      public boolean hasRoot() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INode root = 2;</code>
       *
       * <pre>
       * Snapshot root
       * </pre>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode getRoot() {
        return root_;
      }
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INode root = 2;</code>
       *
       * <pre>
       * Snapshot root
       * </pre>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeOrBuilder getRootOrBuilder() {
        return root_;
      }

      private void initFields() {
        snapshotId_ = 0L;
        root_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.getDefaultInstance();
      }
      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized != -1) return isInitialized == 1;

        if (hasRoot()) {
          if (!getRoot().isInitialized()) {
            memoizedIsInitialized = 0;
            return false;
          }
        }
        memoizedIsInitialized = 1;
        return true;
      }

      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          output.writeUInt64(1, snapshotId_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          output.writeMessage(2, root_);
        }
        getUnknownFields().writeTo(output);
      }

      private int memoizedSerializedSize = -1;
      public int getSerializedSize() {
        int size = memoizedSerializedSize;
        if (size != -1) return size;

        size = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(1, snapshotId_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(2, root_);
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSerializedSize = size;
        return size;
      }

      private static final long serialVersionUID = 0L;
      @java.lang.Override
      protected java.lang.Object writeReplace()
          throws java.io.ObjectStreamException {
        return super.writeReplace();
      }

      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }

      public static Builder newBuilder() { return Builder.create(); }
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot prototype) {
        return newBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() { return newBuilder(this); }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code hadoop.hdfs.fsimage.SnapshotSection.Snapshot}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder>
         implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.SnapshotOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotSection_Snapshot_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotSection_Snapshot_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot.Builder.class);
        }

        // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
            getRootFieldBuilder();
          }
        }
        private static Builder create() {
          return new Builder();
        }

        public Builder clear() {
          super.clear();
          snapshotId_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000001);
          if (rootBuilder_ == null) {
            root_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.getDefaultInstance();
          } else {
            rootBuilder_.clear();
          }
          bitField0_ = (bitField0_ & ~0x00000002);
          return this;
        }

        public Builder clone() {
          return create().mergeFrom(buildPartial());
        }

        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotSection_Snapshot_descriptor;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot getDefaultInstanceForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot.getDefaultInstance();
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot build() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot buildPartial() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
            to_bitField0_ |= 0x00000001;
          }
          result.snapshotId_ = snapshotId_;
          if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
            to_bitField0_ |= 0x00000002;
          }
          if (rootBuilder_ == null) {
            result.root_ = root_;
          } else {
            result.root_ = rootBuilder_.build();
          }
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot) {
            return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot other) {
          if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot.getDefaultInstance()) return this;
          if (other.hasSnapshotId()) {
            setSnapshotId(other.getSnapshotId());
          }
          if (other.hasRoot()) {
            mergeRoot(other.getRoot());
          }
          this.mergeUnknownFields(other.getUnknownFields());
          return this;
        }

        public final boolean isInitialized() {
          if (hasRoot()) {
            if (!getRoot().isInitialized()) {
              
              return false;
            }
          }
          return true;
        }

        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Snapshot) e.getUnfinishedMessage();
            throw e;
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        // optional uint64 snapshotId = 1;
        private long snapshotId_ ;
        /**
         * <code>optional uint64 snapshotId = 1;</code>
         */
        public boolean hasSnapshotId() {
          return ((bitField0_ & 0x00000001) == 0x00000001);
        }
        /**
         * <code>optional uint64 snapshotId = 1;</code>
         */
        public long getSnapshotId() {
          return snapshotId_;
        }
        /**
         * <code>optional uint64 snapshotId = 1;</code>
         */
        public Builder setSnapshotId(long value) {
          bitField0_ |= 0x00000001;
          snapshotId_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 snapshotId = 1;</code>
         */
        public Builder clearSnapshotId() {
          bitField0_ = (bitField0_ & ~0x00000001);
          snapshotId_ = 0L;
          onChanged();
          return this;
        }

        // optional .hadoop.hdfs.fsimage.INodeSection.INode root = 2;
        private org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode root_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.getDefaultInstance();
        private com.google.protobuf.SingleFieldBuilder<
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeOrBuilder> rootBuilder_;
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INode root = 2;</code>
         *
         * <pre>
         * Snapshot root
         * </pre>
         */
        public boolean hasRoot() {
          return ((bitField0_ & 0x00000002) == 0x00000002);
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INode root = 2;</code>
         *
         * <pre>
         * Snapshot root
         * </pre>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode getRoot() {
          if (rootBuilder_ == null) {
            return root_;
          } else {
            return rootBuilder_.getMessage();
          }
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INode root = 2;</code>
         *
         * <pre>
         * Snapshot root
         * </pre>
         */
        public Builder setRoot(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode value) {
          if (rootBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            root_ = value;
            onChanged();
          } else {
            rootBuilder_.setMessage(value);
          }
          bitField0_ |= 0x00000002;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INode root = 2;</code>
         *
         * <pre>
         * Snapshot root
         * </pre>
         */
        public Builder setRoot(
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.Builder builderForValue) {
          if (rootBuilder_ == null) {
            root_ = builderForValue.build();
            onChanged();
          } else {
            rootBuilder_.setMessage(builderForValue.build());
          }
          bitField0_ |= 0x00000002;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INode root = 2;</code>
         *
         * <pre>
         * Snapshot root
         * </pre>
         */
        public Builder mergeRoot(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode value) {
          if (rootBuilder_ == null) {
            if (((bitField0_ & 0x00000002) == 0x00000002) &&
                root_ != org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.getDefaultInstance()) {
              root_ =
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.newBuilder(root_).mergeFrom(value).buildPartial();
            } else {
              root_ = value;
            }
            onChanged();
          } else {
            rootBuilder_.mergeFrom(value);
          }
          bitField0_ |= 0x00000002;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INode root = 2;</code>
         *
         * <pre>
         * Snapshot root
         * </pre>
         */
        public Builder clearRoot() {
          if (rootBuilder_ == null) {
            root_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.getDefaultInstance();
            onChanged();
          } else {
            rootBuilder_.clear();
          }
          bitField0_ = (bitField0_ & ~0x00000002);
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INode root = 2;</code>
         *
         * <pre>
         * Snapshot root
         * </pre>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.Builder getRootBuilder() {
          bitField0_ |= 0x00000002;
          onChanged();
          return getRootFieldBuilder().getBuilder();
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INode root = 2;</code>
         *
         * <pre>
         * Snapshot root
         * </pre>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeOrBuilder getRootOrBuilder() {
          if (rootBuilder_ != null) {
            return rootBuilder_.getMessageOrBuilder();
          } else {
            return root_;
          }
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INode root = 2;</code>
         *
         * <pre>
         * Snapshot root
         * </pre>
         */
        private com.google.protobuf.SingleFieldBuilder<
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeOrBuilder> 
            getRootFieldBuilder() {
          if (rootBuilder_ == null) {
            rootBuilder_ = new com.google.protobuf.SingleFieldBuilder<
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeOrBuilder>(
                    root_,
                    getParentForChildren(),
                    isClean());
            root_ = null;
          }
          return rootBuilder_;
        }

        // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.SnapshotSection.Snapshot)
      }

      static {
        defaultInstance = new Snapshot(true);
        defaultInstance.initFields();
      }

      // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.SnapshotSection.Snapshot)
    }

    private int bitField0_;
    // optional uint32 snapshotCounter = 1;
    public static final int SNAPSHOTCOUNTER_FIELD_NUMBER = 1;
    private int snapshotCounter_;
    /**
     * <code>optional uint32 snapshotCounter = 1;</code>
     */
    public boolean hasSnapshotCounter() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional uint32 snapshotCounter = 1;</code>
     */
    public int getSnapshotCounter() {
      return snapshotCounter_;
    }

    // repeated uint64 snapshottableDir = 2 [packed = true];
    public static final int SNAPSHOTTABLEDIR_FIELD_NUMBER = 2;
    private java.util.List<java.lang.Long> snapshottableDir_;
    /**
     * <code>repeated uint64 snapshottableDir = 2 [packed = true];</code>
     */
    public java.util.List<java.lang.Long>
        getSnapshottableDirList() {
      return snapshottableDir_;
    }
    /**
     * <code>repeated uint64 snapshottableDir = 2 [packed = true];</code>
     */
    public int getSnapshottableDirCount() {
      return snapshottableDir_.size();
    }
    /**
     * <code>repeated uint64 snapshottableDir = 2 [packed = true];</code>
     */
    public long getSnapshottableDir(int index) {
      return snapshottableDir_.get(index);
    }
    private int snapshottableDirMemoizedSerializedSize = -1;

    // optional uint32 numSnapshots = 3;
    public static final int NUMSNAPSHOTS_FIELD_NUMBER = 3;
    private int numSnapshots_;
    /**
     * <code>optional uint32 numSnapshots = 3;</code>
     *
     * <pre>
     * total number of snapshots
     * </pre>
     */
    public boolean hasNumSnapshots() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional uint32 numSnapshots = 3;</code>
     *
     * <pre>
     * total number of snapshots
     * </pre>
     */
    public int getNumSnapshots() {
      return numSnapshots_;
    }

    private void initFields() {
      snapshotCounter_ = 0;
      snapshottableDir_ = java.util.Collections.emptyList();
      numSnapshots_ = 0;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeUInt32(1, snapshotCounter_);
      }
      if (getSnapshottableDirList().size() > 0) {
        output.writeRawVarint32(18);
        output.writeRawVarint32(snapshottableDirMemoizedSerializedSize);
      }
      for (int i = 0; i < snapshottableDir_.size(); i++) {
        output.writeUInt64NoTag(snapshottableDir_.get(i));
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt32(3, numSnapshots_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(1, snapshotCounter_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < snapshottableDir_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeUInt64SizeNoTag(snapshottableDir_.get(i));
        }
        size += dataSize;
        if (!getSnapshottableDirList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        snapshottableDirMemoizedSerializedSize = dataSize;
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(3, numSnapshots_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.SnapshotSection}
     *
     * <pre>
     **
     * This section records the information about snapshot
     * NAME: SNAPSHOT
     * </pre>
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSectionOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotSection_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotSection_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        snapshotCounter_ = 0;
        bitField0_ = (bitField0_ & ~0x00000001);
        snapshottableDir_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000002);
        numSnapshots_ = 0;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotSection_descriptor;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection build() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection buildPartial() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.snapshotCounter_ = snapshotCounter_;
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          snapshottableDir_ = java.util.Collections.unmodifiableList(snapshottableDir_);
          bitField0_ = (bitField0_ & ~0x00000002);
        }
        result.snapshottableDir_ = snapshottableDir_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000002;
        }
        result.numSnapshots_ = numSnapshots_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection) {
          return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection other) {
        if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection.getDefaultInstance()) return this;
        if (other.hasSnapshotCounter()) {
          setSnapshotCounter(other.getSnapshotCounter());
        }
        if (!other.snapshottableDir_.isEmpty()) {
          if (snapshottableDir_.isEmpty()) {
            snapshottableDir_ = other.snapshottableDir_;
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            ensureSnapshottableDirIsMutable();
            snapshottableDir_.addAll(other.snapshottableDir_);
          }
          onChanged();
        }
        if (other.hasNumSnapshots()) {
          setNumSnapshots(other.getNumSnapshots());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotSection) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional uint32 snapshotCounter = 1;
      private int snapshotCounter_ ;
      /**
       * <code>optional uint32 snapshotCounter = 1;</code>
       */
      public boolean hasSnapshotCounter() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint32 snapshotCounter = 1;</code>
       */
      public int getSnapshotCounter() {
        return snapshotCounter_;
      }
      /**
       * <code>optional uint32 snapshotCounter = 1;</code>
       */
      public Builder setSnapshotCounter(int value) {
        bitField0_ |= 0x00000001;
        snapshotCounter_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 snapshotCounter = 1;</code>
       */
      public Builder clearSnapshotCounter() {
        bitField0_ = (bitField0_ & ~0x00000001);
        snapshotCounter_ = 0;
        onChanged();
        return this;
      }

      // repeated uint64 snapshottableDir = 2 [packed = true];
      private java.util.List<java.lang.Long> snapshottableDir_ = java.util.Collections.emptyList();
      private void ensureSnapshottableDirIsMutable() {
        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
          snapshottableDir_ = new java.util.ArrayList<java.lang.Long>(snapshottableDir_);
          bitField0_ |= 0x00000002;
         }
      }
      /**
       * <code>repeated uint64 snapshottableDir = 2 [packed = true];</code>
       */
      public java.util.List<java.lang.Long>
          getSnapshottableDirList() {
        return java.util.Collections.unmodifiableList(snapshottableDir_);
      }
      /**
       * <code>repeated uint64 snapshottableDir = 2 [packed = true];</code>
       */
      public int getSnapshottableDirCount() {
        return snapshottableDir_.size();
      }
      /**
       * <code>repeated uint64 snapshottableDir = 2 [packed = true];</code>
       */
      public long getSnapshottableDir(int index) {
        return snapshottableDir_.get(index);
      }
      /**
       * <code>repeated uint64 snapshottableDir = 2 [packed = true];</code>
       */
      public Builder setSnapshottableDir(
          int index, long value) {
        ensureSnapshottableDirIsMutable();
        snapshottableDir_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated uint64 snapshottableDir = 2 [packed = true];</code>
       */
      public Builder addSnapshottableDir(long value) {
        ensureSnapshottableDirIsMutable();
        snapshottableDir_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated uint64 snapshottableDir = 2 [packed = true];</code>
       */
      public Builder addAllSnapshottableDir(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureSnapshottableDirIsMutable();
        super.addAll(values, snapshottableDir_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated uint64 snapshottableDir = 2 [packed = true];</code>
       */
      public Builder clearSnapshottableDir() {
        snapshottableDir_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
        return this;
      }

      // optional uint32 numSnapshots = 3;
      private int numSnapshots_ ;
      /**
       * <code>optional uint32 numSnapshots = 3;</code>
       *
       * <pre>
       * total number of snapshots
       * </pre>
       */
      public boolean hasNumSnapshots() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional uint32 numSnapshots = 3;</code>
       *
       * <pre>
       * total number of snapshots
       * </pre>
       */
      public int getNumSnapshots() {
        return numSnapshots_;
      }
      /**
       * <code>optional uint32 numSnapshots = 3;</code>
       *
       * <pre>
       * total number of snapshots
       * </pre>
       */
      public Builder setNumSnapshots(int value) {
        bitField0_ |= 0x00000004;
        numSnapshots_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 numSnapshots = 3;</code>
       *
       * <pre>
       * total number of snapshots
       * </pre>
       */
      public Builder clearNumSnapshots() {
        bitField0_ = (bitField0_ & ~0x00000004);
        numSnapshots_ = 0;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.SnapshotSection)
    }

    static {
      defaultInstance = new SnapshotSection(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.SnapshotSection)
  }

  public interface SnapshotDiffSectionOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.hdfs.fsimage.SnapshotDiffSection}
   *
   * <pre>
   **
   * This section records information about snapshot diffs
   * NAME: SNAPSHOT_DIFF
   * </pre>
   */
  public static final class SnapshotDiffSection extends
      com.google.protobuf.GeneratedMessage
      implements SnapshotDiffSectionOrBuilder {
    // Use SnapshotDiffSection.newBuilder() to construct.
    private SnapshotDiffSection(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private SnapshotDiffSection(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final SnapshotDiffSection defaultInstance;
    public static SnapshotDiffSection getDefaultInstance() {
      return defaultInstance;
    }

    public SnapshotDiffSection getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private SnapshotDiffSection(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.Builder.class);
    }

    public static com.google.protobuf.Parser<SnapshotDiffSection> PARSER =
        new com.google.protobuf.AbstractParser<SnapshotDiffSection>() {
      public SnapshotDiffSection parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SnapshotDiffSection(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<SnapshotDiffSection> getParserForType() {
      return PARSER;
    }

    public interface CreatedListEntryOrBuilder
        extends com.google.protobuf.MessageOrBuilder {

      // optional bytes name = 1;
      /**
       * <code>optional bytes name = 1;</code>
       */
      boolean hasName();
      /**
       * <code>optional bytes name = 1;</code>
       */
      com.google.protobuf.ByteString getName();
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.SnapshotDiffSection.CreatedListEntry}
     */
    public static final class CreatedListEntry extends
        com.google.protobuf.GeneratedMessage
        implements CreatedListEntryOrBuilder {
      // Use CreatedListEntry.newBuilder() to construct.
      private CreatedListEntry(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
        super(builder);
        this.unknownFields = builder.getUnknownFields();
      }
      private CreatedListEntry(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

      private static final CreatedListEntry defaultInstance;
      public static CreatedListEntry getDefaultInstance() {
        return defaultInstance;
      }

      public CreatedListEntry getDefaultInstanceForType() {
        return defaultInstance;
      }

      private final com.google.protobuf.UnknownFieldSet unknownFields;
      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
          getUnknownFields() {
        return this.unknownFields;
      }
      private CreatedListEntry(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        initFields();
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
              case 10: {
                bitField0_ |= 0x00000001;
                name_ = input.readBytes();
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e.getMessage()).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_CreatedListEntry_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_CreatedListEntry_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry.Builder.class);
      }

      public static com.google.protobuf.Parser<CreatedListEntry> PARSER =
          new com.google.protobuf.AbstractParser<CreatedListEntry>() {
        public CreatedListEntry parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new CreatedListEntry(input, extensionRegistry);
        }
      };

      @java.lang.Override
      public com.google.protobuf.Parser<CreatedListEntry> getParserForType() {
        return PARSER;
      }

      private int bitField0_;
      // optional bytes name = 1;
      public static final int NAME_FIELD_NUMBER = 1;
      private com.google.protobuf.ByteString name_;
      /**
       * <code>optional bytes name = 1;</code>
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional bytes name = 1;</code>
       */
      public com.google.protobuf.ByteString getName() {
        return name_;
      }

      private void initFields() {
        name_ = com.google.protobuf.ByteString.EMPTY;
      }
      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized != -1) return isInitialized == 1;

        memoizedIsInitialized = 1;
        return true;
      }

      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          output.writeBytes(1, name_);
        }
        getUnknownFields().writeTo(output);
      }

      private int memoizedSerializedSize = -1;
      public int getSerializedSize() {
        int size = memoizedSerializedSize;
        if (size != -1) return size;

        size = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          size += com.google.protobuf.CodedOutputStream
            .computeBytesSize(1, name_);
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSerializedSize = size;
        return size;
      }

      private static final long serialVersionUID = 0L;
      @java.lang.Override
      protected java.lang.Object writeReplace()
          throws java.io.ObjectStreamException {
        return super.writeReplace();
      }

      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }

      public static Builder newBuilder() { return Builder.create(); }
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry prototype) {
        return newBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() { return newBuilder(this); }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code hadoop.hdfs.fsimage.SnapshotDiffSection.CreatedListEntry}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder>
         implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntryOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_CreatedListEntry_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_CreatedListEntry_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry.Builder.class);
        }

        // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          }
        }
        private static Builder create() {
          return new Builder();
        }

        public Builder clear() {
          super.clear();
          name_ = com.google.protobuf.ByteString.EMPTY;
          bitField0_ = (bitField0_ & ~0x00000001);
          return this;
        }

        public Builder clone() {
          return create().mergeFrom(buildPartial());
        }

        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_CreatedListEntry_descriptor;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry getDefaultInstanceForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry.getDefaultInstance();
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry build() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry buildPartial() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
            to_bitField0_ |= 0x00000001;
          }
          result.name_ = name_;
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry) {
            return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry other) {
          if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry.getDefaultInstance()) return this;
          if (other.hasName()) {
            setName(other.getName());
          }
          this.mergeUnknownFields(other.getUnknownFields());
          return this;
        }

        public final boolean isInitialized() {
          return true;
        }

        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.CreatedListEntry) e.getUnfinishedMessage();
            throw e;
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        // optional bytes name = 1;
        private com.google.protobuf.ByteString name_ = com.google.protobuf.ByteString.EMPTY;
        /**
         * <code>optional bytes name = 1;</code>
         */
        public boolean hasName() {
          return ((bitField0_ & 0x00000001) == 0x00000001);
        }
        /**
         * <code>optional bytes name = 1;</code>
         */
        public com.google.protobuf.ByteString getName() {
          return name_;
        }
        /**
         * <code>optional bytes name = 1;</code>
         */
        public Builder setName(com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
          name_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional bytes name = 1;</code>
         */
        public Builder clearName() {
          bitField0_ = (bitField0_ & ~0x00000001);
          name_ = getDefaultInstance().getName();
          onChanged();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.SnapshotDiffSection.CreatedListEntry)
      }

      static {
        defaultInstance = new CreatedListEntry(true);
        defaultInstance.initFields();
      }

      // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.SnapshotDiffSection.CreatedListEntry)
    }

    public interface DirectoryDiffOrBuilder
        extends com.google.protobuf.MessageOrBuilder {

      // optional uint64 snapshotId = 1;
      /**
       * <code>optional uint64 snapshotId = 1;</code>
       */
      boolean hasSnapshotId();
      /**
       * <code>optional uint64 snapshotId = 1;</code>
       */
      long getSnapshotId();

      // optional uint32 childrenSize = 2;
      /**
       * <code>optional uint32 childrenSize = 2;</code>
       */
      boolean hasChildrenSize();
      /**
       * <code>optional uint32 childrenSize = 2;</code>
       */
      int getChildrenSize();

      // optional bool isSnapshotRoot = 3;
      /**
       * <code>optional bool isSnapshotRoot = 3;</code>
       */
      boolean hasIsSnapshotRoot();
      /**
       * <code>optional bool isSnapshotRoot = 3;</code>
       */
      boolean getIsSnapshotRoot();

      // optional bytes name = 4;
      /**
       * <code>optional bytes name = 4;</code>
       */
      boolean hasName();
      /**
       * <code>optional bytes name = 4;</code>
       */
      com.google.protobuf.ByteString getName();

      // optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory snapshotCopy = 5;
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory snapshotCopy = 5;</code>
       */
      boolean hasSnapshotCopy();
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory snapshotCopy = 5;</code>
       */
      org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory getSnapshotCopy();
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory snapshotCopy = 5;</code>
       */
      org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectoryOrBuilder getSnapshotCopyOrBuilder();

      // optional uint32 createdListSize = 6;
      /**
       * <code>optional uint32 createdListSize = 6;</code>
       */
      boolean hasCreatedListSize();
      /**
       * <code>optional uint32 createdListSize = 6;</code>
       */
      int getCreatedListSize();

      // repeated uint64 deletedINode = 7 [packed = true];
      /**
       * <code>repeated uint64 deletedINode = 7 [packed = true];</code>
       *
       * <pre>
       * id of deleted inodes
       * </pre>
       */
      java.util.List<java.lang.Long> getDeletedINodeList();
      /**
       * <code>repeated uint64 deletedINode = 7 [packed = true];</code>
       *
       * <pre>
       * id of deleted inodes
       * </pre>
       */
      int getDeletedINodeCount();
      /**
       * <code>repeated uint64 deletedINode = 7 [packed = true];</code>
       *
       * <pre>
       * id of deleted inodes
       * </pre>
       */
      long getDeletedINode(int index);

      // repeated uint32 deletedINodeRef = 8 [packed = true];
      /**
       * <code>repeated uint32 deletedINodeRef = 8 [packed = true];</code>
       *
       * <pre>
       * id of reference nodes in the deleted list
       * </pre>
       */
      java.util.List<java.lang.Integer> getDeletedINodeRefList();
      /**
       * <code>repeated uint32 deletedINodeRef = 8 [packed = true];</code>
       *
       * <pre>
       * id of reference nodes in the deleted list
       * </pre>
       */
      int getDeletedINodeRefCount();
      /**
       * <code>repeated uint32 deletedINodeRef = 8 [packed = true];</code>
       *
       * <pre>
       * id of reference nodes in the deleted list
       * </pre>
       */
      int getDeletedINodeRef(int index);
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.SnapshotDiffSection.DirectoryDiff}
     */
    public static final class DirectoryDiff extends
        com.google.protobuf.GeneratedMessage
        implements DirectoryDiffOrBuilder {
      // Use DirectoryDiff.newBuilder() to construct.
      private DirectoryDiff(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
        super(builder);
        this.unknownFields = builder.getUnknownFields();
      }
      private DirectoryDiff(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

      private static final DirectoryDiff defaultInstance;
      public static DirectoryDiff getDefaultInstance() {
        return defaultInstance;
      }

      public DirectoryDiff getDefaultInstanceForType() {
        return defaultInstance;
      }

      private final com.google.protobuf.UnknownFieldSet unknownFields;
      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
          getUnknownFields() {
        return this.unknownFields;
      }
      private DirectoryDiff(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        initFields();
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
              case 8: {
                bitField0_ |= 0x00000001;
                snapshotId_ = input.readUInt64();
                break;
              }
              case 16: {
                bitField0_ |= 0x00000002;
                childrenSize_ = input.readUInt32();
                break;
              }
              case 24: {
                bitField0_ |= 0x00000004;
                isSnapshotRoot_ = input.readBool();
                break;
              }
              case 34: {
                bitField0_ |= 0x00000008;
                name_ = input.readBytes();
                break;
              }
              case 42: {
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.Builder subBuilder = null;
                if (((bitField0_ & 0x00000010) == 0x00000010)) {
                  subBuilder = snapshotCopy_.toBuilder();
                }
                snapshotCopy_ = input.readMessage(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.PARSER, extensionRegistry);
                if (subBuilder != null) {
                  subBuilder.mergeFrom(snapshotCopy_);
                  snapshotCopy_ = subBuilder.buildPartial();
                }
                bitField0_ |= 0x00000010;
                break;
              }
              case 48: {
                bitField0_ |= 0x00000020;
                createdListSize_ = input.readUInt32();
                break;
              }
              case 56: {
                if (!((mutable_bitField0_ & 0x00000040) == 0x00000040)) {
                  deletedINode_ = new java.util.ArrayList<java.lang.Long>();
                  mutable_bitField0_ |= 0x00000040;
                }
                deletedINode_.add(input.readUInt64());
                break;
              }
              case 58: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000040) == 0x00000040) && input.getBytesUntilLimit() > 0) {
                  deletedINode_ = new java.util.ArrayList<java.lang.Long>();
                  mutable_bitField0_ |= 0x00000040;
                }
                while (input.getBytesUntilLimit() > 0) {
                  deletedINode_.add(input.readUInt64());
                }
                input.popLimit(limit);
                break;
              }
              case 64: {
                if (!((mutable_bitField0_ & 0x00000080) == 0x00000080)) {
                  deletedINodeRef_ = new java.util.ArrayList<java.lang.Integer>();
                  mutable_bitField0_ |= 0x00000080;
                }
                deletedINodeRef_.add(input.readUInt32());
                break;
              }
              case 66: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000080) == 0x00000080) && input.getBytesUntilLimit() > 0) {
                  deletedINodeRef_ = new java.util.ArrayList<java.lang.Integer>();
                  mutable_bitField0_ |= 0x00000080;
                }
                while (input.getBytesUntilLimit() > 0) {
                  deletedINodeRef_.add(input.readUInt32());
                }
                input.popLimit(limit);
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e.getMessage()).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000040) == 0x00000040)) {
            deletedINode_ = java.util.Collections.unmodifiableList(deletedINode_);
          }
          if (((mutable_bitField0_ & 0x00000080) == 0x00000080)) {
            deletedINodeRef_ = java.util.Collections.unmodifiableList(deletedINodeRef_);
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_DirectoryDiff_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_DirectoryDiff_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff.Builder.class);
      }

      public static com.google.protobuf.Parser<DirectoryDiff> PARSER =
          new com.google.protobuf.AbstractParser<DirectoryDiff>() {
        public DirectoryDiff parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new DirectoryDiff(input, extensionRegistry);
        }
      };

      @java.lang.Override
      public com.google.protobuf.Parser<DirectoryDiff> getParserForType() {
        return PARSER;
      }

      private int bitField0_;
      // optional uint64 snapshotId = 1;
      public static final int SNAPSHOTID_FIELD_NUMBER = 1;
      private long snapshotId_;
      /**
       * <code>optional uint64 snapshotId = 1;</code>
       */
      public boolean hasSnapshotId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint64 snapshotId = 1;</code>
       */
      public long getSnapshotId() {
        return snapshotId_;
      }

      // optional uint32 childrenSize = 2;
      public static final int CHILDRENSIZE_FIELD_NUMBER = 2;
      private int childrenSize_;
      /**
       * <code>optional uint32 childrenSize = 2;</code>
       */
      public boolean hasChildrenSize() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional uint32 childrenSize = 2;</code>
       */
      public int getChildrenSize() {
        return childrenSize_;
      }

      // optional bool isSnapshotRoot = 3;
      public static final int ISSNAPSHOTROOT_FIELD_NUMBER = 3;
      private boolean isSnapshotRoot_;
      /**
       * <code>optional bool isSnapshotRoot = 3;</code>
       */
      public boolean hasIsSnapshotRoot() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional bool isSnapshotRoot = 3;</code>
       */
      public boolean getIsSnapshotRoot() {
        return isSnapshotRoot_;
      }

      // optional bytes name = 4;
      public static final int NAME_FIELD_NUMBER = 4;
      private com.google.protobuf.ByteString name_;
      /**
       * <code>optional bytes name = 4;</code>
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional bytes name = 4;</code>
       */
      public com.google.protobuf.ByteString getName() {
        return name_;
      }

      // optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory snapshotCopy = 5;
      public static final int SNAPSHOTCOPY_FIELD_NUMBER = 5;
      private org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory snapshotCopy_;
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory snapshotCopy = 5;</code>
       */
      public boolean hasSnapshotCopy() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory snapshotCopy = 5;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory getSnapshotCopy() {
        return snapshotCopy_;
      }
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory snapshotCopy = 5;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectoryOrBuilder getSnapshotCopyOrBuilder() {
        return snapshotCopy_;
      }

      // optional uint32 createdListSize = 6;
      public static final int CREATEDLISTSIZE_FIELD_NUMBER = 6;
      private int createdListSize_;
      /**
       * <code>optional uint32 createdListSize = 6;</code>
       */
      public boolean hasCreatedListSize() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      /**
       * <code>optional uint32 createdListSize = 6;</code>
       */
      public int getCreatedListSize() {
        return createdListSize_;
      }

      // repeated uint64 deletedINode = 7 [packed = true];
      public static final int DELETEDINODE_FIELD_NUMBER = 7;
      private java.util.List<java.lang.Long> deletedINode_;
      /**
       * <code>repeated uint64 deletedINode = 7 [packed = true];</code>
       *
       * <pre>
       * id of deleted inodes
       * </pre>
       */
      public java.util.List<java.lang.Long>
          getDeletedINodeList() {
        return deletedINode_;
      }
      /**
       * <code>repeated uint64 deletedINode = 7 [packed = true];</code>
       *
       * <pre>
       * id of deleted inodes
       * </pre>
       */
      public int getDeletedINodeCount() {
        return deletedINode_.size();
      }
      /**
       * <code>repeated uint64 deletedINode = 7 [packed = true];</code>
       *
       * <pre>
       * id of deleted inodes
       * </pre>
       */
      public long getDeletedINode(int index) {
        return deletedINode_.get(index);
      }
      private int deletedINodeMemoizedSerializedSize = -1;

      // repeated uint32 deletedINodeRef = 8 [packed = true];
      public static final int DELETEDINODEREF_FIELD_NUMBER = 8;
      private java.util.List<java.lang.Integer> deletedINodeRef_;
      /**
       * <code>repeated uint32 deletedINodeRef = 8 [packed = true];</code>
       *
       * <pre>
       * id of reference nodes in the deleted list
       * </pre>
       */
      public java.util.List<java.lang.Integer>
          getDeletedINodeRefList() {
        return deletedINodeRef_;
      }
      /**
       * <code>repeated uint32 deletedINodeRef = 8 [packed = true];</code>
       *
       * <pre>
       * id of reference nodes in the deleted list
       * </pre>
       */
      public int getDeletedINodeRefCount() {
        return deletedINodeRef_.size();
      }
      /**
       * <code>repeated uint32 deletedINodeRef = 8 [packed = true];</code>
       *
       * <pre>
       * id of reference nodes in the deleted list
       * </pre>
       */
      public int getDeletedINodeRef(int index) {
        return deletedINodeRef_.get(index);
      }
      private int deletedINodeRefMemoizedSerializedSize = -1;

      private void initFields() {
        snapshotId_ = 0L;
        childrenSize_ = 0;
        isSnapshotRoot_ = false;
        name_ = com.google.protobuf.ByteString.EMPTY;
        snapshotCopy_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.getDefaultInstance();
        createdListSize_ = 0;
        deletedINode_ = java.util.Collections.emptyList();
        deletedINodeRef_ = java.util.Collections.emptyList();
      }
      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized != -1) return isInitialized == 1;

        memoizedIsInitialized = 1;
        return true;
      }

      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          output.writeUInt64(1, snapshotId_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          output.writeUInt32(2, childrenSize_);
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          output.writeBool(3, isSnapshotRoot_);
        }
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          output.writeBytes(4, name_);
        }
        if (((bitField0_ & 0x00000010) == 0x00000010)) {
          output.writeMessage(5, snapshotCopy_);
        }
        if (((bitField0_ & 0x00000020) == 0x00000020)) {
          output.writeUInt32(6, createdListSize_);
        }
        if (getDeletedINodeList().size() > 0) {
          output.writeRawVarint32(58);
          output.writeRawVarint32(deletedINodeMemoizedSerializedSize);
        }
        for (int i = 0; i < deletedINode_.size(); i++) {
          output.writeUInt64NoTag(deletedINode_.get(i));
        }
        if (getDeletedINodeRefList().size() > 0) {
          output.writeRawVarint32(66);
          output.writeRawVarint32(deletedINodeRefMemoizedSerializedSize);
        }
        for (int i = 0; i < deletedINodeRef_.size(); i++) {
          output.writeUInt32NoTag(deletedINodeRef_.get(i));
        }
        getUnknownFields().writeTo(output);
      }

      private int memoizedSerializedSize = -1;
      public int getSerializedSize() {
        int size = memoizedSerializedSize;
        if (size != -1) return size;

        size = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(1, snapshotId_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt32Size(2, childrenSize_);
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          size += com.google.protobuf.CodedOutputStream
            .computeBoolSize(3, isSnapshotRoot_);
        }
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          size += com.google.protobuf.CodedOutputStream
            .computeBytesSize(4, name_);
        }
        if (((bitField0_ & 0x00000010) == 0x00000010)) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(5, snapshotCopy_);
        }
        if (((bitField0_ & 0x00000020) == 0x00000020)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt32Size(6, createdListSize_);
        }
        {
          int dataSize = 0;
          for (int i = 0; i < deletedINode_.size(); i++) {
            dataSize += com.google.protobuf.CodedOutputStream
              .computeUInt64SizeNoTag(deletedINode_.get(i));
          }
          size += dataSize;
          if (!getDeletedINodeList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          deletedINodeMemoizedSerializedSize = dataSize;
        }
        {
          int dataSize = 0;
          for (int i = 0; i < deletedINodeRef_.size(); i++) {
            dataSize += com.google.protobuf.CodedOutputStream
              .computeUInt32SizeNoTag(deletedINodeRef_.get(i));
          }
          size += dataSize;
          if (!getDeletedINodeRefList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          deletedINodeRefMemoizedSerializedSize = dataSize;
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSerializedSize = size;
        return size;
      }

      private static final long serialVersionUID = 0L;
      @java.lang.Override
      protected java.lang.Object writeReplace()
          throws java.io.ObjectStreamException {
        return super.writeReplace();
      }

      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }

      public static Builder newBuilder() { return Builder.create(); }
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff prototype) {
        return newBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() { return newBuilder(this); }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code hadoop.hdfs.fsimage.SnapshotDiffSection.DirectoryDiff}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder>
         implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiffOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_DirectoryDiff_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_DirectoryDiff_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff.Builder.class);
        }

        // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
            getSnapshotCopyFieldBuilder();
          }
        }
        private static Builder create() {
          return new Builder();
        }

        public Builder clear() {
          super.clear();
          snapshotId_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000001);
          childrenSize_ = 0;
          bitField0_ = (bitField0_ & ~0x00000002);
          isSnapshotRoot_ = false;
          bitField0_ = (bitField0_ & ~0x00000004);
          name_ = com.google.protobuf.ByteString.EMPTY;
          bitField0_ = (bitField0_ & ~0x00000008);
          if (snapshotCopyBuilder_ == null) {
            snapshotCopy_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.getDefaultInstance();
          } else {
            snapshotCopyBuilder_.clear();
          }
          bitField0_ = (bitField0_ & ~0x00000010);
          createdListSize_ = 0;
          bitField0_ = (bitField0_ & ~0x00000020);
          deletedINode_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000040);
          deletedINodeRef_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000080);
          return this;
        }

        public Builder clone() {
          return create().mergeFrom(buildPartial());
        }

        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_DirectoryDiff_descriptor;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff getDefaultInstanceForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff.getDefaultInstance();
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff build() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff buildPartial() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
            to_bitField0_ |= 0x00000001;
          }
          result.snapshotId_ = snapshotId_;
          if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
            to_bitField0_ |= 0x00000002;
          }
          result.childrenSize_ = childrenSize_;
          if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
            to_bitField0_ |= 0x00000004;
          }
          result.isSnapshotRoot_ = isSnapshotRoot_;
          if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
            to_bitField0_ |= 0x00000008;
          }
          result.name_ = name_;
          if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
            to_bitField0_ |= 0x00000010;
          }
          if (snapshotCopyBuilder_ == null) {
            result.snapshotCopy_ = snapshotCopy_;
          } else {
            result.snapshotCopy_ = snapshotCopyBuilder_.build();
          }
          if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
            to_bitField0_ |= 0x00000020;
          }
          result.createdListSize_ = createdListSize_;
          if (((bitField0_ & 0x00000040) == 0x00000040)) {
            deletedINode_ = java.util.Collections.unmodifiableList(deletedINode_);
            bitField0_ = (bitField0_ & ~0x00000040);
          }
          result.deletedINode_ = deletedINode_;
          if (((bitField0_ & 0x00000080) == 0x00000080)) {
            deletedINodeRef_ = java.util.Collections.unmodifiableList(deletedINodeRef_);
            bitField0_ = (bitField0_ & ~0x00000080);
          }
          result.deletedINodeRef_ = deletedINodeRef_;
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff) {
            return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff other) {
          if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff.getDefaultInstance()) return this;
          if (other.hasSnapshotId()) {
            setSnapshotId(other.getSnapshotId());
          }
          if (other.hasChildrenSize()) {
            setChildrenSize(other.getChildrenSize());
          }
          if (other.hasIsSnapshotRoot()) {
            setIsSnapshotRoot(other.getIsSnapshotRoot());
          }
          if (other.hasName()) {
            setName(other.getName());
          }
          if (other.hasSnapshotCopy()) {
            mergeSnapshotCopy(other.getSnapshotCopy());
          }
          if (other.hasCreatedListSize()) {
            setCreatedListSize(other.getCreatedListSize());
          }
          if (!other.deletedINode_.isEmpty()) {
            if (deletedINode_.isEmpty()) {
              deletedINode_ = other.deletedINode_;
              bitField0_ = (bitField0_ & ~0x00000040);
            } else {
              ensureDeletedINodeIsMutable();
              deletedINode_.addAll(other.deletedINode_);
            }
            onChanged();
          }
          if (!other.deletedINodeRef_.isEmpty()) {
            if (deletedINodeRef_.isEmpty()) {
              deletedINodeRef_ = other.deletedINodeRef_;
              bitField0_ = (bitField0_ & ~0x00000080);
            } else {
              ensureDeletedINodeRefIsMutable();
              deletedINodeRef_.addAll(other.deletedINodeRef_);
            }
            onChanged();
          }
          this.mergeUnknownFields(other.getUnknownFields());
          return this;
        }

        public final boolean isInitialized() {
          return true;
        }

        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DirectoryDiff) e.getUnfinishedMessage();
            throw e;
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        // optional uint64 snapshotId = 1;
        private long snapshotId_ ;
        /**
         * <code>optional uint64 snapshotId = 1;</code>
         */
        public boolean hasSnapshotId() {
          return ((bitField0_ & 0x00000001) == 0x00000001);
        }
        /**
         * <code>optional uint64 snapshotId = 1;</code>
         */
        public long getSnapshotId() {
          return snapshotId_;
        }
        /**
         * <code>optional uint64 snapshotId = 1;</code>
         */
        public Builder setSnapshotId(long value) {
          bitField0_ |= 0x00000001;
          snapshotId_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 snapshotId = 1;</code>
         */
        public Builder clearSnapshotId() {
          bitField0_ = (bitField0_ & ~0x00000001);
          snapshotId_ = 0L;
          onChanged();
          return this;
        }

        // optional uint32 childrenSize = 2;
        private int childrenSize_ ;
        /**
         * <code>optional uint32 childrenSize = 2;</code>
         */
        public boolean hasChildrenSize() {
          return ((bitField0_ & 0x00000002) == 0x00000002);
        }
        /**
         * <code>optional uint32 childrenSize = 2;</code>
         */
        public int getChildrenSize() {
          return childrenSize_;
        }
        /**
         * <code>optional uint32 childrenSize = 2;</code>
         */
        public Builder setChildrenSize(int value) {
          bitField0_ |= 0x00000002;
          childrenSize_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint32 childrenSize = 2;</code>
         */
        public Builder clearChildrenSize() {
          bitField0_ = (bitField0_ & ~0x00000002);
          childrenSize_ = 0;
          onChanged();
          return this;
        }

        // optional bool isSnapshotRoot = 3;
        private boolean isSnapshotRoot_ ;
        /**
         * <code>optional bool isSnapshotRoot = 3;</code>
         */
        public boolean hasIsSnapshotRoot() {
          return ((bitField0_ & 0x00000004) == 0x00000004);
        }
        /**
         * <code>optional bool isSnapshotRoot = 3;</code>
         */
        public boolean getIsSnapshotRoot() {
          return isSnapshotRoot_;
        }
        /**
         * <code>optional bool isSnapshotRoot = 3;</code>
         */
        public Builder setIsSnapshotRoot(boolean value) {
          bitField0_ |= 0x00000004;
          isSnapshotRoot_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional bool isSnapshotRoot = 3;</code>
         */
        public Builder clearIsSnapshotRoot() {
          bitField0_ = (bitField0_ & ~0x00000004);
          isSnapshotRoot_ = false;
          onChanged();
          return this;
        }

        // optional bytes name = 4;
        private com.google.protobuf.ByteString name_ = com.google.protobuf.ByteString.EMPTY;
        /**
         * <code>optional bytes name = 4;</code>
         */
        public boolean hasName() {
          return ((bitField0_ & 0x00000008) == 0x00000008);
        }
        /**
         * <code>optional bytes name = 4;</code>
         */
        public com.google.protobuf.ByteString getName() {
          return name_;
        }
        /**
         * <code>optional bytes name = 4;</code>
         */
        public Builder setName(com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
          name_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional bytes name = 4;</code>
         */
        public Builder clearName() {
          bitField0_ = (bitField0_ & ~0x00000008);
          name_ = getDefaultInstance().getName();
          onChanged();
          return this;
        }

        // optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory snapshotCopy = 5;
        private org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory snapshotCopy_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.getDefaultInstance();
        private com.google.protobuf.SingleFieldBuilder<
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectoryOrBuilder> snapshotCopyBuilder_;
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory snapshotCopy = 5;</code>
         */
        public boolean hasSnapshotCopy() {
          return ((bitField0_ & 0x00000010) == 0x00000010);
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory snapshotCopy = 5;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory getSnapshotCopy() {
          if (snapshotCopyBuilder_ == null) {
            return snapshotCopy_;
          } else {
            return snapshotCopyBuilder_.getMessage();
          }
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory snapshotCopy = 5;</code>
         */
        public Builder setSnapshotCopy(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory value) {
          if (snapshotCopyBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            snapshotCopy_ = value;
            onChanged();
          } else {
            snapshotCopyBuilder_.setMessage(value);
          }
          bitField0_ |= 0x00000010;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory snapshotCopy = 5;</code>
         */
        public Builder setSnapshotCopy(
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.Builder builderForValue) {
          if (snapshotCopyBuilder_ == null) {
            snapshotCopy_ = builderForValue.build();
            onChanged();
          } else {
            snapshotCopyBuilder_.setMessage(builderForValue.build());
          }
          bitField0_ |= 0x00000010;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory snapshotCopy = 5;</code>
         */
        public Builder mergeSnapshotCopy(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory value) {
          if (snapshotCopyBuilder_ == null) {
            if (((bitField0_ & 0x00000010) == 0x00000010) &&
                snapshotCopy_ != org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.getDefaultInstance()) {
              snapshotCopy_ =
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.newBuilder(snapshotCopy_).mergeFrom(value).buildPartial();
            } else {
              snapshotCopy_ = value;
            }
            onChanged();
          } else {
            snapshotCopyBuilder_.mergeFrom(value);
          }
          bitField0_ |= 0x00000010;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory snapshotCopy = 5;</code>
         */
        public Builder clearSnapshotCopy() {
          if (snapshotCopyBuilder_ == null) {
            snapshotCopy_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.getDefaultInstance();
            onChanged();
          } else {
            snapshotCopyBuilder_.clear();
          }
          bitField0_ = (bitField0_ & ~0x00000010);
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory snapshotCopy = 5;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.Builder getSnapshotCopyBuilder() {
          bitField0_ |= 0x00000010;
          onChanged();
          return getSnapshotCopyFieldBuilder().getBuilder();
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory snapshotCopy = 5;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectoryOrBuilder getSnapshotCopyOrBuilder() {
          if (snapshotCopyBuilder_ != null) {
            return snapshotCopyBuilder_.getMessageOrBuilder();
          } else {
            return snapshotCopy_;
          }
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeDirectory snapshotCopy = 5;</code>
         */
        private com.google.protobuf.SingleFieldBuilder<
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectoryOrBuilder> 
            getSnapshotCopyFieldBuilder() {
          if (snapshotCopyBuilder_ == null) {
            snapshotCopyBuilder_ = new com.google.protobuf.SingleFieldBuilder<
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectory.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeDirectoryOrBuilder>(
                    snapshotCopy_,
                    getParentForChildren(),
                    isClean());
            snapshotCopy_ = null;
          }
          return snapshotCopyBuilder_;
        }

        // optional uint32 createdListSize = 6;
        private int createdListSize_ ;
        /**
         * <code>optional uint32 createdListSize = 6;</code>
         */
        public boolean hasCreatedListSize() {
          return ((bitField0_ & 0x00000020) == 0x00000020);
        }
        /**
         * <code>optional uint32 createdListSize = 6;</code>
         */
        public int getCreatedListSize() {
          return createdListSize_;
        }
        /**
         * <code>optional uint32 createdListSize = 6;</code>
         */
        public Builder setCreatedListSize(int value) {
          bitField0_ |= 0x00000020;
          createdListSize_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint32 createdListSize = 6;</code>
         */
        public Builder clearCreatedListSize() {
          bitField0_ = (bitField0_ & ~0x00000020);
          createdListSize_ = 0;
          onChanged();
          return this;
        }

        // repeated uint64 deletedINode = 7 [packed = true];
        private java.util.List<java.lang.Long> deletedINode_ = java.util.Collections.emptyList();
        private void ensureDeletedINodeIsMutable() {
          if (!((bitField0_ & 0x00000040) == 0x00000040)) {
            deletedINode_ = new java.util.ArrayList<java.lang.Long>(deletedINode_);
            bitField0_ |= 0x00000040;
           }
        }
        /**
         * <code>repeated uint64 deletedINode = 7 [packed = true];</code>
         *
         * <pre>
         * id of deleted inodes
         * </pre>
         */
        public java.util.List<java.lang.Long>
            getDeletedINodeList() {
          return java.util.Collections.unmodifiableList(deletedINode_);
        }
        /**
         * <code>repeated uint64 deletedINode = 7 [packed = true];</code>
         *
         * <pre>
         * id of deleted inodes
         * </pre>
         */
        public int getDeletedINodeCount() {
          return deletedINode_.size();
        }
        /**
         * <code>repeated uint64 deletedINode = 7 [packed = true];</code>
         *
         * <pre>
         * id of deleted inodes
         * </pre>
         */
        public long getDeletedINode(int index) {
          return deletedINode_.get(index);
        }
        /**
         * <code>repeated uint64 deletedINode = 7 [packed = true];</code>
         *
         * <pre>
         * id of deleted inodes
         * </pre>
         */
        public Builder setDeletedINode(
            int index, long value) {
          ensureDeletedINodeIsMutable();
          deletedINode_.set(index, value);
          onChanged();
          return this;
        }
        /**
         * <code>repeated uint64 deletedINode = 7 [packed = true];</code>
         *
         * <pre>
         * id of deleted inodes
         * </pre>
         */
        public Builder addDeletedINode(long value) {
          ensureDeletedINodeIsMutable();
          deletedINode_.add(value);
          onChanged();
          return this;
        }
        /**
         * <code>repeated uint64 deletedINode = 7 [packed = true];</code>
         *
         * <pre>
         * id of deleted inodes
         * </pre>
         */
        public Builder addAllDeletedINode(
            java.lang.Iterable<? extends java.lang.Long> values) {
          ensureDeletedINodeIsMutable();
          super.addAll(values, deletedINode_);
          onChanged();
          return this;
        }
        /**
         * <code>repeated uint64 deletedINode = 7 [packed = true];</code>
         *
         * <pre>
         * id of deleted inodes
         * </pre>
         */
        public Builder clearDeletedINode() {
          deletedINode_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000040);
          onChanged();
          return this;
        }

        // repeated uint32 deletedINodeRef = 8 [packed = true];
        private java.util.List<java.lang.Integer> deletedINodeRef_ = java.util.Collections.emptyList();
        private void ensureDeletedINodeRefIsMutable() {
          if (!((bitField0_ & 0x00000080) == 0x00000080)) {
            deletedINodeRef_ = new java.util.ArrayList<java.lang.Integer>(deletedINodeRef_);
            bitField0_ |= 0x00000080;
           }
        }
        /**
         * <code>repeated uint32 deletedINodeRef = 8 [packed = true];</code>
         *
         * <pre>
         * id of reference nodes in the deleted list
         * </pre>
         */
        public java.util.List<java.lang.Integer>
            getDeletedINodeRefList() {
          return java.util.Collections.unmodifiableList(deletedINodeRef_);
        }
        /**
         * <code>repeated uint32 deletedINodeRef = 8 [packed = true];</code>
         *
         * <pre>
         * id of reference nodes in the deleted list
         * </pre>
         */
        public int getDeletedINodeRefCount() {
          return deletedINodeRef_.size();
        }
        /**
         * <code>repeated uint32 deletedINodeRef = 8 [packed = true];</code>
         *
         * <pre>
         * id of reference nodes in the deleted list
         * </pre>
         */
        public int getDeletedINodeRef(int index) {
          return deletedINodeRef_.get(index);
        }
        /**
         * <code>repeated uint32 deletedINodeRef = 8 [packed = true];</code>
         *
         * <pre>
         * id of reference nodes in the deleted list
         * </pre>
         */
        public Builder setDeletedINodeRef(
            int index, int value) {
          ensureDeletedINodeRefIsMutable();
          deletedINodeRef_.set(index, value);
          onChanged();
          return this;
        }
        /**
         * <code>repeated uint32 deletedINodeRef = 8 [packed = true];</code>
         *
         * <pre>
         * id of reference nodes in the deleted list
         * </pre>
         */
        public Builder addDeletedINodeRef(int value) {
          ensureDeletedINodeRefIsMutable();
          deletedINodeRef_.add(value);
          onChanged();
          return this;
        }
        /**
         * <code>repeated uint32 deletedINodeRef = 8 [packed = true];</code>
         *
         * <pre>
         * id of reference nodes in the deleted list
         * </pre>
         */
        public Builder addAllDeletedINodeRef(
            java.lang.Iterable<? extends java.lang.Integer> values) {
          ensureDeletedINodeRefIsMutable();
          super.addAll(values, deletedINodeRef_);
          onChanged();
          return this;
        }
        /**
         * <code>repeated uint32 deletedINodeRef = 8 [packed = true];</code>
         *
         * <pre>
         * id of reference nodes in the deleted list
         * </pre>
         */
        public Builder clearDeletedINodeRef() {
          deletedINodeRef_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000080);
          onChanged();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.SnapshotDiffSection.DirectoryDiff)
      }

      static {
        defaultInstance = new DirectoryDiff(true);
        defaultInstance.initFields();
      }

      // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.SnapshotDiffSection.DirectoryDiff)
    }

    public interface FileDiffOrBuilder
        extends com.google.protobuf.MessageOrBuilder {

      // optional uint64 snapshotId = 1;
      /**
       * <code>optional uint64 snapshotId = 1;</code>
       */
      boolean hasSnapshotId();
      /**
       * <code>optional uint64 snapshotId = 1;</code>
       */
      long getSnapshotId();

      // optional uint64 fileSize = 2;
      /**
       * <code>optional uint64 fileSize = 2;</code>
       */
      boolean hasFileSize();
      /**
       * <code>optional uint64 fileSize = 2;</code>
       */
      long getFileSize();

      // optional bytes name = 3;
      /**
       * <code>optional bytes name = 3;</code>
       */
      boolean hasName();
      /**
       * <code>optional bytes name = 3;</code>
       */
      com.google.protobuf.ByteString getName();

      // optional .hadoop.hdfs.fsimage.INodeSection.INodeFile snapshotCopy = 4;
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile snapshotCopy = 4;</code>
       */
      boolean hasSnapshotCopy();
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile snapshotCopy = 4;</code>
       */
      org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile getSnapshotCopy();
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile snapshotCopy = 4;</code>
       */
      org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFileOrBuilder getSnapshotCopyOrBuilder();
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.SnapshotDiffSection.FileDiff}
     */
    public static final class FileDiff extends
        com.google.protobuf.GeneratedMessage
        implements FileDiffOrBuilder {
      // Use FileDiff.newBuilder() to construct.
      private FileDiff(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
        super(builder);
        this.unknownFields = builder.getUnknownFields();
      }
      private FileDiff(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

      private static final FileDiff defaultInstance;
      public static FileDiff getDefaultInstance() {
        return defaultInstance;
      }

      public FileDiff getDefaultInstanceForType() {
        return defaultInstance;
      }

      private final com.google.protobuf.UnknownFieldSet unknownFields;
      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
          getUnknownFields() {
        return this.unknownFields;
      }
      private FileDiff(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        initFields();
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
              case 8: {
                bitField0_ |= 0x00000001;
                snapshotId_ = input.readUInt64();
                break;
              }
              case 16: {
                bitField0_ |= 0x00000002;
                fileSize_ = input.readUInt64();
                break;
              }
              case 26: {
                bitField0_ |= 0x00000004;
                name_ = input.readBytes();
                break;
              }
              case 34: {
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.Builder subBuilder = null;
                if (((bitField0_ & 0x00000008) == 0x00000008)) {
                  subBuilder = snapshotCopy_.toBuilder();
                }
                snapshotCopy_ = input.readMessage(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.PARSER, extensionRegistry);
                if (subBuilder != null) {
                  subBuilder.mergeFrom(snapshotCopy_);
                  snapshotCopy_ = subBuilder.buildPartial();
                }
                bitField0_ |= 0x00000008;
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e.getMessage()).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_FileDiff_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_FileDiff_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff.Builder.class);
      }

      public static com.google.protobuf.Parser<FileDiff> PARSER =
          new com.google.protobuf.AbstractParser<FileDiff>() {
        public FileDiff parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new FileDiff(input, extensionRegistry);
        }
      };

      @java.lang.Override
      public com.google.protobuf.Parser<FileDiff> getParserForType() {
        return PARSER;
      }

      private int bitField0_;
      // optional uint64 snapshotId = 1;
      public static final int SNAPSHOTID_FIELD_NUMBER = 1;
      private long snapshotId_;
      /**
       * <code>optional uint64 snapshotId = 1;</code>
       */
      public boolean hasSnapshotId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint64 snapshotId = 1;</code>
       */
      public long getSnapshotId() {
        return snapshotId_;
      }

      // optional uint64 fileSize = 2;
      public static final int FILESIZE_FIELD_NUMBER = 2;
      private long fileSize_;
      /**
       * <code>optional uint64 fileSize = 2;</code>
       */
      public boolean hasFileSize() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional uint64 fileSize = 2;</code>
       */
      public long getFileSize() {
        return fileSize_;
      }

      // optional bytes name = 3;
      public static final int NAME_FIELD_NUMBER = 3;
      private com.google.protobuf.ByteString name_;
      /**
       * <code>optional bytes name = 3;</code>
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional bytes name = 3;</code>
       */
      public com.google.protobuf.ByteString getName() {
        return name_;
      }

      // optional .hadoop.hdfs.fsimage.INodeSection.INodeFile snapshotCopy = 4;
      public static final int SNAPSHOTCOPY_FIELD_NUMBER = 4;
      private org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile snapshotCopy_;
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile snapshotCopy = 4;</code>
       */
      public boolean hasSnapshotCopy() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile snapshotCopy = 4;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile getSnapshotCopy() {
        return snapshotCopy_;
      }
      /**
       * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile snapshotCopy = 4;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFileOrBuilder getSnapshotCopyOrBuilder() {
        return snapshotCopy_;
      }

      private void initFields() {
        snapshotId_ = 0L;
        fileSize_ = 0L;
        name_ = com.google.protobuf.ByteString.EMPTY;
        snapshotCopy_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.getDefaultInstance();
      }
      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized != -1) return isInitialized == 1;

        if (hasSnapshotCopy()) {
          if (!getSnapshotCopy().isInitialized()) {
            memoizedIsInitialized = 0;
            return false;
          }
        }
        memoizedIsInitialized = 1;
        return true;
      }

      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          output.writeUInt64(1, snapshotId_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          output.writeUInt64(2, fileSize_);
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          output.writeBytes(3, name_);
        }
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          output.writeMessage(4, snapshotCopy_);
        }
        getUnknownFields().writeTo(output);
      }

      private int memoizedSerializedSize = -1;
      public int getSerializedSize() {
        int size = memoizedSerializedSize;
        if (size != -1) return size;

        size = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(1, snapshotId_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(2, fileSize_);
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          size += com.google.protobuf.CodedOutputStream
            .computeBytesSize(3, name_);
        }
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(4, snapshotCopy_);
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSerializedSize = size;
        return size;
      }

      private static final long serialVersionUID = 0L;
      @java.lang.Override
      protected java.lang.Object writeReplace()
          throws java.io.ObjectStreamException {
        return super.writeReplace();
      }

      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }

      public static Builder newBuilder() { return Builder.create(); }
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff prototype) {
        return newBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() { return newBuilder(this); }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code hadoop.hdfs.fsimage.SnapshotDiffSection.FileDiff}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder>
         implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiffOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_FileDiff_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_FileDiff_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff.Builder.class);
        }

        // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
            getSnapshotCopyFieldBuilder();
          }
        }
        private static Builder create() {
          return new Builder();
        }

        public Builder clear() {
          super.clear();
          snapshotId_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000001);
          fileSize_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000002);
          name_ = com.google.protobuf.ByteString.EMPTY;
          bitField0_ = (bitField0_ & ~0x00000004);
          if (snapshotCopyBuilder_ == null) {
            snapshotCopy_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.getDefaultInstance();
          } else {
            snapshotCopyBuilder_.clear();
          }
          bitField0_ = (bitField0_ & ~0x00000008);
          return this;
        }

        public Builder clone() {
          return create().mergeFrom(buildPartial());
        }

        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_FileDiff_descriptor;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff getDefaultInstanceForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff.getDefaultInstance();
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff build() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff buildPartial() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
            to_bitField0_ |= 0x00000001;
          }
          result.snapshotId_ = snapshotId_;
          if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
            to_bitField0_ |= 0x00000002;
          }
          result.fileSize_ = fileSize_;
          if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
            to_bitField0_ |= 0x00000004;
          }
          result.name_ = name_;
          if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
            to_bitField0_ |= 0x00000008;
          }
          if (snapshotCopyBuilder_ == null) {
            result.snapshotCopy_ = snapshotCopy_;
          } else {
            result.snapshotCopy_ = snapshotCopyBuilder_.build();
          }
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff) {
            return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff other) {
          if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff.getDefaultInstance()) return this;
          if (other.hasSnapshotId()) {
            setSnapshotId(other.getSnapshotId());
          }
          if (other.hasFileSize()) {
            setFileSize(other.getFileSize());
          }
          if (other.hasName()) {
            setName(other.getName());
          }
          if (other.hasSnapshotCopy()) {
            mergeSnapshotCopy(other.getSnapshotCopy());
          }
          this.mergeUnknownFields(other.getUnknownFields());
          return this;
        }

        public final boolean isInitialized() {
          if (hasSnapshotCopy()) {
            if (!getSnapshotCopy().isInitialized()) {
              
              return false;
            }
          }
          return true;
        }

        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.FileDiff) e.getUnfinishedMessage();
            throw e;
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        // optional uint64 snapshotId = 1;
        private long snapshotId_ ;
        /**
         * <code>optional uint64 snapshotId = 1;</code>
         */
        public boolean hasSnapshotId() {
          return ((bitField0_ & 0x00000001) == 0x00000001);
        }
        /**
         * <code>optional uint64 snapshotId = 1;</code>
         */
        public long getSnapshotId() {
          return snapshotId_;
        }
        /**
         * <code>optional uint64 snapshotId = 1;</code>
         */
        public Builder setSnapshotId(long value) {
          bitField0_ |= 0x00000001;
          snapshotId_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 snapshotId = 1;</code>
         */
        public Builder clearSnapshotId() {
          bitField0_ = (bitField0_ & ~0x00000001);
          snapshotId_ = 0L;
          onChanged();
          return this;
        }

        // optional uint64 fileSize = 2;
        private long fileSize_ ;
        /**
         * <code>optional uint64 fileSize = 2;</code>
         */
        public boolean hasFileSize() {
          return ((bitField0_ & 0x00000002) == 0x00000002);
        }
        /**
         * <code>optional uint64 fileSize = 2;</code>
         */
        public long getFileSize() {
          return fileSize_;
        }
        /**
         * <code>optional uint64 fileSize = 2;</code>
         */
        public Builder setFileSize(long value) {
          bitField0_ |= 0x00000002;
          fileSize_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 fileSize = 2;</code>
         */
        public Builder clearFileSize() {
          bitField0_ = (bitField0_ & ~0x00000002);
          fileSize_ = 0L;
          onChanged();
          return this;
        }

        // optional bytes name = 3;
        private com.google.protobuf.ByteString name_ = com.google.protobuf.ByteString.EMPTY;
        /**
         * <code>optional bytes name = 3;</code>
         */
        public boolean hasName() {
          return ((bitField0_ & 0x00000004) == 0x00000004);
        }
        /**
         * <code>optional bytes name = 3;</code>
         */
        public com.google.protobuf.ByteString getName() {
          return name_;
        }
        /**
         * <code>optional bytes name = 3;</code>
         */
        public Builder setName(com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
          name_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional bytes name = 3;</code>
         */
        public Builder clearName() {
          bitField0_ = (bitField0_ & ~0x00000004);
          name_ = getDefaultInstance().getName();
          onChanged();
          return this;
        }

        // optional .hadoop.hdfs.fsimage.INodeSection.INodeFile snapshotCopy = 4;
        private org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile snapshotCopy_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.getDefaultInstance();
        private com.google.protobuf.SingleFieldBuilder<
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFileOrBuilder> snapshotCopyBuilder_;
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile snapshotCopy = 4;</code>
         */
        public boolean hasSnapshotCopy() {
          return ((bitField0_ & 0x00000008) == 0x00000008);
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile snapshotCopy = 4;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile getSnapshotCopy() {
          if (snapshotCopyBuilder_ == null) {
            return snapshotCopy_;
          } else {
            return snapshotCopyBuilder_.getMessage();
          }
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile snapshotCopy = 4;</code>
         */
        public Builder setSnapshotCopy(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile value) {
          if (snapshotCopyBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            snapshotCopy_ = value;
            onChanged();
          } else {
            snapshotCopyBuilder_.setMessage(value);
          }
          bitField0_ |= 0x00000008;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile snapshotCopy = 4;</code>
         */
        public Builder setSnapshotCopy(
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.Builder builderForValue) {
          if (snapshotCopyBuilder_ == null) {
            snapshotCopy_ = builderForValue.build();
            onChanged();
          } else {
            snapshotCopyBuilder_.setMessage(builderForValue.build());
          }
          bitField0_ |= 0x00000008;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile snapshotCopy = 4;</code>
         */
        public Builder mergeSnapshotCopy(org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile value) {
          if (snapshotCopyBuilder_ == null) {
            if (((bitField0_ & 0x00000008) == 0x00000008) &&
                snapshotCopy_ != org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.getDefaultInstance()) {
              snapshotCopy_ =
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.newBuilder(snapshotCopy_).mergeFrom(value).buildPartial();
            } else {
              snapshotCopy_ = value;
            }
            onChanged();
          } else {
            snapshotCopyBuilder_.mergeFrom(value);
          }
          bitField0_ |= 0x00000008;
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile snapshotCopy = 4;</code>
         */
        public Builder clearSnapshotCopy() {
          if (snapshotCopyBuilder_ == null) {
            snapshotCopy_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.getDefaultInstance();
            onChanged();
          } else {
            snapshotCopyBuilder_.clear();
          }
          bitField0_ = (bitField0_ & ~0x00000008);
          return this;
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile snapshotCopy = 4;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.Builder getSnapshotCopyBuilder() {
          bitField0_ |= 0x00000008;
          onChanged();
          return getSnapshotCopyFieldBuilder().getBuilder();
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile snapshotCopy = 4;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFileOrBuilder getSnapshotCopyOrBuilder() {
          if (snapshotCopyBuilder_ != null) {
            return snapshotCopyBuilder_.getMessageOrBuilder();
          } else {
            return snapshotCopy_;
          }
        }
        /**
         * <code>optional .hadoop.hdfs.fsimage.INodeSection.INodeFile snapshotCopy = 4;</code>
         */
        private com.google.protobuf.SingleFieldBuilder<
            org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFileOrBuilder> 
            getSnapshotCopyFieldBuilder() {
          if (snapshotCopyBuilder_ == null) {
            snapshotCopyBuilder_ = new com.google.protobuf.SingleFieldBuilder<
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFile.Builder, org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INodeFileOrBuilder>(
                    snapshotCopy_,
                    getParentForChildren(),
                    isClean());
            snapshotCopy_ = null;
          }
          return snapshotCopyBuilder_;
        }

        // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.SnapshotDiffSection.FileDiff)
      }

      static {
        defaultInstance = new FileDiff(true);
        defaultInstance.initFields();
      }

      // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.SnapshotDiffSection.FileDiff)
    }

    public interface DiffEntryOrBuilder
        extends com.google.protobuf.MessageOrBuilder {

      // required .hadoop.hdfs.fsimage.SnapshotDiffSection.DiffEntry.Type type = 1;
      /**
       * <code>required .hadoop.hdfs.fsimage.SnapshotDiffSection.DiffEntry.Type type = 1;</code>
       */
      boolean hasType();
      /**
       * <code>required .hadoop.hdfs.fsimage.SnapshotDiffSection.DiffEntry.Type type = 1;</code>
       */
      org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry.Type getType();

      // optional uint64 inodeId = 2;
      /**
       * <code>optional uint64 inodeId = 2;</code>
       */
      boolean hasInodeId();
      /**
       * <code>optional uint64 inodeId = 2;</code>
       */
      long getInodeId();

      // optional uint32 numOfDiff = 3;
      /**
       * <code>optional uint32 numOfDiff = 3;</code>
       */
      boolean hasNumOfDiff();
      /**
       * <code>optional uint32 numOfDiff = 3;</code>
       */
      int getNumOfDiff();
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.SnapshotDiffSection.DiffEntry}
     */
    public static final class DiffEntry extends
        com.google.protobuf.GeneratedMessage
        implements DiffEntryOrBuilder {
      // Use DiffEntry.newBuilder() to construct.
      private DiffEntry(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
        super(builder);
        this.unknownFields = builder.getUnknownFields();
      }
      private DiffEntry(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

      private static final DiffEntry defaultInstance;
      public static DiffEntry getDefaultInstance() {
        return defaultInstance;
      }

      public DiffEntry getDefaultInstanceForType() {
        return defaultInstance;
      }

      private final com.google.protobuf.UnknownFieldSet unknownFields;
      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
          getUnknownFields() {
        return this.unknownFields;
      }
      private DiffEntry(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        initFields();
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
              case 8: {
                int rawValue = input.readEnum();
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry.Type value = org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry.Type.valueOf(rawValue);
                if (value == null) {
                  unknownFields.mergeVarintField(1, rawValue);
                } else {
                  bitField0_ |= 0x00000001;
                  type_ = value;
                }
                break;
              }
              case 16: {
                bitField0_ |= 0x00000002;
                inodeId_ = input.readUInt64();
                break;
              }
              case 24: {
                bitField0_ |= 0x00000004;
                numOfDiff_ = input.readUInt32();
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e.getMessage()).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_DiffEntry_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_DiffEntry_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry.Builder.class);
      }

      public static com.google.protobuf.Parser<DiffEntry> PARSER =
          new com.google.protobuf.AbstractParser<DiffEntry>() {
        public DiffEntry parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new DiffEntry(input, extensionRegistry);
        }
      };

      @java.lang.Override
      public com.google.protobuf.Parser<DiffEntry> getParserForType() {
        return PARSER;
      }

      /**
       * Protobuf enum {@code hadoop.hdfs.fsimage.SnapshotDiffSection.DiffEntry.Type}
       */
      public enum Type
          implements com.google.protobuf.ProtocolMessageEnum {
        /**
         * <code>FILEDIFF = 1;</code>
         */
        FILEDIFF(0, 1),
        /**
         * <code>DIRECTORYDIFF = 2;</code>
         */
        DIRECTORYDIFF(1, 2),
        ;

        /**
         * <code>FILEDIFF = 1;</code>
         */
        public static final int FILEDIFF_VALUE = 1;
        /**
         * <code>DIRECTORYDIFF = 2;</code>
         */
        public static final int DIRECTORYDIFF_VALUE = 2;


        public final int getNumber() { return value; }

        public static Type valueOf(int value) {
          switch (value) {
            case 1: return FILEDIFF;
            case 2: return DIRECTORYDIFF;
            default: return null;
          }
        }

        public static com.google.protobuf.Internal.EnumLiteMap<Type>
            internalGetValueMap() {
          return internalValueMap;
        }
        private static com.google.protobuf.Internal.EnumLiteMap<Type>
            internalValueMap =
              new com.google.protobuf.Internal.EnumLiteMap<Type>() {
                public Type findValueByNumber(int number) {
                  return Type.valueOf(number);
                }
              };

        public final com.google.protobuf.Descriptors.EnumValueDescriptor
            getValueDescriptor() {
          return getDescriptor().getValues().get(index);
        }
        public final com.google.protobuf.Descriptors.EnumDescriptor
            getDescriptorForType() {
          return getDescriptor();
        }
        public static final com.google.protobuf.Descriptors.EnumDescriptor
            getDescriptor() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry.getDescriptor().getEnumTypes().get(0);
        }

        private static final Type[] VALUES = values();

        public static Type valueOf(
            com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
          if (desc.getType() != getDescriptor()) {
            throw new java.lang.IllegalArgumentException(
              "EnumValueDescriptor is not for this type.");
          }
          return VALUES[desc.getIndex()];
        }

        private final int index;
        private final int value;

        private Type(int index, int value) {
          this.index = index;
          this.value = value;
        }

        // @@protoc_insertion_point(enum_scope:hadoop.hdfs.fsimage.SnapshotDiffSection.DiffEntry.Type)
      }

      private int bitField0_;
      // required .hadoop.hdfs.fsimage.SnapshotDiffSection.DiffEntry.Type type = 1;
      public static final int TYPE_FIELD_NUMBER = 1;
      private org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry.Type type_;
      /**
       * <code>required .hadoop.hdfs.fsimage.SnapshotDiffSection.DiffEntry.Type type = 1;</code>
       */
      public boolean hasType() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .hadoop.hdfs.fsimage.SnapshotDiffSection.DiffEntry.Type type = 1;</code>
       */
      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry.Type getType() {
        return type_;
      }

      // optional uint64 inodeId = 2;
      public static final int INODEID_FIELD_NUMBER = 2;
      private long inodeId_;
      /**
       * <code>optional uint64 inodeId = 2;</code>
       */
      public boolean hasInodeId() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional uint64 inodeId = 2;</code>
       */
      public long getInodeId() {
        return inodeId_;
      }

      // optional uint32 numOfDiff = 3;
      public static final int NUMOFDIFF_FIELD_NUMBER = 3;
      private int numOfDiff_;
      /**
       * <code>optional uint32 numOfDiff = 3;</code>
       */
      public boolean hasNumOfDiff() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional uint32 numOfDiff = 3;</code>
       */
      public int getNumOfDiff() {
        return numOfDiff_;
      }

      private void initFields() {
        type_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry.Type.FILEDIFF;
        inodeId_ = 0L;
        numOfDiff_ = 0;
      }
      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized != -1) return isInitialized == 1;

        if (!hasType()) {
          memoizedIsInitialized = 0;
          return false;
        }
        memoizedIsInitialized = 1;
        return true;
      }

      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          output.writeEnum(1, type_.getNumber());
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          output.writeUInt64(2, inodeId_);
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          output.writeUInt32(3, numOfDiff_);
        }
        getUnknownFields().writeTo(output);
      }

      private int memoizedSerializedSize = -1;
      public int getSerializedSize() {
        int size = memoizedSerializedSize;
        if (size != -1) return size;

        size = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          size += com.google.protobuf.CodedOutputStream
            .computeEnumSize(1, type_.getNumber());
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(2, inodeId_);
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt32Size(3, numOfDiff_);
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSerializedSize = size;
        return size;
      }

      private static final long serialVersionUID = 0L;
      @java.lang.Override
      protected java.lang.Object writeReplace()
          throws java.io.ObjectStreamException {
        return super.writeReplace();
      }

      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }

      public static Builder newBuilder() { return Builder.create(); }
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry prototype) {
        return newBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() { return newBuilder(this); }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code hadoop.hdfs.fsimage.SnapshotDiffSection.DiffEntry}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder>
         implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntryOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_DiffEntry_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_DiffEntry_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry.Builder.class);
        }

        // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          }
        }
        private static Builder create() {
          return new Builder();
        }

        public Builder clear() {
          super.clear();
          type_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry.Type.FILEDIFF;
          bitField0_ = (bitField0_ & ~0x00000001);
          inodeId_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000002);
          numOfDiff_ = 0;
          bitField0_ = (bitField0_ & ~0x00000004);
          return this;
        }

        public Builder clone() {
          return create().mergeFrom(buildPartial());
        }

        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_DiffEntry_descriptor;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry getDefaultInstanceForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry.getDefaultInstance();
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry build() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry buildPartial() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
            to_bitField0_ |= 0x00000001;
          }
          result.type_ = type_;
          if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
            to_bitField0_ |= 0x00000002;
          }
          result.inodeId_ = inodeId_;
          if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
            to_bitField0_ |= 0x00000004;
          }
          result.numOfDiff_ = numOfDiff_;
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry) {
            return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry other) {
          if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry.getDefaultInstance()) return this;
          if (other.hasType()) {
            setType(other.getType());
          }
          if (other.hasInodeId()) {
            setInodeId(other.getInodeId());
          }
          if (other.hasNumOfDiff()) {
            setNumOfDiff(other.getNumOfDiff());
          }
          this.mergeUnknownFields(other.getUnknownFields());
          return this;
        }

        public final boolean isInitialized() {
          if (!hasType()) {
            
            return false;
          }
          return true;
        }

        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry) e.getUnfinishedMessage();
            throw e;
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        // required .hadoop.hdfs.fsimage.SnapshotDiffSection.DiffEntry.Type type = 1;
        private org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry.Type type_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry.Type.FILEDIFF;
        /**
         * <code>required .hadoop.hdfs.fsimage.SnapshotDiffSection.DiffEntry.Type type = 1;</code>
         */
        public boolean hasType() {
          return ((bitField0_ & 0x00000001) == 0x00000001);
        }
        /**
         * <code>required .hadoop.hdfs.fsimage.SnapshotDiffSection.DiffEntry.Type type = 1;</code>
         */
        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry.Type getType() {
          return type_;
        }
        /**
         * <code>required .hadoop.hdfs.fsimage.SnapshotDiffSection.DiffEntry.Type type = 1;</code>
         */
        public Builder setType(org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry.Type value) {
          if (value == null) {
            throw new NullPointerException();
          }
          bitField0_ |= 0x00000001;
          type_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>required .hadoop.hdfs.fsimage.SnapshotDiffSection.DiffEntry.Type type = 1;</code>
         */
        public Builder clearType() {
          bitField0_ = (bitField0_ & ~0x00000001);
          type_ = org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.DiffEntry.Type.FILEDIFF;
          onChanged();
          return this;
        }

        // optional uint64 inodeId = 2;
        private long inodeId_ ;
        /**
         * <code>optional uint64 inodeId = 2;</code>
         */
        public boolean hasInodeId() {
          return ((bitField0_ & 0x00000002) == 0x00000002);
        }
        /**
         * <code>optional uint64 inodeId = 2;</code>
         */
        public long getInodeId() {
          return inodeId_;
        }
        /**
         * <code>optional uint64 inodeId = 2;</code>
         */
        public Builder setInodeId(long value) {
          bitField0_ |= 0x00000002;
          inodeId_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 inodeId = 2;</code>
         */
        public Builder clearInodeId() {
          bitField0_ = (bitField0_ & ~0x00000002);
          inodeId_ = 0L;
          onChanged();
          return this;
        }

        // optional uint32 numOfDiff = 3;
        private int numOfDiff_ ;
        /**
         * <code>optional uint32 numOfDiff = 3;</code>
         */
        public boolean hasNumOfDiff() {
          return ((bitField0_ & 0x00000004) == 0x00000004);
        }
        /**
         * <code>optional uint32 numOfDiff = 3;</code>
         */
        public int getNumOfDiff() {
          return numOfDiff_;
        }
        /**
         * <code>optional uint32 numOfDiff = 3;</code>
         */
        public Builder setNumOfDiff(int value) {
          bitField0_ |= 0x00000004;
          numOfDiff_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint32 numOfDiff = 3;</code>
         */
        public Builder clearNumOfDiff() {
          bitField0_ = (bitField0_ & ~0x00000004);
          numOfDiff_ = 0;
          onChanged();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.SnapshotDiffSection.DiffEntry)
      }

      static {
        defaultInstance = new DiffEntry(true);
        defaultInstance.initFields();
      }

      // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.SnapshotDiffSection.DiffEntry)
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.SnapshotDiffSection}
     *
     * <pre>
     **
     * This section records information about snapshot diffs
     * NAME: SNAPSHOT_DIFF
     * </pre>
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSectionOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_descriptor;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection build() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection buildPartial() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection) {
          return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection other) {
        if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.SnapshotDiffSection) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.SnapshotDiffSection)
    }

    static {
      defaultInstance = new SnapshotDiffSection(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.SnapshotDiffSection)
  }

  public interface StringTableSectionOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional uint32 numEntry = 1;
    /**
     * <code>optional uint32 numEntry = 1;</code>
     *
     * <pre>
     * repeated Entry
     * </pre>
     */
    boolean hasNumEntry();
    /**
     * <code>optional uint32 numEntry = 1;</code>
     *
     * <pre>
     * repeated Entry
     * </pre>
     */
    int getNumEntry();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.fsimage.StringTableSection}
   *
   * <pre>
   **
   * This section maps string to id
   * NAME: STRING_TABLE
   * </pre>
   */
  public static final class StringTableSection extends
      com.google.protobuf.GeneratedMessage
      implements StringTableSectionOrBuilder {
    // Use StringTableSection.newBuilder() to construct.
    private StringTableSection(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private StringTableSection(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final StringTableSection defaultInstance;
    public static StringTableSection getDefaultInstance() {
      return defaultInstance;
    }

    public StringTableSection getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private StringTableSection(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              numEntry_ = input.readUInt32();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_StringTableSection_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_StringTableSection_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Builder.class);
    }

    public static com.google.protobuf.Parser<StringTableSection> PARSER =
        new com.google.protobuf.AbstractParser<StringTableSection>() {
      public StringTableSection parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StringTableSection(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<StringTableSection> getParserForType() {
      return PARSER;
    }

    public interface EntryOrBuilder
        extends com.google.protobuf.MessageOrBuilder {

      // optional uint32 id = 1;
      /**
       * <code>optional uint32 id = 1;</code>
       */
      boolean hasId();
      /**
       * <code>optional uint32 id = 1;</code>
       */
      int getId();

      // optional string str = 2;
      /**
       * <code>optional string str = 2;</code>
       */
      boolean hasStr();
      /**
       * <code>optional string str = 2;</code>
       */
      java.lang.String getStr();
      /**
       * <code>optional string str = 2;</code>
       */
      com.google.protobuf.ByteString
          getStrBytes();
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.StringTableSection.Entry}
     */
    public static final class Entry extends
        com.google.protobuf.GeneratedMessage
        implements EntryOrBuilder {
      // Use Entry.newBuilder() to construct.
      private Entry(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
        super(builder);
        this.unknownFields = builder.getUnknownFields();
      }
      private Entry(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

      private static final Entry defaultInstance;
      public static Entry getDefaultInstance() {
        return defaultInstance;
      }

      public Entry getDefaultInstanceForType() {
        return defaultInstance;
      }

      private final com.google.protobuf.UnknownFieldSet unknownFields;
      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
          getUnknownFields() {
        return this.unknownFields;
      }
      private Entry(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        initFields();
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
              case 8: {
                bitField0_ |= 0x00000001;
                id_ = input.readUInt32();
                break;
              }
              case 18: {
                bitField0_ |= 0x00000002;
                str_ = input.readBytes();
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e.getMessage()).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_StringTableSection_Entry_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_StringTableSection_Entry_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry.Builder.class);
      }

      public static com.google.protobuf.Parser<Entry> PARSER =
          new com.google.protobuf.AbstractParser<Entry>() {
        public Entry parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Entry(input, extensionRegistry);
        }
      };

      @java.lang.Override
      public com.google.protobuf.Parser<Entry> getParserForType() {
        return PARSER;
      }

      private int bitField0_;
      // optional uint32 id = 1;
      public static final int ID_FIELD_NUMBER = 1;
      private int id_;
      /**
       * <code>optional uint32 id = 1;</code>
       */
      public boolean hasId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint32 id = 1;</code>
       */
      public int getId() {
        return id_;
      }

      // optional string str = 2;
      public static final int STR_FIELD_NUMBER = 2;
      private java.lang.Object str_;
      /**
       * <code>optional string str = 2;</code>
       */
      public boolean hasStr() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional string str = 2;</code>
       */
      public java.lang.String getStr() {
        java.lang.Object ref = str_;
        if (ref instanceof java.lang.String) {
          return (java.lang.String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            str_ = s;
          }
          return s;
        }
      }
      /**
       * <code>optional string str = 2;</code>
       */
      public com.google.protobuf.ByteString
          getStrBytes() {
        java.lang.Object ref = str_;
        if (ref instanceof java.lang.String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          str_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      private void initFields() {
        id_ = 0;
        str_ = "";
      }
      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized != -1) return isInitialized == 1;

        memoizedIsInitialized = 1;
        return true;
      }

      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          output.writeUInt32(1, id_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          output.writeBytes(2, getStrBytes());
        }
        getUnknownFields().writeTo(output);
      }

      private int memoizedSerializedSize = -1;
      public int getSerializedSize() {
        int size = memoizedSerializedSize;
        if (size != -1) return size;

        size = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt32Size(1, id_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          size += com.google.protobuf.CodedOutputStream
            .computeBytesSize(2, getStrBytes());
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSerializedSize = size;
        return size;
      }

      private static final long serialVersionUID = 0L;
      @java.lang.Override
      protected java.lang.Object writeReplace()
          throws java.io.ObjectStreamException {
        return super.writeReplace();
      }

      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }

      public static Builder newBuilder() { return Builder.create(); }
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry prototype) {
        return newBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() { return newBuilder(this); }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code hadoop.hdfs.fsimage.StringTableSection.Entry}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder>
         implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.EntryOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_StringTableSection_Entry_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_StringTableSection_Entry_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry.Builder.class);
        }

        // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          }
        }
        private static Builder create() {
          return new Builder();
        }

        public Builder clear() {
          super.clear();
          id_ = 0;
          bitField0_ = (bitField0_ & ~0x00000001);
          str_ = "";
          bitField0_ = (bitField0_ & ~0x00000002);
          return this;
        }

        public Builder clone() {
          return create().mergeFrom(buildPartial());
        }

        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_StringTableSection_Entry_descriptor;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry getDefaultInstanceForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry.getDefaultInstance();
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry build() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry buildPartial() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
            to_bitField0_ |= 0x00000001;
          }
          result.id_ = id_;
          if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
            to_bitField0_ |= 0x00000002;
          }
          result.str_ = str_;
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry) {
            return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry other) {
          if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry.getDefaultInstance()) return this;
          if (other.hasId()) {
            setId(other.getId());
          }
          if (other.hasStr()) {
            bitField0_ |= 0x00000002;
            str_ = other.str_;
            onChanged();
          }
          this.mergeUnknownFields(other.getUnknownFields());
          return this;
        }

        public final boolean isInitialized() {
          return true;
        }

        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Entry) e.getUnfinishedMessage();
            throw e;
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        // optional uint32 id = 1;
        private int id_ ;
        /**
         * <code>optional uint32 id = 1;</code>
         */
        public boolean hasId() {
          return ((bitField0_ & 0x00000001) == 0x00000001);
        }
        /**
         * <code>optional uint32 id = 1;</code>
         */
        public int getId() {
          return id_;
        }
        /**
         * <code>optional uint32 id = 1;</code>
         */
        public Builder setId(int value) {
          bitField0_ |= 0x00000001;
          id_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint32 id = 1;</code>
         */
        public Builder clearId() {
          bitField0_ = (bitField0_ & ~0x00000001);
          id_ = 0;
          onChanged();
          return this;
        }

        // optional string str = 2;
        private java.lang.Object str_ = "";
        /**
         * <code>optional string str = 2;</code>
         */
        public boolean hasStr() {
          return ((bitField0_ & 0x00000002) == 0x00000002);
        }
        /**
         * <code>optional string str = 2;</code>
         */
        public java.lang.String getStr() {
          java.lang.Object ref = str_;
          if (!(ref instanceof java.lang.String)) {
            java.lang.String s = ((com.google.protobuf.ByteString) ref)
                .toStringUtf8();
            str_ = s;
            return s;
          } else {
            return (java.lang.String) ref;
          }
        }
        /**
         * <code>optional string str = 2;</code>
         */
        public com.google.protobuf.ByteString
            getStrBytes() {
          java.lang.Object ref = str_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            str_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <code>optional string str = 2;</code>
         */
        public Builder setStr(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
          str_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional string str = 2;</code>
         */
        public Builder clearStr() {
          bitField0_ = (bitField0_ & ~0x00000002);
          str_ = getDefaultInstance().getStr();
          onChanged();
          return this;
        }
        /**
         * <code>optional string str = 2;</code>
         */
        public Builder setStrBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
          str_ = value;
          onChanged();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.StringTableSection.Entry)
      }

      static {
        defaultInstance = new Entry(true);
        defaultInstance.initFields();
      }

      // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.StringTableSection.Entry)
    }

    private int bitField0_;
    // optional uint32 numEntry = 1;
    public static final int NUMENTRY_FIELD_NUMBER = 1;
    private int numEntry_;
    /**
     * <code>optional uint32 numEntry = 1;</code>
     *
     * <pre>
     * repeated Entry
     * </pre>
     */
    public boolean hasNumEntry() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional uint32 numEntry = 1;</code>
     *
     * <pre>
     * repeated Entry
     * </pre>
     */
    public int getNumEntry() {
      return numEntry_;
    }

    private void initFields() {
      numEntry_ = 0;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeUInt32(1, numEntry_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(1, numEntry_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.StringTableSection}
     *
     * <pre>
     **
     * This section maps string to id
     * NAME: STRING_TABLE
     * </pre>
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSectionOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_StringTableSection_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_StringTableSection_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        numEntry_ = 0;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_StringTableSection_descriptor;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection build() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection buildPartial() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.numEntry_ = numEntry_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection) {
          return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection other) {
        if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection.getDefaultInstance()) return this;
        if (other.hasNumEntry()) {
          setNumEntry(other.getNumEntry());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.StringTableSection) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional uint32 numEntry = 1;
      private int numEntry_ ;
      /**
       * <code>optional uint32 numEntry = 1;</code>
       *
       * <pre>
       * repeated Entry
       * </pre>
       */
      public boolean hasNumEntry() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint32 numEntry = 1;</code>
       *
       * <pre>
       * repeated Entry
       * </pre>
       */
      public int getNumEntry() {
        return numEntry_;
      }
      /**
       * <code>optional uint32 numEntry = 1;</code>
       *
       * <pre>
       * repeated Entry
       * </pre>
       */
      public Builder setNumEntry(int value) {
        bitField0_ |= 0x00000001;
        numEntry_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 numEntry = 1;</code>
       *
       * <pre>
       * repeated Entry
       * </pre>
       */
      public Builder clearNumEntry() {
        bitField0_ = (bitField0_ & ~0x00000001);
        numEntry_ = 0;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.StringTableSection)
    }

    static {
      defaultInstance = new StringTableSection(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.StringTableSection)
  }

  public interface SecretManagerSectionOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional uint32 currentId = 1;
    /**
     * <code>optional uint32 currentId = 1;</code>
     */
    boolean hasCurrentId();
    /**
     * <code>optional uint32 currentId = 1;</code>
     */
    int getCurrentId();

    // optional uint32 tokenSequenceNumber = 2;
    /**
     * <code>optional uint32 tokenSequenceNumber = 2;</code>
     */
    boolean hasTokenSequenceNumber();
    /**
     * <code>optional uint32 tokenSequenceNumber = 2;</code>
     */
    int getTokenSequenceNumber();

    // optional uint32 numKeys = 3;
    /**
     * <code>optional uint32 numKeys = 3;</code>
     */
    boolean hasNumKeys();
    /**
     * <code>optional uint32 numKeys = 3;</code>
     */
    int getNumKeys();

    // optional uint32 numTokens = 4;
    /**
     * <code>optional uint32 numTokens = 4;</code>
     *
     * <pre>
     * repeated DelegationKey keys
     * repeated PersistToken tokens
     * </pre>
     */
    boolean hasNumTokens();
    /**
     * <code>optional uint32 numTokens = 4;</code>
     *
     * <pre>
     * repeated DelegationKey keys
     * repeated PersistToken tokens
     * </pre>
     */
    int getNumTokens();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.fsimage.SecretManagerSection}
   */
  public static final class SecretManagerSection extends
      com.google.protobuf.GeneratedMessage
      implements SecretManagerSectionOrBuilder {
    // Use SecretManagerSection.newBuilder() to construct.
    private SecretManagerSection(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private SecretManagerSection(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final SecretManagerSection defaultInstance;
    public static SecretManagerSection getDefaultInstance() {
      return defaultInstance;
    }

    public SecretManagerSection getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private SecretManagerSection(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              currentId_ = input.readUInt32();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              tokenSequenceNumber_ = input.readUInt32();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              numKeys_ = input.readUInt32();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              numTokens_ = input.readUInt32();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SecretManagerSection_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SecretManagerSection_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.Builder.class);
    }

    public static com.google.protobuf.Parser<SecretManagerSection> PARSER =
        new com.google.protobuf.AbstractParser<SecretManagerSection>() {
      public SecretManagerSection parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SecretManagerSection(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<SecretManagerSection> getParserForType() {
      return PARSER;
    }

    public interface DelegationKeyOrBuilder
        extends com.google.protobuf.MessageOrBuilder {

      // optional uint32 id = 1;
      /**
       * <code>optional uint32 id = 1;</code>
       */
      boolean hasId();
      /**
       * <code>optional uint32 id = 1;</code>
       */
      int getId();

      // optional uint64 expiryDate = 2;
      /**
       * <code>optional uint64 expiryDate = 2;</code>
       */
      boolean hasExpiryDate();
      /**
       * <code>optional uint64 expiryDate = 2;</code>
       */
      long getExpiryDate();

      // optional bytes key = 3;
      /**
       * <code>optional bytes key = 3;</code>
       */
      boolean hasKey();
      /**
       * <code>optional bytes key = 3;</code>
       */
      com.google.protobuf.ByteString getKey();
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.SecretManagerSection.DelegationKey}
     */
    public static final class DelegationKey extends
        com.google.protobuf.GeneratedMessage
        implements DelegationKeyOrBuilder {
      // Use DelegationKey.newBuilder() to construct.
      private DelegationKey(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
        super(builder);
        this.unknownFields = builder.getUnknownFields();
      }
      private DelegationKey(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

      private static final DelegationKey defaultInstance;
      public static DelegationKey getDefaultInstance() {
        return defaultInstance;
      }

      public DelegationKey getDefaultInstanceForType() {
        return defaultInstance;
      }

      private final com.google.protobuf.UnknownFieldSet unknownFields;
      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
          getUnknownFields() {
        return this.unknownFields;
      }
      private DelegationKey(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        initFields();
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
              case 8: {
                bitField0_ |= 0x00000001;
                id_ = input.readUInt32();
                break;
              }
              case 16: {
                bitField0_ |= 0x00000002;
                expiryDate_ = input.readUInt64();
                break;
              }
              case 26: {
                bitField0_ |= 0x00000004;
                key_ = input.readBytes();
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e.getMessage()).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SecretManagerSection_DelegationKey_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SecretManagerSection_DelegationKey_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey.Builder.class);
      }

      public static com.google.protobuf.Parser<DelegationKey> PARSER =
          new com.google.protobuf.AbstractParser<DelegationKey>() {
        public DelegationKey parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new DelegationKey(input, extensionRegistry);
        }
      };

      @java.lang.Override
      public com.google.protobuf.Parser<DelegationKey> getParserForType() {
        return PARSER;
      }

      private int bitField0_;
      // optional uint32 id = 1;
      public static final int ID_FIELD_NUMBER = 1;
      private int id_;
      /**
       * <code>optional uint32 id = 1;</code>
       */
      public boolean hasId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint32 id = 1;</code>
       */
      public int getId() {
        return id_;
      }

      // optional uint64 expiryDate = 2;
      public static final int EXPIRYDATE_FIELD_NUMBER = 2;
      private long expiryDate_;
      /**
       * <code>optional uint64 expiryDate = 2;</code>
       */
      public boolean hasExpiryDate() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional uint64 expiryDate = 2;</code>
       */
      public long getExpiryDate() {
        return expiryDate_;
      }

      // optional bytes key = 3;
      public static final int KEY_FIELD_NUMBER = 3;
      private com.google.protobuf.ByteString key_;
      /**
       * <code>optional bytes key = 3;</code>
       */
      public boolean hasKey() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional bytes key = 3;</code>
       */
      public com.google.protobuf.ByteString getKey() {
        return key_;
      }

      private void initFields() {
        id_ = 0;
        expiryDate_ = 0L;
        key_ = com.google.protobuf.ByteString.EMPTY;
      }
      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized != -1) return isInitialized == 1;

        memoizedIsInitialized = 1;
        return true;
      }

      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          output.writeUInt32(1, id_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          output.writeUInt64(2, expiryDate_);
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          output.writeBytes(3, key_);
        }
        getUnknownFields().writeTo(output);
      }

      private int memoizedSerializedSize = -1;
      public int getSerializedSize() {
        int size = memoizedSerializedSize;
        if (size != -1) return size;

        size = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt32Size(1, id_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(2, expiryDate_);
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          size += com.google.protobuf.CodedOutputStream
            .computeBytesSize(3, key_);
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSerializedSize = size;
        return size;
      }

      private static final long serialVersionUID = 0L;
      @java.lang.Override
      protected java.lang.Object writeReplace()
          throws java.io.ObjectStreamException {
        return super.writeReplace();
      }

      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }

      public static Builder newBuilder() { return Builder.create(); }
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey prototype) {
        return newBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() { return newBuilder(this); }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code hadoop.hdfs.fsimage.SecretManagerSection.DelegationKey}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder>
         implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKeyOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SecretManagerSection_DelegationKey_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SecretManagerSection_DelegationKey_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey.Builder.class);
        }

        // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          }
        }
        private static Builder create() {
          return new Builder();
        }

        public Builder clear() {
          super.clear();
          id_ = 0;
          bitField0_ = (bitField0_ & ~0x00000001);
          expiryDate_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000002);
          key_ = com.google.protobuf.ByteString.EMPTY;
          bitField0_ = (bitField0_ & ~0x00000004);
          return this;
        }

        public Builder clone() {
          return create().mergeFrom(buildPartial());
        }

        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SecretManagerSection_DelegationKey_descriptor;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey getDefaultInstanceForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey.getDefaultInstance();
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey build() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey buildPartial() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
            to_bitField0_ |= 0x00000001;
          }
          result.id_ = id_;
          if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
            to_bitField0_ |= 0x00000002;
          }
          result.expiryDate_ = expiryDate_;
          if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
            to_bitField0_ |= 0x00000004;
          }
          result.key_ = key_;
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey) {
            return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey other) {
          if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey.getDefaultInstance()) return this;
          if (other.hasId()) {
            setId(other.getId());
          }
          if (other.hasExpiryDate()) {
            setExpiryDate(other.getExpiryDate());
          }
          if (other.hasKey()) {
            setKey(other.getKey());
          }
          this.mergeUnknownFields(other.getUnknownFields());
          return this;
        }

        public final boolean isInitialized() {
          return true;
        }

        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.DelegationKey) e.getUnfinishedMessage();
            throw e;
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        // optional uint32 id = 1;
        private int id_ ;
        /**
         * <code>optional uint32 id = 1;</code>
         */
        public boolean hasId() {
          return ((bitField0_ & 0x00000001) == 0x00000001);
        }
        /**
         * <code>optional uint32 id = 1;</code>
         */
        public int getId() {
          return id_;
        }
        /**
         * <code>optional uint32 id = 1;</code>
         */
        public Builder setId(int value) {
          bitField0_ |= 0x00000001;
          id_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint32 id = 1;</code>
         */
        public Builder clearId() {
          bitField0_ = (bitField0_ & ~0x00000001);
          id_ = 0;
          onChanged();
          return this;
        }

        // optional uint64 expiryDate = 2;
        private long expiryDate_ ;
        /**
         * <code>optional uint64 expiryDate = 2;</code>
         */
        public boolean hasExpiryDate() {
          return ((bitField0_ & 0x00000002) == 0x00000002);
        }
        /**
         * <code>optional uint64 expiryDate = 2;</code>
         */
        public long getExpiryDate() {
          return expiryDate_;
        }
        /**
         * <code>optional uint64 expiryDate = 2;</code>
         */
        public Builder setExpiryDate(long value) {
          bitField0_ |= 0x00000002;
          expiryDate_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 expiryDate = 2;</code>
         */
        public Builder clearExpiryDate() {
          bitField0_ = (bitField0_ & ~0x00000002);
          expiryDate_ = 0L;
          onChanged();
          return this;
        }

        // optional bytes key = 3;
        private com.google.protobuf.ByteString key_ = com.google.protobuf.ByteString.EMPTY;
        /**
         * <code>optional bytes key = 3;</code>
         */
        public boolean hasKey() {
          return ((bitField0_ & 0x00000004) == 0x00000004);
        }
        /**
         * <code>optional bytes key = 3;</code>
         */
        public com.google.protobuf.ByteString getKey() {
          return key_;
        }
        /**
         * <code>optional bytes key = 3;</code>
         */
        public Builder setKey(com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
          key_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional bytes key = 3;</code>
         */
        public Builder clearKey() {
          bitField0_ = (bitField0_ & ~0x00000004);
          key_ = getDefaultInstance().getKey();
          onChanged();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.SecretManagerSection.DelegationKey)
      }

      static {
        defaultInstance = new DelegationKey(true);
        defaultInstance.initFields();
      }

      // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.SecretManagerSection.DelegationKey)
    }

    public interface PersistTokenOrBuilder
        extends com.google.protobuf.MessageOrBuilder {

      // optional uint32 version = 1;
      /**
       * <code>optional uint32 version = 1;</code>
       */
      boolean hasVersion();
      /**
       * <code>optional uint32 version = 1;</code>
       */
      int getVersion();

      // optional string owner = 2;
      /**
       * <code>optional string owner = 2;</code>
       */
      boolean hasOwner();
      /**
       * <code>optional string owner = 2;</code>
       */
      java.lang.String getOwner();
      /**
       * <code>optional string owner = 2;</code>
       */
      com.google.protobuf.ByteString
          getOwnerBytes();

      // optional string renewer = 3;
      /**
       * <code>optional string renewer = 3;</code>
       */
      boolean hasRenewer();
      /**
       * <code>optional string renewer = 3;</code>
       */
      java.lang.String getRenewer();
      /**
       * <code>optional string renewer = 3;</code>
       */
      com.google.protobuf.ByteString
          getRenewerBytes();

      // optional string realUser = 4;
      /**
       * <code>optional string realUser = 4;</code>
       */
      boolean hasRealUser();
      /**
       * <code>optional string realUser = 4;</code>
       */
      java.lang.String getRealUser();
      /**
       * <code>optional string realUser = 4;</code>
       */
      com.google.protobuf.ByteString
          getRealUserBytes();

      // optional uint64 issueDate = 5;
      /**
       * <code>optional uint64 issueDate = 5;</code>
       */
      boolean hasIssueDate();
      /**
       * <code>optional uint64 issueDate = 5;</code>
       */
      long getIssueDate();

      // optional uint64 maxDate = 6;
      /**
       * <code>optional uint64 maxDate = 6;</code>
       */
      boolean hasMaxDate();
      /**
       * <code>optional uint64 maxDate = 6;</code>
       */
      long getMaxDate();

      // optional uint32 sequenceNumber = 7;
      /**
       * <code>optional uint32 sequenceNumber = 7;</code>
       */
      boolean hasSequenceNumber();
      /**
       * <code>optional uint32 sequenceNumber = 7;</code>
       */
      int getSequenceNumber();

      // optional uint32 masterKeyId = 8;
      /**
       * <code>optional uint32 masterKeyId = 8;</code>
       */
      boolean hasMasterKeyId();
      /**
       * <code>optional uint32 masterKeyId = 8;</code>
       */
      int getMasterKeyId();

      // optional uint64 expiryDate = 9;
      /**
       * <code>optional uint64 expiryDate = 9;</code>
       */
      boolean hasExpiryDate();
      /**
       * <code>optional uint64 expiryDate = 9;</code>
       */
      long getExpiryDate();
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.SecretManagerSection.PersistToken}
     */
    public static final class PersistToken extends
        com.google.protobuf.GeneratedMessage
        implements PersistTokenOrBuilder {
      // Use PersistToken.newBuilder() to construct.
      private PersistToken(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
        super(builder);
        this.unknownFields = builder.getUnknownFields();
      }
      private PersistToken(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

      private static final PersistToken defaultInstance;
      public static PersistToken getDefaultInstance() {
        return defaultInstance;
      }

      public PersistToken getDefaultInstanceForType() {
        return defaultInstance;
      }

      private final com.google.protobuf.UnknownFieldSet unknownFields;
      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
          getUnknownFields() {
        return this.unknownFields;
      }
      private PersistToken(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        initFields();
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
              case 8: {
                bitField0_ |= 0x00000001;
                version_ = input.readUInt32();
                break;
              }
              case 18: {
                bitField0_ |= 0x00000002;
                owner_ = input.readBytes();
                break;
              }
              case 26: {
                bitField0_ |= 0x00000004;
                renewer_ = input.readBytes();
                break;
              }
              case 34: {
                bitField0_ |= 0x00000008;
                realUser_ = input.readBytes();
                break;
              }
              case 40: {
                bitField0_ |= 0x00000010;
                issueDate_ = input.readUInt64();
                break;
              }
              case 48: {
                bitField0_ |= 0x00000020;
                maxDate_ = input.readUInt64();
                break;
              }
              case 56: {
                bitField0_ |= 0x00000040;
                sequenceNumber_ = input.readUInt32();
                break;
              }
              case 64: {
                bitField0_ |= 0x00000080;
                masterKeyId_ = input.readUInt32();
                break;
              }
              case 72: {
                bitField0_ |= 0x00000100;
                expiryDate_ = input.readUInt64();
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e.getMessage()).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SecretManagerSection_PersistToken_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SecretManagerSection_PersistToken_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken.Builder.class);
      }

      public static com.google.protobuf.Parser<PersistToken> PARSER =
          new com.google.protobuf.AbstractParser<PersistToken>() {
        public PersistToken parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new PersistToken(input, extensionRegistry);
        }
      };

      @java.lang.Override
      public com.google.protobuf.Parser<PersistToken> getParserForType() {
        return PARSER;
      }

      private int bitField0_;
      // optional uint32 version = 1;
      public static final int VERSION_FIELD_NUMBER = 1;
      private int version_;
      /**
       * <code>optional uint32 version = 1;</code>
       */
      public boolean hasVersion() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint32 version = 1;</code>
       */
      public int getVersion() {
        return version_;
      }

      // optional string owner = 2;
      public static final int OWNER_FIELD_NUMBER = 2;
      private java.lang.Object owner_;
      /**
       * <code>optional string owner = 2;</code>
       */
      public boolean hasOwner() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional string owner = 2;</code>
       */
      public java.lang.String getOwner() {
        java.lang.Object ref = owner_;
        if (ref instanceof java.lang.String) {
          return (java.lang.String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            owner_ = s;
          }
          return s;
        }
      }
      /**
       * <code>optional string owner = 2;</code>
       */
      public com.google.protobuf.ByteString
          getOwnerBytes() {
        java.lang.Object ref = owner_;
        if (ref instanceof java.lang.String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          owner_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      // optional string renewer = 3;
      public static final int RENEWER_FIELD_NUMBER = 3;
      private java.lang.Object renewer_;
      /**
       * <code>optional string renewer = 3;</code>
       */
      public boolean hasRenewer() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional string renewer = 3;</code>
       */
      public java.lang.String getRenewer() {
        java.lang.Object ref = renewer_;
        if (ref instanceof java.lang.String) {
          return (java.lang.String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            renewer_ = s;
          }
          return s;
        }
      }
      /**
       * <code>optional string renewer = 3;</code>
       */
      public com.google.protobuf.ByteString
          getRenewerBytes() {
        java.lang.Object ref = renewer_;
        if (ref instanceof java.lang.String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          renewer_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      // optional string realUser = 4;
      public static final int REALUSER_FIELD_NUMBER = 4;
      private java.lang.Object realUser_;
      /**
       * <code>optional string realUser = 4;</code>
       */
      public boolean hasRealUser() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional string realUser = 4;</code>
       */
      public java.lang.String getRealUser() {
        java.lang.Object ref = realUser_;
        if (ref instanceof java.lang.String) {
          return (java.lang.String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            realUser_ = s;
          }
          return s;
        }
      }
      /**
       * <code>optional string realUser = 4;</code>
       */
      public com.google.protobuf.ByteString
          getRealUserBytes() {
        java.lang.Object ref = realUser_;
        if (ref instanceof java.lang.String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          realUser_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      // optional uint64 issueDate = 5;
      public static final int ISSUEDATE_FIELD_NUMBER = 5;
      private long issueDate_;
      /**
       * <code>optional uint64 issueDate = 5;</code>
       */
      public boolean hasIssueDate() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional uint64 issueDate = 5;</code>
       */
      public long getIssueDate() {
        return issueDate_;
      }

      // optional uint64 maxDate = 6;
      public static final int MAXDATE_FIELD_NUMBER = 6;
      private long maxDate_;
      /**
       * <code>optional uint64 maxDate = 6;</code>
       */
      public boolean hasMaxDate() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      /**
       * <code>optional uint64 maxDate = 6;</code>
       */
      public long getMaxDate() {
        return maxDate_;
      }

      // optional uint32 sequenceNumber = 7;
      public static final int SEQUENCENUMBER_FIELD_NUMBER = 7;
      private int sequenceNumber_;
      /**
       * <code>optional uint32 sequenceNumber = 7;</code>
       */
      public boolean hasSequenceNumber() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional uint32 sequenceNumber = 7;</code>
       */
      public int getSequenceNumber() {
        return sequenceNumber_;
      }

      // optional uint32 masterKeyId = 8;
      public static final int MASTERKEYID_FIELD_NUMBER = 8;
      private int masterKeyId_;
      /**
       * <code>optional uint32 masterKeyId = 8;</code>
       */
      public boolean hasMasterKeyId() {
        return ((bitField0_ & 0x00000080) == 0x00000080);
      }
      /**
       * <code>optional uint32 masterKeyId = 8;</code>
       */
      public int getMasterKeyId() {
        return masterKeyId_;
      }

      // optional uint64 expiryDate = 9;
      public static final int EXPIRYDATE_FIELD_NUMBER = 9;
      private long expiryDate_;
      /**
       * <code>optional uint64 expiryDate = 9;</code>
       */
      public boolean hasExpiryDate() {
        return ((bitField0_ & 0x00000100) == 0x00000100);
      }
      /**
       * <code>optional uint64 expiryDate = 9;</code>
       */
      public long getExpiryDate() {
        return expiryDate_;
      }

      private void initFields() {
        version_ = 0;
        owner_ = "";
        renewer_ = "";
        realUser_ = "";
        issueDate_ = 0L;
        maxDate_ = 0L;
        sequenceNumber_ = 0;
        masterKeyId_ = 0;
        expiryDate_ = 0L;
      }
      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized != -1) return isInitialized == 1;

        memoizedIsInitialized = 1;
        return true;
      }

      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          output.writeUInt32(1, version_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          output.writeBytes(2, getOwnerBytes());
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          output.writeBytes(3, getRenewerBytes());
        }
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          output.writeBytes(4, getRealUserBytes());
        }
        if (((bitField0_ & 0x00000010) == 0x00000010)) {
          output.writeUInt64(5, issueDate_);
        }
        if (((bitField0_ & 0x00000020) == 0x00000020)) {
          output.writeUInt64(6, maxDate_);
        }
        if (((bitField0_ & 0x00000040) == 0x00000040)) {
          output.writeUInt32(7, sequenceNumber_);
        }
        if (((bitField0_ & 0x00000080) == 0x00000080)) {
          output.writeUInt32(8, masterKeyId_);
        }
        if (((bitField0_ & 0x00000100) == 0x00000100)) {
          output.writeUInt64(9, expiryDate_);
        }
        getUnknownFields().writeTo(output);
      }

      private int memoizedSerializedSize = -1;
      public int getSerializedSize() {
        int size = memoizedSerializedSize;
        if (size != -1) return size;

        size = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt32Size(1, version_);
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          size += com.google.protobuf.CodedOutputStream
            .computeBytesSize(2, getOwnerBytes());
        }
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          size += com.google.protobuf.CodedOutputStream
            .computeBytesSize(3, getRenewerBytes());
        }
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          size += com.google.protobuf.CodedOutputStream
            .computeBytesSize(4, getRealUserBytes());
        }
        if (((bitField0_ & 0x00000010) == 0x00000010)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(5, issueDate_);
        }
        if (((bitField0_ & 0x00000020) == 0x00000020)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(6, maxDate_);
        }
        if (((bitField0_ & 0x00000040) == 0x00000040)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt32Size(7, sequenceNumber_);
        }
        if (((bitField0_ & 0x00000080) == 0x00000080)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt32Size(8, masterKeyId_);
        }
        if (((bitField0_ & 0x00000100) == 0x00000100)) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(9, expiryDate_);
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSerializedSize = size;
        return size;
      }

      private static final long serialVersionUID = 0L;
      @java.lang.Override
      protected java.lang.Object writeReplace()
          throws java.io.ObjectStreamException {
        return super.writeReplace();
      }

      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseDelimitedFrom(input, extensionRegistry);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return PARSER.parseFrom(input);
      }
      public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return PARSER.parseFrom(input, extensionRegistry);
      }

      public static Builder newBuilder() { return Builder.create(); }
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken prototype) {
        return newBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() { return newBuilder(this); }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code hadoop.hdfs.fsimage.SecretManagerSection.PersistToken}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder>
         implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistTokenOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SecretManagerSection_PersistToken_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SecretManagerSection_PersistToken_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken.Builder.class);
        }

        // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          }
        }
        private static Builder create() {
          return new Builder();
        }

        public Builder clear() {
          super.clear();
          version_ = 0;
          bitField0_ = (bitField0_ & ~0x00000001);
          owner_ = "";
          bitField0_ = (bitField0_ & ~0x00000002);
          renewer_ = "";
          bitField0_ = (bitField0_ & ~0x00000004);
          realUser_ = "";
          bitField0_ = (bitField0_ & ~0x00000008);
          issueDate_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000010);
          maxDate_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000020);
          sequenceNumber_ = 0;
          bitField0_ = (bitField0_ & ~0x00000040);
          masterKeyId_ = 0;
          bitField0_ = (bitField0_ & ~0x00000080);
          expiryDate_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000100);
          return this;
        }

        public Builder clone() {
          return create().mergeFrom(buildPartial());
        }

        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SecretManagerSection_PersistToken_descriptor;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken getDefaultInstanceForType() {
          return org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken.getDefaultInstance();
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken build() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken buildPartial() {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
            to_bitField0_ |= 0x00000001;
          }
          result.version_ = version_;
          if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
            to_bitField0_ |= 0x00000002;
          }
          result.owner_ = owner_;
          if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
            to_bitField0_ |= 0x00000004;
          }
          result.renewer_ = renewer_;
          if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
            to_bitField0_ |= 0x00000008;
          }
          result.realUser_ = realUser_;
          if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
            to_bitField0_ |= 0x00000010;
          }
          result.issueDate_ = issueDate_;
          if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
            to_bitField0_ |= 0x00000020;
          }
          result.maxDate_ = maxDate_;
          if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
            to_bitField0_ |= 0x00000040;
          }
          result.sequenceNumber_ = sequenceNumber_;
          if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
            to_bitField0_ |= 0x00000080;
          }
          result.masterKeyId_ = masterKeyId_;
          if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
            to_bitField0_ |= 0x00000100;
          }
          result.expiryDate_ = expiryDate_;
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken) {
            return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken other) {
          if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken.getDefaultInstance()) return this;
          if (other.hasVersion()) {
            setVersion(other.getVersion());
          }
          if (other.hasOwner()) {
            bitField0_ |= 0x00000002;
            owner_ = other.owner_;
            onChanged();
          }
          if (other.hasRenewer()) {
            bitField0_ |= 0x00000004;
            renewer_ = other.renewer_;
            onChanged();
          }
          if (other.hasRealUser()) {
            bitField0_ |= 0x00000008;
            realUser_ = other.realUser_;
            onChanged();
          }
          if (other.hasIssueDate()) {
            setIssueDate(other.getIssueDate());
          }
          if (other.hasMaxDate()) {
            setMaxDate(other.getMaxDate());
          }
          if (other.hasSequenceNumber()) {
            setSequenceNumber(other.getSequenceNumber());
          }
          if (other.hasMasterKeyId()) {
            setMasterKeyId(other.getMasterKeyId());
          }
          if (other.hasExpiryDate()) {
            setExpiryDate(other.getExpiryDate());
          }
          this.mergeUnknownFields(other.getUnknownFields());
          return this;
        }

        public final boolean isInitialized() {
          return true;
        }

        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.PersistToken) e.getUnfinishedMessage();
            throw e;
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        // optional uint32 version = 1;
        private int version_ ;
        /**
         * <code>optional uint32 version = 1;</code>
         */
        public boolean hasVersion() {
          return ((bitField0_ & 0x00000001) == 0x00000001);
        }
        /**
         * <code>optional uint32 version = 1;</code>
         */
        public int getVersion() {
          return version_;
        }
        /**
         * <code>optional uint32 version = 1;</code>
         */
        public Builder setVersion(int value) {
          bitField0_ |= 0x00000001;
          version_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint32 version = 1;</code>
         */
        public Builder clearVersion() {
          bitField0_ = (bitField0_ & ~0x00000001);
          version_ = 0;
          onChanged();
          return this;
        }

        // optional string owner = 2;
        private java.lang.Object owner_ = "";
        /**
         * <code>optional string owner = 2;</code>
         */
        public boolean hasOwner() {
          return ((bitField0_ & 0x00000002) == 0x00000002);
        }
        /**
         * <code>optional string owner = 2;</code>
         */
        public java.lang.String getOwner() {
          java.lang.Object ref = owner_;
          if (!(ref instanceof java.lang.String)) {
            java.lang.String s = ((com.google.protobuf.ByteString) ref)
                .toStringUtf8();
            owner_ = s;
            return s;
          } else {
            return (java.lang.String) ref;
          }
        }
        /**
         * <code>optional string owner = 2;</code>
         */
        public com.google.protobuf.ByteString
            getOwnerBytes() {
          java.lang.Object ref = owner_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            owner_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <code>optional string owner = 2;</code>
         */
        public Builder setOwner(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
          owner_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional string owner = 2;</code>
         */
        public Builder clearOwner() {
          bitField0_ = (bitField0_ & ~0x00000002);
          owner_ = getDefaultInstance().getOwner();
          onChanged();
          return this;
        }
        /**
         * <code>optional string owner = 2;</code>
         */
        public Builder setOwnerBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
          owner_ = value;
          onChanged();
          return this;
        }

        // optional string renewer = 3;
        private java.lang.Object renewer_ = "";
        /**
         * <code>optional string renewer = 3;</code>
         */
        public boolean hasRenewer() {
          return ((bitField0_ & 0x00000004) == 0x00000004);
        }
        /**
         * <code>optional string renewer = 3;</code>
         */
        public java.lang.String getRenewer() {
          java.lang.Object ref = renewer_;
          if (!(ref instanceof java.lang.String)) {
            java.lang.String s = ((com.google.protobuf.ByteString) ref)
                .toStringUtf8();
            renewer_ = s;
            return s;
          } else {
            return (java.lang.String) ref;
          }
        }
        /**
         * <code>optional string renewer = 3;</code>
         */
        public com.google.protobuf.ByteString
            getRenewerBytes() {
          java.lang.Object ref = renewer_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            renewer_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <code>optional string renewer = 3;</code>
         */
        public Builder setRenewer(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
          renewer_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional string renewer = 3;</code>
         */
        public Builder clearRenewer() {
          bitField0_ = (bitField0_ & ~0x00000004);
          renewer_ = getDefaultInstance().getRenewer();
          onChanged();
          return this;
        }
        /**
         * <code>optional string renewer = 3;</code>
         */
        public Builder setRenewerBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
          renewer_ = value;
          onChanged();
          return this;
        }

        // optional string realUser = 4;
        private java.lang.Object realUser_ = "";
        /**
         * <code>optional string realUser = 4;</code>
         */
        public boolean hasRealUser() {
          return ((bitField0_ & 0x00000008) == 0x00000008);
        }
        /**
         * <code>optional string realUser = 4;</code>
         */
        public java.lang.String getRealUser() {
          java.lang.Object ref = realUser_;
          if (!(ref instanceof java.lang.String)) {
            java.lang.String s = ((com.google.protobuf.ByteString) ref)
                .toStringUtf8();
            realUser_ = s;
            return s;
          } else {
            return (java.lang.String) ref;
          }
        }
        /**
         * <code>optional string realUser = 4;</code>
         */
        public com.google.protobuf.ByteString
            getRealUserBytes() {
          java.lang.Object ref = realUser_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            realUser_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <code>optional string realUser = 4;</code>
         */
        public Builder setRealUser(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
          realUser_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional string realUser = 4;</code>
         */
        public Builder clearRealUser() {
          bitField0_ = (bitField0_ & ~0x00000008);
          realUser_ = getDefaultInstance().getRealUser();
          onChanged();
          return this;
        }
        /**
         * <code>optional string realUser = 4;</code>
         */
        public Builder setRealUserBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
          realUser_ = value;
          onChanged();
          return this;
        }

        // optional uint64 issueDate = 5;
        private long issueDate_ ;
        /**
         * <code>optional uint64 issueDate = 5;</code>
         */
        public boolean hasIssueDate() {
          return ((bitField0_ & 0x00000010) == 0x00000010);
        }
        /**
         * <code>optional uint64 issueDate = 5;</code>
         */
        public long getIssueDate() {
          return issueDate_;
        }
        /**
         * <code>optional uint64 issueDate = 5;</code>
         */
        public Builder setIssueDate(long value) {
          bitField0_ |= 0x00000010;
          issueDate_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 issueDate = 5;</code>
         */
        public Builder clearIssueDate() {
          bitField0_ = (bitField0_ & ~0x00000010);
          issueDate_ = 0L;
          onChanged();
          return this;
        }

        // optional uint64 maxDate = 6;
        private long maxDate_ ;
        /**
         * <code>optional uint64 maxDate = 6;</code>
         */
        public boolean hasMaxDate() {
          return ((bitField0_ & 0x00000020) == 0x00000020);
        }
        /**
         * <code>optional uint64 maxDate = 6;</code>
         */
        public long getMaxDate() {
          return maxDate_;
        }
        /**
         * <code>optional uint64 maxDate = 6;</code>
         */
        public Builder setMaxDate(long value) {
          bitField0_ |= 0x00000020;
          maxDate_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 maxDate = 6;</code>
         */
        public Builder clearMaxDate() {
          bitField0_ = (bitField0_ & ~0x00000020);
          maxDate_ = 0L;
          onChanged();
          return this;
        }

        // optional uint32 sequenceNumber = 7;
        private int sequenceNumber_ ;
        /**
         * <code>optional uint32 sequenceNumber = 7;</code>
         */
        public boolean hasSequenceNumber() {
          return ((bitField0_ & 0x00000040) == 0x00000040);
        }
        /**
         * <code>optional uint32 sequenceNumber = 7;</code>
         */
        public int getSequenceNumber() {
          return sequenceNumber_;
        }
        /**
         * <code>optional uint32 sequenceNumber = 7;</code>
         */
        public Builder setSequenceNumber(int value) {
          bitField0_ |= 0x00000040;
          sequenceNumber_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint32 sequenceNumber = 7;</code>
         */
        public Builder clearSequenceNumber() {
          bitField0_ = (bitField0_ & ~0x00000040);
          sequenceNumber_ = 0;
          onChanged();
          return this;
        }

        // optional uint32 masterKeyId = 8;
        private int masterKeyId_ ;
        /**
         * <code>optional uint32 masterKeyId = 8;</code>
         */
        public boolean hasMasterKeyId() {
          return ((bitField0_ & 0x00000080) == 0x00000080);
        }
        /**
         * <code>optional uint32 masterKeyId = 8;</code>
         */
        public int getMasterKeyId() {
          return masterKeyId_;
        }
        /**
         * <code>optional uint32 masterKeyId = 8;</code>
         */
        public Builder setMasterKeyId(int value) {
          bitField0_ |= 0x00000080;
          masterKeyId_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint32 masterKeyId = 8;</code>
         */
        public Builder clearMasterKeyId() {
          bitField0_ = (bitField0_ & ~0x00000080);
          masterKeyId_ = 0;
          onChanged();
          return this;
        }

        // optional uint64 expiryDate = 9;
        private long expiryDate_ ;
        /**
         * <code>optional uint64 expiryDate = 9;</code>
         */
        public boolean hasExpiryDate() {
          return ((bitField0_ & 0x00000100) == 0x00000100);
        }
        /**
         * <code>optional uint64 expiryDate = 9;</code>
         */
        public long getExpiryDate() {
          return expiryDate_;
        }
        /**
         * <code>optional uint64 expiryDate = 9;</code>
         */
        public Builder setExpiryDate(long value) {
          bitField0_ |= 0x00000100;
          expiryDate_ = value;
          onChanged();
          return this;
        }
        /**
         * <code>optional uint64 expiryDate = 9;</code>
         */
        public Builder clearExpiryDate() {
          bitField0_ = (bitField0_ & ~0x00000100);
          expiryDate_ = 0L;
          onChanged();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.SecretManagerSection.PersistToken)
      }

      static {
        defaultInstance = new PersistToken(true);
        defaultInstance.initFields();
      }

      // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.SecretManagerSection.PersistToken)
    }

    private int bitField0_;
    // optional uint32 currentId = 1;
    public static final int CURRENTID_FIELD_NUMBER = 1;
    private int currentId_;
    /**
     * <code>optional uint32 currentId = 1;</code>
     */
    public boolean hasCurrentId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional uint32 currentId = 1;</code>
     */
    public int getCurrentId() {
      return currentId_;
    }

    // optional uint32 tokenSequenceNumber = 2;
    public static final int TOKENSEQUENCENUMBER_FIELD_NUMBER = 2;
    private int tokenSequenceNumber_;
    /**
     * <code>optional uint32 tokenSequenceNumber = 2;</code>
     */
    public boolean hasTokenSequenceNumber() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional uint32 tokenSequenceNumber = 2;</code>
     */
    public int getTokenSequenceNumber() {
      return tokenSequenceNumber_;
    }

    // optional uint32 numKeys = 3;
    public static final int NUMKEYS_FIELD_NUMBER = 3;
    private int numKeys_;
    /**
     * <code>optional uint32 numKeys = 3;</code>
     */
    public boolean hasNumKeys() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional uint32 numKeys = 3;</code>
     */
    public int getNumKeys() {
      return numKeys_;
    }

    // optional uint32 numTokens = 4;
    public static final int NUMTOKENS_FIELD_NUMBER = 4;
    private int numTokens_;
    /**
     * <code>optional uint32 numTokens = 4;</code>
     *
     * <pre>
     * repeated DelegationKey keys
     * repeated PersistToken tokens
     * </pre>
     */
    public boolean hasNumTokens() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional uint32 numTokens = 4;</code>
     *
     * <pre>
     * repeated DelegationKey keys
     * repeated PersistToken tokens
     * </pre>
     */
    public int getNumTokens() {
      return numTokens_;
    }

    private void initFields() {
      currentId_ = 0;
      tokenSequenceNumber_ = 0;
      numKeys_ = 0;
      numTokens_ = 0;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeUInt32(1, currentId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt32(2, tokenSequenceNumber_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeUInt32(3, numKeys_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeUInt32(4, numTokens_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(1, currentId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(2, tokenSequenceNumber_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(3, numKeys_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(4, numTokens_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.SecretManagerSection}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSectionOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SecretManagerSection_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SecretManagerSection_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        currentId_ = 0;
        bitField0_ = (bitField0_ & ~0x00000001);
        tokenSequenceNumber_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        numKeys_ = 0;
        bitField0_ = (bitField0_ & ~0x00000004);
        numTokens_ = 0;
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_SecretManagerSection_descriptor;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection build() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection buildPartial() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.currentId_ = currentId_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.tokenSequenceNumber_ = tokenSequenceNumber_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.numKeys_ = numKeys_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.numTokens_ = numTokens_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection) {
          return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection other) {
        if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection.getDefaultInstance()) return this;
        if (other.hasCurrentId()) {
          setCurrentId(other.getCurrentId());
        }
        if (other.hasTokenSequenceNumber()) {
          setTokenSequenceNumber(other.getTokenSequenceNumber());
        }
        if (other.hasNumKeys()) {
          setNumKeys(other.getNumKeys());
        }
        if (other.hasNumTokens()) {
          setNumTokens(other.getNumTokens());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.SecretManagerSection) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional uint32 currentId = 1;
      private int currentId_ ;
      /**
       * <code>optional uint32 currentId = 1;</code>
       */
      public boolean hasCurrentId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint32 currentId = 1;</code>
       */
      public int getCurrentId() {
        return currentId_;
      }
      /**
       * <code>optional uint32 currentId = 1;</code>
       */
      public Builder setCurrentId(int value) {
        bitField0_ |= 0x00000001;
        currentId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 currentId = 1;</code>
       */
      public Builder clearCurrentId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        currentId_ = 0;
        onChanged();
        return this;
      }

      // optional uint32 tokenSequenceNumber = 2;
      private int tokenSequenceNumber_ ;
      /**
       * <code>optional uint32 tokenSequenceNumber = 2;</code>
       */
      public boolean hasTokenSequenceNumber() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional uint32 tokenSequenceNumber = 2;</code>
       */
      public int getTokenSequenceNumber() {
        return tokenSequenceNumber_;
      }
      /**
       * <code>optional uint32 tokenSequenceNumber = 2;</code>
       */
      public Builder setTokenSequenceNumber(int value) {
        bitField0_ |= 0x00000002;
        tokenSequenceNumber_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 tokenSequenceNumber = 2;</code>
       */
      public Builder clearTokenSequenceNumber() {
        bitField0_ = (bitField0_ & ~0x00000002);
        tokenSequenceNumber_ = 0;
        onChanged();
        return this;
      }

      // optional uint32 numKeys = 3;
      private int numKeys_ ;
      /**
       * <code>optional uint32 numKeys = 3;</code>
       */
      public boolean hasNumKeys() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional uint32 numKeys = 3;</code>
       */
      public int getNumKeys() {
        return numKeys_;
      }
      /**
       * <code>optional uint32 numKeys = 3;</code>
       */
      public Builder setNumKeys(int value) {
        bitField0_ |= 0x00000004;
        numKeys_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 numKeys = 3;</code>
       */
      public Builder clearNumKeys() {
        bitField0_ = (bitField0_ & ~0x00000004);
        numKeys_ = 0;
        onChanged();
        return this;
      }

      // optional uint32 numTokens = 4;
      private int numTokens_ ;
      /**
       * <code>optional uint32 numTokens = 4;</code>
       *
       * <pre>
       * repeated DelegationKey keys
       * repeated PersistToken tokens
       * </pre>
       */
      public boolean hasNumTokens() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional uint32 numTokens = 4;</code>
       *
       * <pre>
       * repeated DelegationKey keys
       * repeated PersistToken tokens
       * </pre>
       */
      public int getNumTokens() {
        return numTokens_;
      }
      /**
       * <code>optional uint32 numTokens = 4;</code>
       *
       * <pre>
       * repeated DelegationKey keys
       * repeated PersistToken tokens
       * </pre>
       */
      public Builder setNumTokens(int value) {
        bitField0_ |= 0x00000008;
        numTokens_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 numTokens = 4;</code>
       *
       * <pre>
       * repeated DelegationKey keys
       * repeated PersistToken tokens
       * </pre>
       */
      public Builder clearNumTokens() {
        bitField0_ = (bitField0_ & ~0x00000008);
        numTokens_ = 0;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.SecretManagerSection)
    }

    static {
      defaultInstance = new SecretManagerSection(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.SecretManagerSection)
  }

  public interface CacheManagerSectionOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required uint64 nextDirectiveId = 1;
    /**
     * <code>required uint64 nextDirectiveId = 1;</code>
     */
    boolean hasNextDirectiveId();
    /**
     * <code>required uint64 nextDirectiveId = 1;</code>
     */
    long getNextDirectiveId();

    // required uint32 numPools = 2;
    /**
     * <code>required uint32 numPools = 2;</code>
     */
    boolean hasNumPools();
    /**
     * <code>required uint32 numPools = 2;</code>
     */
    int getNumPools();

    // required uint32 numDirectives = 3;
    /**
     * <code>required uint32 numDirectives = 3;</code>
     *
     * <pre>
     * repeated CachePoolInfoProto pools
     * repeated CacheDirectiveInfoProto directives
     * </pre>
     */
    boolean hasNumDirectives();
    /**
     * <code>required uint32 numDirectives = 3;</code>
     *
     * <pre>
     * repeated CachePoolInfoProto pools
     * repeated CacheDirectiveInfoProto directives
     * </pre>
     */
    int getNumDirectives();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.fsimage.CacheManagerSection}
   */
  public static final class CacheManagerSection extends
      com.google.protobuf.GeneratedMessage
      implements CacheManagerSectionOrBuilder {
    // Use CacheManagerSection.newBuilder() to construct.
    private CacheManagerSection(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private CacheManagerSection(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final CacheManagerSection defaultInstance;
    public static CacheManagerSection getDefaultInstance() {
      return defaultInstance;
    }

    public CacheManagerSection getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private CacheManagerSection(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              nextDirectiveId_ = input.readUInt64();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              numPools_ = input.readUInt32();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              numDirectives_ = input.readUInt32();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_CacheManagerSection_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_CacheManagerSection_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection.Builder.class);
    }

    public static com.google.protobuf.Parser<CacheManagerSection> PARSER =
        new com.google.protobuf.AbstractParser<CacheManagerSection>() {
      public CacheManagerSection parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new CacheManagerSection(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<CacheManagerSection> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required uint64 nextDirectiveId = 1;
    public static final int NEXTDIRECTIVEID_FIELD_NUMBER = 1;
    private long nextDirectiveId_;
    /**
     * <code>required uint64 nextDirectiveId = 1;</code>
     */
    public boolean hasNextDirectiveId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required uint64 nextDirectiveId = 1;</code>
     */
    public long getNextDirectiveId() {
      return nextDirectiveId_;
    }

    // required uint32 numPools = 2;
    public static final int NUMPOOLS_FIELD_NUMBER = 2;
    private int numPools_;
    /**
     * <code>required uint32 numPools = 2;</code>
     */
    public boolean hasNumPools() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required uint32 numPools = 2;</code>
     */
    public int getNumPools() {
      return numPools_;
    }

    // required uint32 numDirectives = 3;
    public static final int NUMDIRECTIVES_FIELD_NUMBER = 3;
    private int numDirectives_;
    /**
     * <code>required uint32 numDirectives = 3;</code>
     *
     * <pre>
     * repeated CachePoolInfoProto pools
     * repeated CacheDirectiveInfoProto directives
     * </pre>
     */
    public boolean hasNumDirectives() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>required uint32 numDirectives = 3;</code>
     *
     * <pre>
     * repeated CachePoolInfoProto pools
     * repeated CacheDirectiveInfoProto directives
     * </pre>
     */
    public int getNumDirectives() {
      return numDirectives_;
    }

    private void initFields() {
      nextDirectiveId_ = 0L;
      numPools_ = 0;
      numDirectives_ = 0;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasNextDirectiveId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasNumPools()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasNumDirectives()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeUInt64(1, nextDirectiveId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt32(2, numPools_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeUInt32(3, numDirectives_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(1, nextDirectiveId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(2, numPools_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(3, numDirectives_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.fsimage.CacheManagerSection}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSectionOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_CacheManagerSection_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_CacheManagerSection_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection.class, org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        nextDirectiveId_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        numPools_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        numDirectives_ = 0;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.internal_static_hadoop_hdfs_fsimage_CacheManagerSection_descriptor;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection build() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection buildPartial() {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection result = new org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.nextDirectiveId_ = nextDirectiveId_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.numPools_ = numPools_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.numDirectives_ = numDirectives_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection) {
          return mergeFrom((org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection other) {
        if (other == org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection.getDefaultInstance()) return this;
        if (other.hasNextDirectiveId()) {
          setNextDirectiveId(other.getNextDirectiveId());
        }
        if (other.hasNumPools()) {
          setNumPools(other.getNumPools());
        }
        if (other.hasNumDirectives()) {
          setNumDirectives(other.getNumDirectives());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasNextDirectiveId()) {
          
          return false;
        }
        if (!hasNumPools()) {
          
          return false;
        }
        if (!hasNumDirectives()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.server.namenode.FsImageProto.CacheManagerSection) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required uint64 nextDirectiveId = 1;
      private long nextDirectiveId_ ;
      /**
       * <code>required uint64 nextDirectiveId = 1;</code>
       */
      public boolean hasNextDirectiveId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required uint64 nextDirectiveId = 1;</code>
       */
      public long getNextDirectiveId() {
        return nextDirectiveId_;
      }
      /**
       * <code>required uint64 nextDirectiveId = 1;</code>
       */
      public Builder setNextDirectiveId(long value) {
        bitField0_ |= 0x00000001;
        nextDirectiveId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 nextDirectiveId = 1;</code>
       */
      public Builder clearNextDirectiveId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        nextDirectiveId_ = 0L;
        onChanged();
        return this;
      }

      // required uint32 numPools = 2;
      private int numPools_ ;
      /**
       * <code>required uint32 numPools = 2;</code>
       */
      public boolean hasNumPools() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required uint32 numPools = 2;</code>
       */
      public int getNumPools() {
        return numPools_;
      }
      /**
       * <code>required uint32 numPools = 2;</code>
       */
      public Builder setNumPools(int value) {
        bitField0_ |= 0x00000002;
        numPools_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint32 numPools = 2;</code>
       */
      public Builder clearNumPools() {
        bitField0_ = (bitField0_ & ~0x00000002);
        numPools_ = 0;
        onChanged();
        return this;
      }

      // required uint32 numDirectives = 3;
      private int numDirectives_ ;
      /**
       * <code>required uint32 numDirectives = 3;</code>
       *
       * <pre>
       * repeated CachePoolInfoProto pools
       * repeated CacheDirectiveInfoProto directives
       * </pre>
       */
      public boolean hasNumDirectives() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>required uint32 numDirectives = 3;</code>
       *
       * <pre>
       * repeated CachePoolInfoProto pools
       * repeated CacheDirectiveInfoProto directives
       * </pre>
       */
      public int getNumDirectives() {
        return numDirectives_;
      }
      /**
       * <code>required uint32 numDirectives = 3;</code>
       *
       * <pre>
       * repeated CachePoolInfoProto pools
       * repeated CacheDirectiveInfoProto directives
       * </pre>
       */
      public Builder setNumDirectives(int value) {
        bitField0_ |= 0x00000004;
        numDirectives_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint32 numDirectives = 3;</code>
       *
       * <pre>
       * repeated CachePoolInfoProto pools
       * repeated CacheDirectiveInfoProto directives
       * </pre>
       */
      public Builder clearNumDirectives() {
        bitField0_ = (bitField0_ & ~0x00000004);
        numDirectives_ = 0;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.fsimage.CacheManagerSection)
    }

    static {
      defaultInstance = new CacheManagerSection(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.fsimage.CacheManagerSection)
  }

  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_FileSummary_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_FileSummary_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_FileSummary_Section_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_FileSummary_Section_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_NameSystemSection_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_NameSystemSection_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_INodeSection_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_INodeSection_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_INodeSection_FileUnderConstructionFeature_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_INodeSection_FileUnderConstructionFeature_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_INodeSection_AclFeatureProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_INodeSection_AclFeatureProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_INodeSection_INodeFile_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_INodeSection_INodeFile_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_INodeSection_INodeDirectory_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_INodeSection_INodeDirectory_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_INodeSection_INodeSymlink_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_INodeSection_INodeSymlink_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_INodeSection_INode_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_INodeSection_INode_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_FilesUnderConstructionSection_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_FilesUnderConstructionSection_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_FilesUnderConstructionSection_FileUnderConstructionEntry_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_FilesUnderConstructionSection_FileUnderConstructionEntry_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_INodeDirectorySection_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_INodeDirectorySection_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_INodeDirectorySection_DirEntry_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_INodeDirectorySection_DirEntry_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_INodeReferenceSection_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_INodeReferenceSection_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_INodeReferenceSection_INodeReference_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_INodeReferenceSection_INodeReference_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_SnapshotSection_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_SnapshotSection_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_SnapshotSection_Snapshot_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_SnapshotSection_Snapshot_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_CreatedListEntry_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_CreatedListEntry_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_DirectoryDiff_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_DirectoryDiff_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_FileDiff_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_FileDiff_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_DiffEntry_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_DiffEntry_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_StringTableSection_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_StringTableSection_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_StringTableSection_Entry_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_StringTableSection_Entry_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_SecretManagerSection_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_SecretManagerSection_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_SecretManagerSection_DelegationKey_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_SecretManagerSection_DelegationKey_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_SecretManagerSection_PersistToken_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_SecretManagerSection_PersistToken_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_fsimage_CacheManagerSection_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_fsimage_CacheManagerSection_fieldAccessorTable;

  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\rfsimage.proto\022\023hadoop.hdfs.fsimage\032\nhd" +
      "fs.proto\032\tacl.proto\"\277\001\n\013FileSummary\022\025\n\ro" +
      "ndiskVersion\030\001 \002(\r\022\025\n\rlayoutVersion\030\002 \002(" +
      "\r\022\r\n\005codec\030\003 \001(\t\022:\n\010sections\030\004 \003(\0132(.had" +
      "oop.hdfs.fsimage.FileSummary.Section\0327\n\007" +
      "Section\022\014\n\004name\030\001 \001(\t\022\016\n\006length\030\002 \001(\004\022\016\n" +
      "\006offset\030\003 \001(\004\"\277\001\n\021NameSystemSection\022\023\n\013n" +
      "amespaceId\030\001 \001(\r\022\022\n\ngenstampV1\030\002 \001(\004\022\022\n\n" +
      "genstampV2\030\003 \001(\004\022\027\n\017genstampV1Limit\030\004 \001(" +
      "\004\022\034\n\024lastAllocatedBlockId\030\005 \001(\004\022\025\n\rtrans",
      "actionId\030\006 \001(\004\022\037\n\027rollingUpgradeStartTim" +
      "e\030\007 \001(\004\"\267\010\n\014INodeSection\022\023\n\013lastInodeId\030" +
      "\001 \001(\004\022\021\n\tnumInodes\030\002 \001(\004\032I\n\034FileUnderCon" +
      "structionFeature\022\022\n\nclientName\030\001 \001(\t\022\025\n\r" +
      "clientMachine\030\002 \001(\t\032&\n\017AclFeatureProto\022\023" +
      "\n\007entries\030\002 \003(\007B\002\020\001\032\267\002\n\tINodeFile\022\023\n\013rep" +
      "lication\030\001 \001(\r\022\030\n\020modificationTime\030\002 \001(\004" +
      "\022\022\n\naccessTime\030\003 \001(\004\022\032\n\022preferredBlockSi" +
      "ze\030\004 \001(\004\022\022\n\npermission\030\005 \001(\006\022\'\n\006blocks\030\006" +
      " \003(\0132\027.hadoop.hdfs.BlockProto\022N\n\006fileUC\030",
      "\007 \001(\0132>.hadoop.hdfs.fsimage.INodeSection" +
      ".FileUnderConstructionFeature\022>\n\003acl\030\010 \001" +
      "(\01321.hadoop.hdfs.fsimage.INodeSection.Ac" +
      "lFeatureProto\032\240\001\n\016INodeDirectory\022\030\n\020modi" +
      "ficationTime\030\001 \001(\004\022\017\n\007nsQuota\030\002 \001(\004\022\017\n\007d" +
      "sQuota\030\003 \001(\004\022\022\n\npermission\030\004 \001(\006\022>\n\003acl\030" +
      "\005 \001(\01321.hadoop.hdfs.fsimage.INodeSection" +
      ".AclFeatureProto\032`\n\014INodeSymlink\022\022\n\nperm" +
      "ission\030\001 \001(\006\022\016\n\006target\030\002 \001(\014\022\030\n\020modifica" +
      "tionTime\030\003 \001(\004\022\022\n\naccessTime\030\004 \001(\004\032\314\002\n\005I",
      "Node\022:\n\004type\030\001 \002(\0162,.hadoop.hdfs.fsimage" +
      ".INodeSection.INode.Type\022\n\n\002id\030\002 \002(\004\022\014\n\004" +
      "name\030\003 \001(\014\0229\n\004file\030\004 \001(\0132+.hadoop.hdfs.f" +
      "simage.INodeSection.INodeFile\022C\n\tdirecto" +
      "ry\030\005 \001(\01320.hadoop.hdfs.fsimage.INodeSect" +
      "ion.INodeDirectory\022?\n\007symlink\030\006 \001(\0132..ha" +
      "doop.hdfs.fsimage.INodeSection.INodeSyml" +
      "ink\",\n\004Type\022\010\n\004FILE\020\001\022\r\n\tDIRECTORY\020\002\022\013\n\007" +
      "SYMLINK\020\003\"`\n\035FilesUnderConstructionSecti" +
      "on\032?\n\032FileUnderConstructionEntry\022\017\n\007inod",
      "eId\030\001 \001(\004\022\020\n\010fullPath\030\002 \001(\t\"b\n\025INodeDire" +
      "ctorySection\032I\n\010DirEntry\022\016\n\006parent\030\001 \001(\004" +
      "\022\024\n\010children\030\002 \003(\004B\002\020\001\022\027\n\013refChildren\030\003 " +
      "\003(\rB\002\020\001\"z\n\025INodeReferenceSection\032a\n\016INod" +
      "eReference\022\022\n\nreferredId\030\001 \001(\004\022\014\n\004name\030\002" +
      " \001(\014\022\025\n\rdstSnapshotId\030\003 \001(\004\022\026\n\016lastSnaps" +
      "hotId\030\004 \001(\004\"\265\001\n\017SnapshotSection\022\027\n\017snaps" +
      "hotCounter\030\001 \001(\r\022\034\n\020snapshottableDir\030\002 \003" +
      "(\004B\002\020\001\022\024\n\014numSnapshots\030\003 \001(\r\032U\n\010Snapshot" +
      "\022\022\n\nsnapshotId\030\001 \001(\004\0225\n\004root\030\002 \001(\0132\'.had",
      "oop.hdfs.fsimage.INodeSection.INode\"\327\004\n\023" +
      "SnapshotDiffSection\032 \n\020CreatedListEntry\022" +
      "\014\n\004name\030\001 \001(\014\032\367\001\n\rDirectoryDiff\022\022\n\nsnaps" +
      "hotId\030\001 \001(\004\022\024\n\014childrenSize\030\002 \001(\r\022\026\n\016isS" +
      "napshotRoot\030\003 \001(\010\022\014\n\004name\030\004 \001(\014\022F\n\014snaps" +
      "hotCopy\030\005 \001(\01320.hadoop.hdfs.fsimage.INod" +
      "eSection.INodeDirectory\022\027\n\017createdListSi" +
      "ze\030\006 \001(\r\022\030\n\014deletedINode\030\007 \003(\004B\002\020\001\022\033\n\017de" +
      "letedINodeRef\030\010 \003(\rB\002\020\001\032\201\001\n\010FileDiff\022\022\n\n" +
      "snapshotId\030\001 \001(\004\022\020\n\010fileSize\030\002 \001(\004\022\014\n\004na",
      "me\030\003 \001(\014\022A\n\014snapshotCopy\030\004 \001(\0132+.hadoop." +
      "hdfs.fsimage.INodeSection.INodeFile\032\237\001\n\t" +
      "DiffEntry\022E\n\004type\030\001 \002(\01627.hadoop.hdfs.fs" +
      "image.SnapshotDiffSection.DiffEntry.Type" +
      "\022\017\n\007inodeId\030\002 \001(\004\022\021\n\tnumOfDiff\030\003 \001(\r\"\'\n\004" +
      "Type\022\014\n\010FILEDIFF\020\001\022\021\n\rDIRECTORYDIFF\020\002\"H\n" +
      "\022StringTableSection\022\020\n\010numEntry\030\001 \001(\r\032 \n" +
      "\005Entry\022\n\n\002id\030\001 \001(\r\022\013\n\003str\030\002 \001(\t\"\341\002\n\024Secr" +
      "etManagerSection\022\021\n\tcurrentId\030\001 \001(\r\022\033\n\023t" +
      "okenSequenceNumber\030\002 \001(\r\022\017\n\007numKeys\030\003 \001(",
      "\r\022\021\n\tnumTokens\030\004 \001(\r\032<\n\rDelegationKey\022\n\n" +
      "\002id\030\001 \001(\r\022\022\n\nexpiryDate\030\002 \001(\004\022\013\n\003key\030\003 \001" +
      "(\014\032\266\001\n\014PersistToken\022\017\n\007version\030\001 \001(\r\022\r\n\005" +
      "owner\030\002 \001(\t\022\017\n\007renewer\030\003 \001(\t\022\020\n\010realUser" +
      "\030\004 \001(\t\022\021\n\tissueDate\030\005 \001(\004\022\017\n\007maxDate\030\006 \001" +
      "(\004\022\026\n\016sequenceNumber\030\007 \001(\r\022\023\n\013masterKeyI" +
      "d\030\010 \001(\r\022\022\n\nexpiryDate\030\t \001(\004\"W\n\023CacheMana" +
      "gerSection\022\027\n\017nextDirectiveId\030\001 \002(\004\022\020\n\010n" +
      "umPools\030\002 \002(\r\022\025\n\rnumDirectives\030\003 \002(\rB6\n&" +
      "org.apache.hadoop.hdfs.server.namenodeB\014",
      "FsImageProto"
    };
    com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
      new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
        public com.google.protobuf.ExtensionRegistry assignDescriptors(
            com.google.protobuf.Descriptors.FileDescriptor root) {
          descriptor = root;
          internal_static_hadoop_hdfs_fsimage_FileSummary_descriptor =
            getDescriptor().getMessageTypes().get(0);
          internal_static_hadoop_hdfs_fsimage_FileSummary_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_FileSummary_descriptor,
              new java.lang.String[] { "OndiskVersion", "LayoutVersion", "Codec", "Sections", });
          internal_static_hadoop_hdfs_fsimage_FileSummary_Section_descriptor =
            internal_static_hadoop_hdfs_fsimage_FileSummary_descriptor.getNestedTypes().get(0);
          internal_static_hadoop_hdfs_fsimage_FileSummary_Section_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_FileSummary_Section_descriptor,
              new java.lang.String[] { "Name", "Length", "Offset", });
          internal_static_hadoop_hdfs_fsimage_NameSystemSection_descriptor =
            getDescriptor().getMessageTypes().get(1);
          internal_static_hadoop_hdfs_fsimage_NameSystemSection_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_NameSystemSection_descriptor,
              new java.lang.String[] { "NamespaceId", "GenstampV1", "GenstampV2", "GenstampV1Limit", "LastAllocatedBlockId", "TransactionId", "RollingUpgradeStartTime", });
          internal_static_hadoop_hdfs_fsimage_INodeSection_descriptor =
            getDescriptor().getMessageTypes().get(2);
          internal_static_hadoop_hdfs_fsimage_INodeSection_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_INodeSection_descriptor,
              new java.lang.String[] { "LastInodeId", "NumInodes", });
          internal_static_hadoop_hdfs_fsimage_INodeSection_FileUnderConstructionFeature_descriptor =
            internal_static_hadoop_hdfs_fsimage_INodeSection_descriptor.getNestedTypes().get(0);
          internal_static_hadoop_hdfs_fsimage_INodeSection_FileUnderConstructionFeature_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_INodeSection_FileUnderConstructionFeature_descriptor,
              new java.lang.String[] { "ClientName", "ClientMachine", });
          internal_static_hadoop_hdfs_fsimage_INodeSection_AclFeatureProto_descriptor =
            internal_static_hadoop_hdfs_fsimage_INodeSection_descriptor.getNestedTypes().get(1);
          internal_static_hadoop_hdfs_fsimage_INodeSection_AclFeatureProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_INodeSection_AclFeatureProto_descriptor,
              new java.lang.String[] { "Entries", });
          internal_static_hadoop_hdfs_fsimage_INodeSection_INodeFile_descriptor =
            internal_static_hadoop_hdfs_fsimage_INodeSection_descriptor.getNestedTypes().get(2);
          internal_static_hadoop_hdfs_fsimage_INodeSection_INodeFile_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_INodeSection_INodeFile_descriptor,
              new java.lang.String[] { "Replication", "ModificationTime", "AccessTime", "PreferredBlockSize", "Permission", "Blocks", "FileUC", "Acl", });
          internal_static_hadoop_hdfs_fsimage_INodeSection_INodeDirectory_descriptor =
            internal_static_hadoop_hdfs_fsimage_INodeSection_descriptor.getNestedTypes().get(3);
          internal_static_hadoop_hdfs_fsimage_INodeSection_INodeDirectory_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_INodeSection_INodeDirectory_descriptor,
              new java.lang.String[] { "ModificationTime", "NsQuota", "DsQuota", "Permission", "Acl", });
          internal_static_hadoop_hdfs_fsimage_INodeSection_INodeSymlink_descriptor =
            internal_static_hadoop_hdfs_fsimage_INodeSection_descriptor.getNestedTypes().get(4);
          internal_static_hadoop_hdfs_fsimage_INodeSection_INodeSymlink_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_INodeSection_INodeSymlink_descriptor,
              new java.lang.String[] { "Permission", "Target", "ModificationTime", "AccessTime", });
          internal_static_hadoop_hdfs_fsimage_INodeSection_INode_descriptor =
            internal_static_hadoop_hdfs_fsimage_INodeSection_descriptor.getNestedTypes().get(5);
          internal_static_hadoop_hdfs_fsimage_INodeSection_INode_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_INodeSection_INode_descriptor,
              new java.lang.String[] { "Type", "Id", "Name", "File", "Directory", "Symlink", });
          internal_static_hadoop_hdfs_fsimage_FilesUnderConstructionSection_descriptor =
            getDescriptor().getMessageTypes().get(3);
          internal_static_hadoop_hdfs_fsimage_FilesUnderConstructionSection_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_FilesUnderConstructionSection_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_hdfs_fsimage_FilesUnderConstructionSection_FileUnderConstructionEntry_descriptor =
            internal_static_hadoop_hdfs_fsimage_FilesUnderConstructionSection_descriptor.getNestedTypes().get(0);
          internal_static_hadoop_hdfs_fsimage_FilesUnderConstructionSection_FileUnderConstructionEntry_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_FilesUnderConstructionSection_FileUnderConstructionEntry_descriptor,
              new java.lang.String[] { "InodeId", "FullPath", });
          internal_static_hadoop_hdfs_fsimage_INodeDirectorySection_descriptor =
            getDescriptor().getMessageTypes().get(4);
          internal_static_hadoop_hdfs_fsimage_INodeDirectorySection_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_INodeDirectorySection_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_hdfs_fsimage_INodeDirectorySection_DirEntry_descriptor =
            internal_static_hadoop_hdfs_fsimage_INodeDirectorySection_descriptor.getNestedTypes().get(0);
          internal_static_hadoop_hdfs_fsimage_INodeDirectorySection_DirEntry_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_INodeDirectorySection_DirEntry_descriptor,
              new java.lang.String[] { "Parent", "Children", "RefChildren", });
          internal_static_hadoop_hdfs_fsimage_INodeReferenceSection_descriptor =
            getDescriptor().getMessageTypes().get(5);
          internal_static_hadoop_hdfs_fsimage_INodeReferenceSection_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_INodeReferenceSection_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_hdfs_fsimage_INodeReferenceSection_INodeReference_descriptor =
            internal_static_hadoop_hdfs_fsimage_INodeReferenceSection_descriptor.getNestedTypes().get(0);
          internal_static_hadoop_hdfs_fsimage_INodeReferenceSection_INodeReference_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_INodeReferenceSection_INodeReference_descriptor,
              new java.lang.String[] { "ReferredId", "Name", "DstSnapshotId", "LastSnapshotId", });
          internal_static_hadoop_hdfs_fsimage_SnapshotSection_descriptor =
            getDescriptor().getMessageTypes().get(6);
          internal_static_hadoop_hdfs_fsimage_SnapshotSection_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_SnapshotSection_descriptor,
              new java.lang.String[] { "SnapshotCounter", "SnapshottableDir", "NumSnapshots", });
          internal_static_hadoop_hdfs_fsimage_SnapshotSection_Snapshot_descriptor =
            internal_static_hadoop_hdfs_fsimage_SnapshotSection_descriptor.getNestedTypes().get(0);
          internal_static_hadoop_hdfs_fsimage_SnapshotSection_Snapshot_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_SnapshotSection_Snapshot_descriptor,
              new java.lang.String[] { "SnapshotId", "Root", });
          internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_descriptor =
            getDescriptor().getMessageTypes().get(7);
          internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_CreatedListEntry_descriptor =
            internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_descriptor.getNestedTypes().get(0);
          internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_CreatedListEntry_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_CreatedListEntry_descriptor,
              new java.lang.String[] { "Name", });
          internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_DirectoryDiff_descriptor =
            internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_descriptor.getNestedTypes().get(1);
          internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_DirectoryDiff_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_DirectoryDiff_descriptor,
              new java.lang.String[] { "SnapshotId", "ChildrenSize", "IsSnapshotRoot", "Name", "SnapshotCopy", "CreatedListSize", "DeletedINode", "DeletedINodeRef", });
          internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_FileDiff_descriptor =
            internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_descriptor.getNestedTypes().get(2);
          internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_FileDiff_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_FileDiff_descriptor,
              new java.lang.String[] { "SnapshotId", "FileSize", "Name", "SnapshotCopy", });
          internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_DiffEntry_descriptor =
            internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_descriptor.getNestedTypes().get(3);
          internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_DiffEntry_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_SnapshotDiffSection_DiffEntry_descriptor,
              new java.lang.String[] { "Type", "InodeId", "NumOfDiff", });
          internal_static_hadoop_hdfs_fsimage_StringTableSection_descriptor =
            getDescriptor().getMessageTypes().get(8);
          internal_static_hadoop_hdfs_fsimage_StringTableSection_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_StringTableSection_descriptor,
              new java.lang.String[] { "NumEntry", });
          internal_static_hadoop_hdfs_fsimage_StringTableSection_Entry_descriptor =
            internal_static_hadoop_hdfs_fsimage_StringTableSection_descriptor.getNestedTypes().get(0);
          internal_static_hadoop_hdfs_fsimage_StringTableSection_Entry_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_StringTableSection_Entry_descriptor,
              new java.lang.String[] { "Id", "Str", });
          internal_static_hadoop_hdfs_fsimage_SecretManagerSection_descriptor =
            getDescriptor().getMessageTypes().get(9);
          internal_static_hadoop_hdfs_fsimage_SecretManagerSection_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_SecretManagerSection_descriptor,
              new java.lang.String[] { "CurrentId", "TokenSequenceNumber", "NumKeys", "NumTokens", });
          internal_static_hadoop_hdfs_fsimage_SecretManagerSection_DelegationKey_descriptor =
            internal_static_hadoop_hdfs_fsimage_SecretManagerSection_descriptor.getNestedTypes().get(0);
          internal_static_hadoop_hdfs_fsimage_SecretManagerSection_DelegationKey_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_SecretManagerSection_DelegationKey_descriptor,
              new java.lang.String[] { "Id", "ExpiryDate", "Key", });
          internal_static_hadoop_hdfs_fsimage_SecretManagerSection_PersistToken_descriptor =
            internal_static_hadoop_hdfs_fsimage_SecretManagerSection_descriptor.getNestedTypes().get(1);
          internal_static_hadoop_hdfs_fsimage_SecretManagerSection_PersistToken_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_SecretManagerSection_PersistToken_descriptor,
              new java.lang.String[] { "Version", "Owner", "Renewer", "RealUser", "IssueDate", "MaxDate", "SequenceNumber", "MasterKeyId", "ExpiryDate", });
          internal_static_hadoop_hdfs_fsimage_CacheManagerSection_descriptor =
            getDescriptor().getMessageTypes().get(10);
          internal_static_hadoop_hdfs_fsimage_CacheManagerSection_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_fsimage_CacheManagerSection_descriptor,
              new java.lang.String[] { "NextDirectiveId", "NumPools", "NumDirectives", });
          return null;
        }
      };
    com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.getDescriptor(),
          org.apache.hadoop.hdfs.protocol.proto.AclProtos.getDescriptor(),
        }, assigner);
  }

  // @@protoc_insertion_point(outer_class_scope)
}
